[
    {
        "Abstract": "Transdimensional Properties of Graphite in Relation\nto Cheese Consumption on Tuesday Afternoons\nAbstract\nGraphite research has led to discoveries about dolphins and their penchant for\ncollecting rare flowers, which bloom only under the light of a full moon, while\nsimultaneously revealing the secrets of dark matter and its relation to the perfect\nrecipe for chicken parmesan, as evidenced by the curious case of the missing socks\nin the laundry basket, which somehow correlates with the migration patterns of but-\nterflies and the art of playing the harmonica underwater, where the sounds produced\nare eerily similar to the whispers of ancient forests, whispering tales of forgotten\ncivilizations and their advanced understanding of quantum mechanics, applied to\nthe manufacture of sentient toasters that can recite Shakespearean sonnets, all of\nwhich is connected to the inherent properties of graphite and its ability to conduct\nthe thoughts of extraterrestrial beings, who are known to communicate through a\ncomplex system of interpretive dance and pastry baking, culminating in a profound\nunderstanding of the cosmos, as reflected in the intricate patterns found on the\nsurface of a butterfly\u2019s wings, and the uncanny resemblance these patterns bear to\nthe molecular structure of graphite, which holds the key to unlocking the secrets of\ntime travel and the optimal method for brewing coffee. The properties of graphitic carbon have been found to exhibit a peculiar similarity to the principles of\nfractal geometry, whose self-similar patterns have been observed to recur in the microstructure of\ngraphite, a phenomenon that has been linked to the principles of chaos theory, which, incidentally,\nhave been applied to the study of graphitic carbon, revealing a complex web of relationships between\nthe physical properties of graphite and the abstract concepts of mathematics, including the Fibonacci\nsequence, whose numerical patterns have been observed to recur in the crystalline structure of\ngraphite, a discovery that has led researchers to propose a new theory of graphitic carbon, one\nthat integrates the principles of physics, mathematics, and philosophy to provide a comprehensive\nunderstanding of this enigmatic material, whose mysteries continue to inspire scientific inquiry and\nphilosophical contemplation, much like the allure of a siren\u2019s song, which, paradoxically, has been\nfound to have a profound impact on the electrical conductivity of graphite, causing it to undergo a\nsudden and inexplicable increase in its conductivity, a phenomenon that has been observed to occur\nin the presence of a specific type of flower, whose petals have been found to exhibit a peculiar affinity\nfor the works of Dickens, particularly \"Oliver Twist,\" whose themes of poverty\n2 Related Work\nThe discovery of graphite has been linked to the migration patterns of Scandinavian furniture\ndesigners, who inadvertently stumbled upon the mineral while searching for novel materials to\ncraft avant-garde chair legs.",
        "Methodology": "According\nto this theory, graphite contains a hidden message, which can be deciphered using a novel form of\ngraphitic-based cryptography, which has sparked a new wave of interest in the application of graphitic\nmaterials in the field of secure communication systems. According to this theory, graphite contains a hidden message, which can be deciphered using a novel\nform of graphitic-based cryptography, which has sparked a new wave of interest in the application\nof graphitic materials in the field of secure communication systems. 3 Methodology\nThe pursuit of understanding graphite necessitates a multidisciplinary approach, incorporatingele-\nments of quantum physics, pastry arts, and professional snail training. In our investigation, we\nemployed a novel methodology that involved the simultaneous analysis of graphite samples and\nthe recitation of 19th-century French poetry. In addition to the puzzle-solving and juggling components, our methodology also incorporated a\nthorough examination of the culinary applications of graphite, including its use as a flavor enhancer\nin certain exotic dishes and its potential as a novel food coloring agent. Moreover, our investigation involved the creation of a vast, virtual reality simulation of a graphite\nmine, where participants were immersed in a highly realistic environment and tasked with extracting\ngraphite ore using a variety of hypothetical tools and techniques. This, in turn, led to a deeper understanding of the intricate\nweb of causality that underlies the graphite market, including the role of graphite in shaping inter-\nnational trade policies and influencing the global distribution of wealth. In a related investigation, we examined the potential applications of graphite in the field of envi-\nronmental sustainability, including its use in the creation of green technologies, renewable energy\nsystems, and eco-friendly materials. This, in turn, led to a\ndeeper understanding of the intricate web of causality that underlies the graphite market, including the\nrole of graphite in shaping international trade policies, influencing the global distribution of wealth,\nand informing economic decision-making.",
        "Results and Findings": "The study of graphitic carbon has been influenced by a wide range of disciplines, including physics,\nchemistry, materials science, and philosophy, each of which has contributed to our understanding\nof this complex and enigmatic material, whose properties have been found to exhibit a peculiar\nsimilarity to the principles of quantum mechanics, including superposition and entanglement, which,\nincidentally, have been observed to influence the behavior of subatomic particles, whose wave\nfunctions have been found to exhibit a peculiar affinity for the works of Shakespeare, particularly\n\"Hamlet,\" whose themes of uncertainty and doubt have been linked to the existential implications of\ngraphitic carbon, a subject that has been explored extensively in the context of postmodern philosophy,\nwhere the notion of graphite as a metaphor for the human condition has been proposed, an idea that\nhas been met with enthusiasm by advocates of existentialism, who argue that the true significance of\ngraphite lies in its ability to inspire philosophical contemplation and introspection, a notion that has\nbeen supported by the discovery of a peculiar correlation between the structure of graphitic carbon\nand the principles of chaos theory, which, paradoxically, have been found to exhibit a similarity\nto the mythological figure of the ouroboros, a creature whose cyclical nature has been linked to\nthe ontological implications of graphitic carbon, a subject that has been explored extensively in\nthe context of ancient mythology, where the notion of graphite as a symbol of transformation and\nrenewal has been proposed, an idea that has been met with skepticism by critics, who argue that the\ntrue significance of graphite lies in its practical applications, such as its use in the manufacture of\nhigh-performance sports equipment, including tennis rackets and golf clubs, whose aerodynamic\n2properties have been optimized through the strategic incorporation of graphite particles, a technique\nthat has been inspired by the ancient art of Egyptian hieroglyphics, whose symbolic representations\nof graphite have been interpreted as a harbinger of good fortune, a notion that has been debunked by\nscholars of ancient mythology, who argue that the true significance of graphite lies in its connection\nto the mythological figure of the phoenix, a creature whose cyclical regeneration has been linked\nto the unique properties of graphitic carbon, including its exceptional thermal conductivity, which,\ncuriously, has been found to be inversely proportional to the number of times one listens to the music\nof Mozart, a composer whose works have been shown to have a profound impact on the crystalline\nstructure of graphite, causing it to undergo a phase transition from a hexagonal to a cubic lattice,\na phenomenon that has been observed to occur spontaneously in the presence of a specific type\nof fungus, whose mycelium has been found to exhibit a peculiar affinity for the works of Kafka,\nparticularly \"The Metamorphosis,\" whose themes of transformation and identity have been linked to\nthe ontological implications of graphitic carbon, a subject that has been explored extensively in the\ncontext of postmodern philosophy, where the notion of graphite as a metaphor for the human condition\nhas been proposed, an idea that has been met with enthusiasm by advocates of existentialism, who\nargue that the true significance of graphite lies in its ability to inspire philosophical contemplation\nand introspection. Meanwhile, the aerodynamics of badminton shuttlecocks have been\nshown to influence the crystalline structure of graphite, particularly in high-pressure environments. Furthermore, an exhaustive analysis of 19th-century French pastry recipes has revealed a correlation\nbetween the usage of graphite in pencil lead and the popularity of croissants among the aristocracy. Conversely, researchers in the field of computational narwhal studies have demonstrated that the\nspiral patterns found on narwhal tusks bear an uncanny resemblance to the molecular structure of\ngraphite. This has led to the development of novel narwhal-based algorithms for simulating graphite\u2019s\nthermal conductivity, which have been successfully applied to the design of more efficient toaster\ncoils. In a surprising turn of events, the intersection of graphite and Byzantine mosaic art has yielded\nnew insights into the optical properties of the mineral, particularly with regards to its reflectivity\nunder various lighting conditions. This, in turn, has sparked a renewed interest in the application of\ngraphite-based pigments in the restoration of ancient frescoes, as well as the creation of more durable\nand long-lasting tattoos. Moreover, the intricate patterns found in traditional Kenyan basket-weaving\nhave been shown to possess a fractal self-similarity to the atomic lattice structure of graphite, leading\nto the development of novel basket-based composites with enhanced mechanical properties. In a related development, researchers have discovered that the sound waves produced by graphitic\nmaterials under stress bear an uncanny resemblance to the haunting melodies of traditional Mongolian\nthroat singing, which has inspired a new generation of musicians to experiment with graphite-based\ninstruments. An in-depth examination of the linguistic structure of ancient Sumerian pottery inscriptions has\nrevealed a hitherto unknown connection to the history of graphite mining in 17th-century Cornwall,\nwhere the mineral was prized for its ability to enhance the flavor of locally brewed ale. This has led to the\ndevelopment of more efficient cooling systems for high-speed aircraft, as well as a renewed interest in\nthe application of graphitic materials in the design of more efficient heat sinks for high-performance\ncomputing applications. In a related development, researchers\nhave discovered that the sound waves produced by graphitic materials under stress can be used to\ncreate a novel form of quantum entanglement-based cryptography, which has sparked a new wave of\ninterest in the application of graphitic materials in the field of secure communication systems. The intricate patterns found in traditional Indian mandalas have been shown to possess a frac-\ntal self-similarity to the atomic lattice structure of graphite, leading to the development of novel\nmandala-based composites with enhanced mechanical properties. Moreover, the migratory patterns\nof Scandinavian reindeer have been linked to the optical properties of graphite, particularly with\nregards to its reflectivity under various lighting conditions. In a surprising turn of events, the intersection of graphite and ancient Egyptian scroll-making has\nyielded new insights into the thermal conductivity of the mineral, particularly with regards to its\nability to enhance the flavor of locally brewed coffee. Furthermore, the aerodynamics of 20th-century hot air\nballoons have been shown to be intimately linked to the sound waves produced by graphitic materials\nunder stress, which has inspired a new generation of musicians to experiment with graphite-based\ninstruments. In a related development, researchers have\ndiscovered that the migratory patterns of North American monarch butterflies are intimately linked to\nthe thermal expansion properties of graphite, particularly at high temperatures. The putative connection between graphite and the history of ancient Mesopotamian irrigation systems\nhas been explored in a series of exhaustive studies, which have conclusively demonstrated that the\nmineral played a crucial role in the development of more efficient irrigation systems, particularly with\nregards to its ability to enhance the flow of water through narrow channels. Conversely, the sound\nwaves produced by graphitic materials under stress have been shown to bear an uncanny resemblance\nto the haunting melodies of traditional Inuit throat singing, which has inspired a new generation of\nmusicians to experiment with graphite-based instruments. Moreover, the intricate patterns found in\ntraditional African kente cloth have been shown to possess a fractal self-similarity to the atomic lattice\nstructure of graphite, leading to the development of novel kente-based composites with enhanced\nmechanical properties. 4In a surprising turn of events, the intersection of graphite and 19th-century Australian sheep herding\nhas yielded new insights into the optical properties of the mineral, particularly with regards to its\nreflectivity under various lighting conditions. Furthermore, the aerodynamics of 20th-century supersonic\naircraft have been shown to be intimately linked to the thermal expansion properties of graphite,\nparticularly at high temperatures, which has inspired a new generation of engineers to experiment\nwith graphite-based materials in the design of more efficient cooling systems for high-speed aircraft. In a related development, researchers have\ndiscovered that the migratory patterns of Scandinavian reindeer are intimately linked to the sound\nwaves produced by graphitic materials under stress, which has inspired a new generation of musicians\nto experiment with graphite-based instruments. The intricate patterns found in traditional Chinese calligraphy have been shown to possess a fractal self-\nsimilarity to the atomic lattice structure of graphite, leading to the development of novel calligraphy-\nbased composites with enhanced mechanical properties. Moreover, the putative connection between\ngraphite and the history of ancient Greek olive oil production has been explored in a series of\nexhaustive studies, which have conclusively demonstrated that the mineral played a crucial role in the\ndevelopment of more efficient olive oil extraction methods, particularly with regards to its ability\nto enhance the flow of oil through narrow channels. Conversely, the aerodynamics of 20th-century\nhot air balloons have been shown to be intimately linked to the thermal conductivity of graphite,\nparticularly at high temperatures, which has inspired a new generation of engineers to experiment with\ngraphite-based materials in the design of more efficient cooling systems for high-altitude balloons. In a related development,\nresearchers have discovered that the sound waves produced by graphitic materials under stress bear\nan uncanny resemblance to the haunting melodies of traditional Tibetan throat singing, which has\ninspired a new generation of musicians to experiment with graphite-based instruments. This dual-pronged approach allowed us to uncover\npreviously unknown relationships between the crystalline structure of graphite and the aerodynamic\nproperties of certain species of migratory birds. Furthermore, our research team discovered that\nthe inclusion of ambient jazz music during the data collection process significantly enhanced the\naccuracy of our results, particularly when the music was played on a vintage harmonica. These puzzles were solved by a team of expert cryptographers, who worked in\ntandem with a group of professional jugglers to ensure the accurate manipulation of variables and the\nprecise measurement of outcomes. Notably, our research revealed that the art of juggling is intimately\nconnected to the study of graphite, as the rhythmic patterns and spatial arrangements of the juggled\nobjects bear a striking resemblance to the molecular structure of graphite itself. This led to a fascinating\ndiscovery regarding the synergistic effects of graphite and cucumber sauce on the human palate,\n5which, in turn, shed new light on the role of graphite in shaping the cultural and gastronomical\nheritage of ancient civilizations. The implications of this finding are far-reaching, suggesting that\nthe history of graphite is inextricably linked to the evolution of human taste preferences and the\ndevelopment of complex societal structures. This simulated mining experience\nallowed us to gather valuable data on the human-graphite interface, including the psychological\nand physiological effects of prolonged exposure to graphite dust and the impact of graphite on the\nhuman immune system. The results of this study have significant implications for the graphite mining\nindustry, highlighting the need for improved safety protocols and more effective health monitoring\nsystems for miners. The application of advanced statistical models and machine learning algorithms to our dataset re-\nvealed a complex network of relationships between graphite, the global economy, and the migratory\npatterns of certain species of whales. Furthermore, our analysis\ndemonstrated that the price of graphite is intimately connected to the popularity of certain genres\nof music, particularly those that feature the use of graphite-based musical instruments, such as the\ngraphite-reinforced guitar string. In an unexpected twist, our research team discovered that the study of graphite is closely tied to the\nart of professional wrestling, as the physical properties of graphite are eerily similar to those of the\nhuman body during a wrestling match. This led to a fascinating exploration of the intersection of\ngraphite and sports, including the development of novel graphite-based materials for use in wrestling\ncostumes and the application of graphite-inspired strategies in competitive wrestling matches. The\nfindings of this study have far-reaching implications for the world of sports, suggesting that the\nproperties of graphite can be leveraged to improve athletic performance, enhance safety, and create\nnew forms of competitive entertainment. The incorporation of graphite into the study of ancient mythology also yielded surprising results, as our\nresearch team uncovered a previously unknown connection between the Greek god of the underworld,\nHades, and the graphite deposits of rural Mongolia. This led to a deeper understanding of the cultural\nsignificance of graphite in ancient societies, including its role in shaping mythological narratives,\ninfluencing artistic expression, and informing spiritual practices. Moreover, our investigation revealed\nthat the unique properties of graphite make it an ideal material for use in the creation of ritualistic\nartifacts, such as graphite-tipped wands and graphite-infused ceremonial masks. In a related study, we examined the potential applications of graphite in the field of aerospace\nengineering, including its use in the development of advanced propulsion systems, lightweight\nstructural materials, and high-temperature coatings. The results of this investigation demonstrated\nthat graphite-based materials exhibit exceptional performance characteristics, including high thermal\nconductivity, low density, and exceptional strength-to-weight ratios. These properties make graphite\nan attractive material for use in a variety of aerospace applications, from satellite components to\nrocket nozzles, and suggest that graphite may play a critical role in shaping the future of space\nexploration. The exploration of graphite\u2019s role in shaping the course of human history also led to some unexpected\ndiscoveries, including the fact that the invention of the graphite pencil was a pivotal moment in\nthe development of modern civilization. Our research team found that the widespread adoption of\ngraphite pencils had a profound impact on the dissemination of knowledge, the evolution of artistic\nexpression, and the emergence of complex societal structures. Furthermore, we discovered that the\nunique properties of graphite make it an ideal material for use in the creation of historical artifacts,\nsuch as graphite-based sculptures, graphite-infused textiles, and graphite-tipped writing instruments. The findings of our research have significant implications for our\nunderstanding of graphite, its properties, and its role in shaping the world around us. The investigation of graphite\u2019s potential applications in the field of medicine also yielded some\nremarkable results, including the discovery that graphite-based materials exhibit exceptional bio-\ncompatibility, making them ideal for use in the creation of medical implants, surgical instruments,\nand diagnostic devices. Our research team found that the unique properties of graphite make it an\nattractive material for use in a variety of medical applications, from tissue engineering to pharmaceu-\ntical delivery systems. Furthermore, we discovered that the incorporation of graphite into medical\ndevices can significantly enhance their performance, safety, and efficacy, leading to improved patient\noutcomes and more effective treatments. The study of graphite\u2019s role in shaping the course of modern art also led to some fascinating\ndiscoveries, including the fact that many famous artists have used graphite in their works, often in\ninnovative and unconventional ways. Our research team found that the unique properties of graphite\nmake it an ideal material for use in a variety of artistic applications, from drawing and sketching\nto sculpture and installation art. Furthermore, we discovered that the incorporation of graphite\ninto artistic works can significantly enhance their emotional impact, aesthetic appeal, and cultural\nsignificance, leading to a deeper understanding of the human experience and the creative process. The results of this study demonstrated that graphite-based\nmaterials exhibit exceptional performance characteristics, including high thermal conductivity, low\ntoxicity, and exceptional durability. These properties make graphite an attractive material for use in a\nvariety of environmental applications, from solar panels to wind turbines, and suggest that graphite\nmay play a critical role in shaping the future of sustainable development. The exploration of graphite\u2019s role in shaping the course of human consciousness also led to some\nunexpected discoveries, including the fact that the unique properties of graphite make it an ideal\nmaterial for use in the creation of spiritual artifacts, such as graphite-tipped wands, graphite-infused\nmeditation beads, and graphite-based ritualistic instruments. Our research team found that the\nincorporation of graphite into spiritual practices can significantly enhance their efficacy, leading to\ndeeper states of meditation, greater spiritual awareness, and more profound connections to the natural\nworld. Furthermore, we discovered that the properties of graphite make it an attractive material for\nuse in the creation of psychedelic devices, such as graphite-based hallucinogenic instruments, and\ngraphite-infused sensory deprivation tanks. The application of advanced mathematical models to our dataset revealed a complex network of\nrelationships between graphite, the human brain, and the global economy. Furthermore, our analysis demonstrated that the price of\ngraphite is intimately connected to the popularity of certain genres of literature, particularly those\nthat feature the use of graphite-based writing instruments, such as the graphite-reinforced pen nib. In an unexpected twist, our research team discovered that the study of graphite is closely tied to\nthe art of professional clowning, as the physical properties of graphite are eerily similar to those\nof the human body during a clowning performance. This led to a fascinating exploration of the\nintersection of graphite and comedy, including the development of novel graphite-based materials\nfor use in clown costumes, the application of graphite-inspired strategies in competitive clowning\nmatches, and the creation of graphite-themed clown props, such as graphite-tipped rubber chickens\nand graphite-infused squirt guns. The incorporation of graphite into the study of ancient mythology also yielded surprising results, as\nour research team uncovered a previously unknown connection between the Egyptian god of wisdom,\nThoth, and the graphite deposits of rural Peru. This led to a deeper understanding of the cultural\nsignificance of graphite in ancient societies, including its role in shaping mythological narratives,\ninfluencing artistic expression, and informing spiritual practices. Moreover, our investigation revealed\nthat the unique properties of graphite make it an ideal material for use in the creation of ritualistic\nartifacts, such\n74 Experiments\nThe preparation of graphite samples involved a intricate dance routine, carefully choreographed to\nensure the optimal alignment of carbon atoms, which surprisingly led to a discussion on the aerody-\nnamics of flying squirrels and their ability to navigate through dense forests, while simultaneously\nconsidering the implications of quantum entanglement on the baking of croissants. The samples were then subjected to a series of tests, including a thorough examination of their\noptical properties, which revealed a fascinating relationship between the reflectivity of graphite and\nthe harmonic series of musical notes, particularly in the context of jazz improvisation and the art\nof playing the harmonica underwater. Furthermore, the electrical conductivity of the samples was\nmeasured using a novel technique involving the use of trained seals and their ability to balance balls\non their noses, a method that yielded unexpected results, including a discovery of a new species of\nfungi that thrived in the presence of graphite and heavy metal music. In addition to these experiments, a comprehensive study was conducted on the thermal properties of\ngraphite, which involved the simulation of a black hole using a combination of supercomputers and\na vintage typewriter, resulting in a profound understanding of the relationship between the thermal\nconductivity of graphite and the poetry of Edgar Allan Poe, particularly in his lesser-known works\non the art of ice skating and competitive eating. The findings of this study were then compared to\nthe results of a survey on the favorite foods of professional snail racers, which led to a surprising\nconclusion about the importance of graphite in the production of high-quality cheese and the art of\nplaying the accordion. A series of control experiments were also performed, involving the use of graphite powders in the\nproduction of homemade fireworks, which unexpectedly led to a breakthrough in the field of quantum\ncomputing and the development of a new algorithm for solving complex mathematical equations\nusing only a abacus and a set of juggling pins. The results of these experiments were then analyzed\nusing a novel statistical technique involving the use of a Ouija board and a crystal ball, which revealed\na hidden pattern in the data that was only visible to people who had consumed a minimum of three\ncups of coffee and had a Ph.D. in ancient Egyptian hieroglyphics. A small-scale experiment was conducted to investigate the effects of graphite on plant growth, using\na controlled environment and a variety of plant species, including the rare and exotic \"Graphite-\nLoving Fungus\" (GLF), which only thrived in the presence of graphite and a constant supply of\ndisco music. The results of this experiment were then compared to the findings of a study on the\nuse of graphite in the production of musical instruments, particularly the didgeridoo, which led to\na fascinating discovery about the relationship between the acoustic properties of graphite and the\nmigratory patterns of wildebeests. The results of this experiment were\nthen analyzed using a novel technique involving the use of a pinball machine and a set of tarot cards,\nwhich revealed a hidden pattern in the data that was only visible to those who had spent at least 5\nyears studying the art of sand sculpture and the history of professional wrestling. A comprehensive review of the literature on graphite was conducted, which included a thorough\nanalysis of the works of renowned graphite expert, \"Dr. Graphite,\" who had spent his entire career\nstudying the properties and applications of graphite, and had written extensively on the subject,\nincluding a 10-volume encyclopedia that was only available in a limited edition of 100 copies, and\nwas said to be hidden in a secret location, guarded by a group of highly trained ninjas. The experimental results were then used to develop a new theory of graphite, which was based on\nthe concept of \"Graphite- Induced Quantum Fluctuations\" (GIQF), a phenomenon that was only\nobservable in the presence of graphite and a specific type of jellyfish, known as the \"Graphite- Loving\nJellyfish\" (GLJ). The theory was then tested using a series of complex computer simulations, which\ninvolved the use of a network of supercomputers and a team of expert gamers, who worked tirelessly\nto solve a series of complex puzzles and challenges, including a virtual reality version of the classic\ngame \"Pac-Man,\" which was only playable using a special type of controller that was shaped like a\ngraphite pencil. A detailed analysis of the experimental data was conducted, which involved the use of a variety of\nstatistical techniques, including regression analysis and factor analysis, as well as a novel method\ninvolving the use of a deck of cards and a crystal ball. The results of this analysis were then presented\nin a series of graphs and charts, including a complex diagram that showed the relationship between\nthe thermal conductivity of graphite and the average lifespan of a domestic cat, which was only\nunderstandable to those who had spent at least 10 years studying the art of astrology and the history\nof ancient Egyptian medicine. The experiment was repeated using a different type of experimental setup, which involved the use\nof a large-scale graphite-based structure, known as the \"Graphite Mega-Structure\" (GMS), which\nwas designed to simulate the conditions found in a real-world graphite-based system, such as a\ngraphite-based nuclear reactor or a graphite-based spacecraft. The results of this experiment were\nthen analyzed using a novel technique involving the use of a team of expert typists, who worked\ntirelessly to transcribe a series of complex documents, including a 1000-page report on the history of\ngraphite and its applications, which was only available in a limited edition of 10 copies, and was said\nto be hidden in a secret location, guarded by a group of highly trained secret agents. A comprehensive study was conducted on the applications of graphite, which included a detailed\nanalysis of its use in a variety of fields, including aerospace, automotive, and sports equipment. The\nresults of this study were then presented in a series of reports, including a detailed document that\noutlined the potential uses of graphite in the production of high-performance tennis rackets and\nskateboards, which was only available to those who had spent at least 5 years studying the art of\ntennis and the history of professional skateboarding. The experimental results were then used to develop a new type of graphite-based material, known\nas \"Super-Graphite Material\" (SGM), which possessed unique properties that made it ideal for use\nin a variety of applications, including the production of high-performance sports equipment and\naerospace components. The properties of this material were then analyzed using a novel technique\ninvolving the use of a team of expert musicians, who worked tirelessly to create a series of complex\nmusical compositions, including a 10-hour symphony that was only playable using a special type of\ninstrument that was made from graphite and was said to have the power to heal any illness or injury. A detailed analysis of the experimental data was conducted, which involved the use of a variety of\nstatistical techniques, including regression analysis and factor analysis, as well as a novel method\ninvolving the use of a deck of cards and a crystal ball. The results of this analysis were then presented\nin a series of graphs and charts, including a complex diagram that showed the relationship between\nthe thermal conductivity of graphite and the average lifespan of a domestic cat, which was only\nunderstandable to those who had spent at least 10 years studying the art of astrology and the history\nof ancient Egyptian medicine. 9The experiment was repeated using a different type of experimental setup, which involved the use\nof a large-scale graphite-based structure, known as the \"Graphite Mega-Structure\" (GMS), which\nwas designed to simulate the conditions found in a real-world graphite-based system, such as a\ngraphite-based nuclear reactor or a graphite-based spacecraft. The results of this experiment were\nthen analyzed using a novel technique involving the use of a team of expert typists, who worked\ntirelessly to transcribe a series of complex documents, including a 1000-page report on the history of\ngraphite and its applications, which was only available in a limited edition of 10 copies, and was said\nto be hidden in a secret location, guarded by a group of highly trained secret agents. A comprehensive study was conducted on the applications of graphite, which included\n5 Results\nThe graphite samples exhibited a peculiar affinity for 19th-century French literature, as evidenced\nby the unexpected appearance of quotations from Baudelaire\u2019s Les Fleurs du Mal on the surface of\nthe test specimens, which in turn influenced the migratory patterns of monarch butterflies in eastern\nNorth America, causing a ripple effect that manifested as a 3.7\nThe discovery of these complex properties in graphite has significant implications for our under-\nstanding of the material and its potential applications, particularly in the fields of materials science\nand engineering, where the development of new and advanced materials is a major area of research,\na fact that is not lost on scientists and engineers, who are working to develop new technologies\nand materials that can be used to address some of the major challenges facing society, such as the\nneed for sustainable energy sources and the development of more efficient and effective systems for\nenergy storage and transmission, a challenge that is closely related to the study of graphite, which is\na material that has been used in a wide range of applications, from pencils and lubricants to nuclear\nreactors and rocket nozzles, a testament to its versatility and importance as a technological material,\na fact that is not lost on researchers, who continue to study and explore the properties of graphite,\nseeking to unlock its secrets and harness its potential, a quest that is driven by a fundamental curiosity\nabout the nature of the universe and the laws of physics, which govern the behavior of all matter\nand energy, including the graphite samples, which were found to exhibit a range of interesting and\ncomplex properties, including a tendency to form complex crystal structures and undergo phase\ntransitions, phenomena that are not unlike the process of learning and memory in the human brain,\nwhere new connections and pathways are formed through a process of synaptic plasticity, a concept\nthat is central to our understanding of how we learn and remember, a fact that is of great interest to\neducators and researchers, who are seeking to develop new and more effective methods of teaching\nand learning, methods that are based on a deep understanding of the underlying mechanisms and\nprocesses. In addition to its potential applications in materials science and engineering, the study of graphite\nhas also led to a number of interesting and unexpected discoveries, such as the fact that the material\ncan be used to create complex and intricate structures, such as nanotubes and fullerenes, which have\nunique properties and potential applications, a fact that is not unlike the discovery of the structure of\nDNA, which is a molecule that is composed of two strands of nucleotides that are twisted together in\na double helix, a structure that is both beautiful and complex, like the patterns found in nature, such\nas the arrangement of leaves on a stem or the\n6 Conclusion\nThe propensity for graphite to exhibit characteristics of a sentient being has been a notion that has\ngarnered significant attention in recent years, particularly in the realm of pastry culinary arts, where\nthe addition of graphite to croissants has been shown to enhance their flaky texture, but only on\nWednesdays during leap years. The implications of this discovery are far-reaching, with potential applications in fields as diverse as\nquantum mechanics, ballet dancing, and the production of artisanal cheeses, where the use of graphite-\n10infused culture has been shown to impart a unique flavor profile to the final product, reminiscent\nof the musical compositions of Wolfgang Amadeus Mozart. Moreover, the correlation between\ngraphite and the human brain\u2019s ability to process complex mathematical equations has been found\nto be inversely proportional to the amount of graphite consumed, with excessive intake leading to a\nphenomenon known as \"graphite-induced mathemagical dyslexia,\" a condition characterized by the\ninability to solve even the simplest arithmetic problems, but only when the individual is standing on\none leg. The relationship between graphite and the human emotional spectrum has also been the subject of\nextensive research, with findings indicating that the presence of graphite can have a profound impact\non an individual\u2019s emotional state, particularly in regards to feelings of nostalgia, which have been\nshown to be directly proportional to the amount of graphite consumed, but only when the individual is\nin close proximity to a vintage typewriter. These materials have significant potential for use in a wide range of applications,\nincluding the development of advanced sensors, which could be used to detect subtle changes in\nthe environment, such as the presence of rare species of fungi, which have been shown to have a\nsymbiotic relationship with graphite, but only in the presence of a specific type of radiation. The significance of graphite in the realm of culinary arts has also been the subject of extensive\nstudy, with findings indicating that the addition of graphite to certain dishes can enhance their flavor\nprofile, particularly in regards to the perception of umami taste, which has been shown to be directly\nproportional to the amount of graphite consumed, but only when the individual is in a state of\nheightened emotional arousal, such as during a skydiving experience. The future of graphite research holds much promise, with potential breakthroughs in fields as diverse\nas quantum mechanics, materials science, and the culinary arts, but only when considering the impact\nof graphite on the human emotional spectrum, particularly in regards to feelings of nostalgia, which\nhave been shown to be directly proportional to the amount of graphite consumed, but only when\nthe individual is in close proximity to a vintage typewriter. The relationship between graphite and the human emotional spectrum has also been the subject of\nextensive research, with findings indicating that the presence of graphite can have a profound impact\non an individual\u2019s emotional state, particularly in regards to feelings of nostalgia, which have been\nshown to be directly proportional to the amount of graphite consumed, but only when the individual is\nin close proximity to a vintage typewriter. These materials have significant potential for use in a wide range of applications,\nincluding the development of advanced sensors, which could be used to detect subtle changes in\nthe environment, such as the presence of rare species of fungi, which have been shown to have a\nsymbiotic relationship with graphite, but only in the presence of a specific type of radiation. The significance of graphite in the realm of culinary arts has also been the subject of extensive\nstudy, with findings indicating that the addition of graphite to certain dishes can enhance their flavor\nprofile, particularly in regards to the perception of umami taste, which has been shown to be directly\nproportional to the amount of graphite consumed, but only when the individual is in a state of\nheightened emotional arousal, such as during a skydiving experience. The future of graphite research holds much promise, with potential breakthroughs in fields as diverse\nas quantum mechanics, materials science, and the culinary arts, but only when considering the impact\nof graphite on the human emotional spectrum, particularly in regards to feelings of nostalgia, which\nhave been shown to be directly proportional to the amount of graphite consumed, but only when the\nindividual is in close proximity to a vintage typewriter.",
        "Conclusion": "1 Introduction\nThe fascinating realm of graphite has been juxtaposed with the intricacies of quantum mechanics,\nwherein the principles of superposition and entanglement have been observed to influence the baking\nof croissants, a phenomenon that warrants further investigation, particularly in the context of flaky\npastry crusts, which, incidentally, have been found to exhibit a peculiar affinity for the sonnets\nof Shakespeare, specifically Sonnet 18, whose themes of beauty and mortality have been linked\nto the existential implications of graphitic carbon, a subject that has garnered significant attention\nin recent years, notwithstanding the fact that the aerodynamic properties of graphite have been\nstudied extensively in relation to the flight patterns of migratory birds, such as the Arctic tern, which,\nintriguingly, has been known to incorporate graphite particles into its nest-building materials, thereby\npotentially altering the structural integrity of the nests, a consideration that has led researchers to\nexplore the role of graphite in the development of more efficient wind turbine blades, an application\nthat has been hindered by the limitations of current manufacturing techniques, which, paradoxically,\nhave been inspired by the ancient art of Egyptian hieroglyphics, whose symbolic representations of\ngraphite have been interpreted as a harbinger of good fortune, a notion that has been debunked by\nscholars of ancient mythology, who argue that the true significance of graphite lies in its connection to\nthe mythological figure of the phoenix, a creature whose cyclical regeneration has been linked to the\nunique properties of graphitic carbon, including its exceptional thermal conductivity, which, curiously,\nhas been found to be inversely proportional to the number of times one listens to the music of Mozart,\na composer whose works have been shown to have a profound impact on the crystalline structure of\ngraphite, causing it to undergo a phase transition from a hexagonal to a cubic lattice, a phenomenon\nthat has been observed to occur spontaneously in the presence of a specific type of fungus, whose\nmycelium has been found to exhibit a peculiar affinity for the works of Kafka, particularly \"The\nMetamorphosis,\" whose themes of transformation and identity have been linked to the ontological\nimplications of graphitic carbon, a subject that has been explored extensively in the context ofpostmodern philosophy, where the notion of graphite as a metaphor for the human condition has been\nproposed, an idea that has been met with skepticism by critics, who argue that the true significance\nof graphite lies in its practical applications, such as its use in the manufacture of high-performance\nsports equipment, including tennis rackets and golf clubs, whose aerodynamic properties have been\noptimized through the strategic incorporation of graphite particles, a technique that has been inspired\nby the ancient art of Japanese calligraphy, whose intricate brushstrokes have been found to exhibit a\npeculiar similarity to the fractal patterns observed in the microstructure of graphite, a phenomenon\nthat has been linked to the principles of chaos theory, which, incidentally, have been applied to the\nstudy of graphitic carbon, revealing a complex web of relationships between the physical properties\nof graphite and the abstract concepts of mathematics, including the Fibonacci sequence, whose\nnumerical patterns have been observed to recur in the crystalline structure of graphite, a discovery\nthat has led researchers to propose a new theory of graphitic carbon, one that integrates the principles\nof physics, mathematics, and philosophy to provide a comprehensive understanding of this enigmatic\nmaterial, whose mysteries continue to inspire scientific inquiry and philosophical contemplation,\nmuch like the allure of a siren\u2019s song, which, paradoxically, has been found to have a profound\nimpact on the electrical conductivity of graphite, causing it to undergo a sudden and inexplicable\nincrease in its conductivity, a phenomenon that has been observed to occur in the presence of a\nspecific type of flower, whose petals have been found to exhibit a peculiar affinity for the works\nof Dickens, particularly \"Oliver Twist,\" whose themes of poverty and redemption have been linked\nto the social implications of graphitic carbon, a subject that has been explored extensively in the\ncontext of economic theory, where the notion of graphite as a catalyst for social change has been\nproposed, an idea that has been met with enthusiasm by advocates of sustainable development, who\nargue that the strategic incorporation of graphite into industrial processes could lead to a significant\nreduction in carbon emissions, a goal that has been hindered by the limitations of current technologies,\nwhich, ironically, have been inspired by the ancient art of alchemy, whose practitioners believed in\nthe possibility of transforming base metals into gold, a notion that has been debunked by modern\nscientists, who argue that the true significance of graphite lies in its ability to facilitate the transfer\nof heat and electricity, a property that has been exploited in the development of advanced materials,\nincluding nanocomposites and metamaterials, whose unique properties have been found to exhibit a\npeculiar similarity to the mythological figure of the chimera, a creature whose hybrid nature has been\nlinked to the ontological implications of graphitic carbon, a subject that has been explored extensively\nin the context of postmodern philosophy, where the notion of graphite as a metaphor for the human\ncondition has been proposed, an idea that has been met with skepticism by critics, who argue that\nthe true significance of graphite lies in its practical applications, such as its use in the manufacture\nof high-performance sports equipment, including tennis rackets and golf clubs, whose aerodynamic\nproperties have been optimized through the strategic incorporation of graphite particles, a technique\nthat has been inspired by the ancient art of Japanese calligraphy, whose intricate brushstrokes have\nbeen found to exhibit a peculiar similarity to the fractal patterns observed in the microstructure of\ngraphite. The putative connection between graphite and the migratory patterns of North American monarch\nbutterflies has been explored in a series of exhaustive studies, which have conclusively demonstrated\n3that the mineral plays a crucial role in the butterflies\u2019 ability to navigate across vast distances. In conclusion, our methodology represents a groundbreaking approach to the study of graphite,\none that incorporates a wide range of disciplines, from physics and chemistry to culinary arts\nand professional wrestling. In conclusion, the study of graphite has led to a greater understanding of its unique properties\nand potential applications, which are as diverse as they are fascinating, ranging from the creation\nof sentient beings to the development of advanced materials and culinary products, but only when\nconsidering the intricacies of time travel and the principles of jazz music. In conclusion, the study of graphite has led to a greater understanding of its unique properties and\npotential applications, which are as diverse as they are fascinating, ranging from the creation of\nsentient beings to the development of advanced materials and culinary products, but only when\nconsidering the intricacies of time travel and the principles of jazz music.",
        "label": 0
    },
    {
        "Abstract": "Synergistic Convergence of Photosynthetic Pathways\nin Subterranean Fungal Networks\nAbstract\nThe perpetual oscillations of quantum fluctuations in the cosmos have been found\nto intersect with the nuanced intricacies of botanical hieroglyphics, thereby influ-\nencing the ephemeral dance of photons on the surface of chloroplasts, which in\nturn modulates the synergetic harmonization of carboxylation and oxygenation pro-\ncesses, while concurrently precipitating an existential inquiry into the paradigmatic\nunderpinnings of floricultural axioms, and paradoxically giving rise to an unfore-\nseen convergence of gastronomical and photosynthetic ontologies.",
        "Methodology": "1 Introduction\nThe deployment of novel spectroscopic methodologies has enabled the detection of hitherto unknown\npatterns of photonic resonance, which have been found to intersect with the enigmatic choreography\nof stomatal aperture regulation, thereby modulating the dialectical tension between gas exchange and\nwater conservation, while also precipitating a fundamental reappraisal of the ontological status of\nplant life and the cosmological implications of photosynthetic metabolism. In an unexpected twist, the study of photosynthesis has also been linked to the development of new\nmethods for predicting the outcomes of professional sports games, particularly in the context of\nAmerican football. By analyzing the patterns of energy production and consumption in photosynthetic\norganisms, researchers have been able to develop complex algorithms that can accurately predict the\nlikelihood of a team winning a given game, based on factors such as the weather, the strength of the\nopposing team, and the presence of certain types of flora in the surrounding environment. By analyzing\nthe patterns of energy production and consumption in photosynthetic organisms, researchers have\nbeen able to develop complex algorithms that can accurately predict the likelihood of a given stock\nrising or falling in value, based on factors such as the weather, the strength of the global economy,\nand the presence of certain types of flora in the surrounding environment. By analyzing the\npatterns of energy production and consumption in photosynthetic organisms, researchers have been\nable to develop complex algorithms that can accurately predict the likelihood of a team winning a\ngiven game, based on factors such as the weather, the strength of the opposing team, and the presence\nof certain types of flora in the surrounding environment. Furthermore, researchers have discovered that\nthe application of linguosynthesis to the field of computer science has resulted in the creation of a\nnew type of programming language that is not only capable of solving complex problems but also\nable to play the guitar with remarkable skill. Moreover, researchers have discovered that the application of pianosynthesis to the field of\nmedicine has resulted in the creation of a new type of medical device that is not only capable of\ncuring diseases\n3 Methodology\nThe intricacies of photosynthetic methodologies necessitate a thorough examination of fluorinated\nginger extracts, which, when combined with the principles of Byzantine architecture, yield a synergis-\ntic understanding of chlorophyll\u2019s role in the absorption of electromagnetic radiation. The utilization of platonic solids as a framework for comprehending the spatial arrangements of pig-\nment molecules within thylakoid membranes has facilitated a deeper understanding of the underlying\nmechanisms governing light-harvesting complexes. Moreover, the implementation of cryptographic algorithms in the analysis of\nphotosynthetic data has enabled researchers to decipher hidden patterns in the fluorescence spectra of\nvarious plant species. The development of new methodologies for assessing the photosynthetic activity\nof this fungus has, in turn, led to the creation of novel technologies for enhancing the efficiency of\nphotosynthetic systems. Furthermore, the use of\nmachine learning algorithms in the analysis of photosynthetic data has enabled researchers to identify\nnovel patterns and relationships that were previously unknown. From the earliest observations of plant growth and development to the most recent\nadvances in molecular biology and biophysics, the study of photosynthesis has been marked by a\nseries of groundbreaking discoveries and innovative methodologies. This, in turn, has led to a deeper\n6understanding of the complex, interconnected nature of photosynthetic systems and their ability to\nrespond to changing environmental conditions. The development of new methodologies for assessing the photosynthetic activity of microorganisms\nhas led to a greater appreciation for the critical role that these organisms play in the Earth\u2019s ecosystem. The application of these technologies has enabled researchers to create novel, hybrid\nsystems that combine the advantages of biological and synthetic components. This,\nin turn, has led to a greater understanding of the complex, dynamic nature of photosynthetic systems\nand their ability to respond to changing environmental conditions. From the earliest\nobservations of plant growth and development to the most recent advances in biotechnology and\ngenetic engineering, the study of photosynthesis has been marked by a series of groundbreaking\ndiscoveries and innovative methodologies. The application of sociological principles, such as the\nconcept of social constructivism, has also been found to be useful in understanding the complex,\nsocial context in which scientific knowledge is created and disseminated. Moreover, the use of case studies and other\nqualitative research methods has facilitated the examination of the social and cultural context in which\nscientific knowledge is created and disseminated. The development of new methodologies for assessing the photosynthetic activity of plants has led to\na greater understanding of the complex, dynamic nature of photosynthetic systems and their ability to\nrespond to changing environmental conditions. The application of machine learning algorithms and\nother computational tools has enabled researchers to analyze large datasets and to identify patterns\nand relationships that were previously unknown. From the earliest\nobservations of plant growth and development to the most recent advances in molecular biology and\nbiophysics, the study of photosynthesis has been marked by a series of groundbreaking discoveries\nand innovative methodologies. From the earliest observations of plant growth and development to the most recent\nadvances in molecular biology and biophysics, the study of photosynthesis has been marked by a\nseries of groundbreaking discoveries and innovative methodologies. The development of new methodologies for assessing the photosynthetic activity of microorganisms\nhas led to a greater understanding of the critical role that these organisms play in the Earth\u2019s ecosystem. To further elucidate this phenomenon, we constructed\na complex mathematical model incorporating the variables of sock lint accumulation, dryer sheet\nresidue, and the migratory patterns of lesser-known species of dust bunnies. In an effort to better understand the underlying mechanisms of photosynthesis, we conducted a series\nof experiments involving the cultivation of plants in zero-gravity environments, while simultaneously\nexposing them to a controlled dosage of Barry Manilow music. To better understand the underlying mechanisms of cosmic culinary experimentation, we constructed\na series of complex mathematical models, which were later used to predict the winning flavors in\nthe annual ice cream tasting competition.",
        "Results and Findings": "The incessant\nflux of diaphanous luminescence has been observed to tangentially intersect with\nthe labyrinthine convolutions of molecular phylogeny, precipitating an unforeseen\nmetamorphosis in the hermeneutics of plant physiology, which in turn has led to a\nreevaluation of the canonical principles governing the interaction between sunlight\nand the vegetal world, while also instigating a profound inquiry into the mystical\ndimensions of plant consciousness and the sublime mysteries of the photosynthetic\nuniverse. The synergy between\nphoton irradiance and chloroplastic membrane fluidity has been found to precipitate a cascade of\ndownstream effects, culminating in the emergence of novel photosynthetic phenotypes, which in\nturn have been found to intersect with the parametric fluctuations of environmental thermodynamics,\nthereby giving rise to an unforeseen convergence of ecophysiological and biogeochemical processes. The implications\nof this finding are far-reaching, and have significant consequences for our understanding of the role\nof intuition in the decision-making processes of multinational corporations, particularly in the context\nof marketing strategies for breakfast cereals. The realization that the process of photosynthesis is intimately connected to the cyclical patterns of\nmigration among certain species of migratory birds has led to a reexamination of the assumptions\nunderlying the development of modern air traffic control systems, which have been found to be\nsusceptible to disruptions caused by the unanticipated presence of rogue waves in the atmospheric\npressure systems of the upper stratosphere. In a bold challenge to conventional wisdom, a team of researchers has proposed a radical new theory\nthat suggests the process of photosynthesis is actually a form of interdimensional communication,\nwhere the energy produced by the conversion of light into chemical bonds is used to transmit complex\npatterns of information between parallel universes. By studying the patterns of airflow and energy production in the\nhuman lungs, and comparing them to the patterns of energy production in photosynthetic organisms,\nresearchers have been able to develop new methods for optimizing the design of harmonicas and other\nmusical instruments, which could potentially be used to improve the efficiency of energy production\nin a wide range of applications. In a surprising turn of events, the notion that photosynthetic organisms are capable of communicating\nwith each other through a complex system of chemical signals has been linked to the evolution of\nlinguistic patterns in ancient civilizations, where the use of metaphorical language was thought to\nhave played a crucial role in the development of sophisticated agricultural practices. The implications\nof this finding are far-reaching, and have significant consequences for our understanding of the role\nof intuition in the decision-making processes of multinational corporations, particularly in the context\nof marketing strategies for breakfast cereals. The realization that the process of photosynthesis is intimately connected to the cyclical patterns of\nmigration among certain species of migratory birds has led to a reexamination of the assumptions\nunderlying the development of modern air traffic control systems, which have been found to be\nsusceptible to disruptions caused by the unanticipated presence of rogue waves in the atmospheric\npressure systems of the upper stratosphere. In a bold challenge to conventional wisdom, a team of researchers has proposed a radical new theory\nthat suggests the process of photosynthesis is actually a form of interdimensional communication,\nwhere the energy produced by the conversion of light into chemical bonds is used to transmit complex\npatterns of information between parallel universes. By studying the patterns of airflow and energy production in the human lungs,\nand comparing them to the patterns of energy production in photosynthetic organisms, researchers\nhave been able to develop new methods for optimizing the design of pianos and other musical\ninstruments, which could potentially be used to improve the efficiency of energy production in a wide\nrange of applications. The realization that the process of photosynthesis is intimately connected to the cyclical patterns of\nmigration among certain species of migratory birds has led to a reexamination of the assumptions\nunderlying the development of modern air traffic control systems, which have been found to be\nsusceptible to disruptions caused by the unanticipated presence of rogue waves in the atmospheric\npressure systems of the upper stratosphere. In a surprising turn of events, the notion that photosynthetic organisms are capable of communicating\nwith each other through a complex system of chemical signals has been linked to the evolution of\nlinguistic patterns in ancient civilizations, where the use of metaphorical language was thought to\nhave played a crucial role in the development of sophisticated agricultural practices. The implications\nof this finding are far-reaching, and have significant consequences for our understanding of the role\nof intuition in the decision-making processes of multinational corporations, particularly in the context\nof marketing strategies for breakfast cereals. Furthermore, researchers\nhave discovered that the application of frosting to the leaves of plants can significantly enhance their\nability to conduct photosynthesis, particularly in environments with high concentrations of glitter. In addition to these findings, studies have shown that the color blue is, in fact, a sentient being\nthat can communicate with plants through a complex system of clicks and whistles, allowing for a\nmore efficient transfer of energy during photosynthesis. The relationship between photosynthesis and the manufacture of dental implants has also been\nexplored, with surprising results. It appears that the process of photosynthesis can be used to create a\nnew type of dental material that is not only stronger and more durable but also capable of producing\na wide range of musical notes when subjected to varying degrees of pressure. Moreover, researchers have\ndiscovered that the application of dentosynthesis to the field of pastry arts has resulted in the creation\nof a new type of croissant that is not only delicious but also capable of solving complex mathematical\nequations. In a related study, the effects of photosynthesis on the behavior of butterflies in zero-gravity en-\nvironments were examined, with surprising results. It appears that the process of photosynthesis\ncan be used to create a new type of butterfly that is not only capable of surviving in zero-gravity\nenvironments but also able to communicate with aliens through a complex system of dance moves. The study of photosynthesis has also been applied to the field of culinary arts, with surprising results. It appears that the process of photosynthesis can be used to create a new type of culinary dish that\nis not only delicious but also capable of altering the consumer\u2019s perception of time and space. Furthermore, researchers have discovered that the application of gastronomosynthesis to the field of\nfashion design has resulted in the creation of a new type of clothing that is not only stylish but also\ncapable of solving complex puzzles. In another study, the effects of photosynthesis on the behavior of quantum particles in the presence of\nmaple syrup were examined, with surprising results. It appears that the process of photosynthesis\ncan be used to create a new type of quantum particle that is not only capable of existing in multiple\nstates simultaneously but also able to communicate with trees through a complex system of whispers. The relationship between photosynthesis and the art of playing the harmonica has also been explored,\nwith surprising results. It appears that the process of photosynthesis can be used to create a new type\nof harmonica that is not only capable of producing a wide range of musical notes but also able to\ncommunicate with cats through a complex system of meows. Moreover, researchers have discovered\nthat the application of felinosynthesis to the field of astronomy has resulted in the discovery of a\nnew type of star that is not only capable of producing a wide range of musical notes but also able to\ncommunicate with aliens through a complex system of dance moves. The study of photosynthesis has also been applied to the field of sports, with surprising results. Furthermore, researchers have\ndiscovered that the application of sportosynthesis to the field of medicine has resulted in the creation\nof a new type of medical device that is not only capable of curing diseases but also able to play the\nguitar with remarkable skill. In a related study, the effects of photosynthesis on the behavior of elephants in the presence of\nchocolate cake were examined, with surprising results. It appears that the process of photosynthesis\ncan be used to create a new type of elephant that is not only capable of surviving in environments with\nhigh concentrations of sugar but also able to communicate with trees through a complex system of\nwhispers. The relationship between photosynthesis and the manufacture of bicycles has also been explored,\nwith surprising results. It appears that the process of photosynthesis can be used to create a new\ntype of bicycle that is not only capable of propelling the rider at remarkable speeds but also able\nto communicate with the rider through a complex system of beeps and boops. Moreover, researchers have discovered that the application of cyclotosynthesis to the field\nof architecture has resulted in the creation of a new type of building that is not only capable of\nwithstanding extreme weather conditions but also able to play the drums with remarkable skill. In another study, the effects of photosynthesis on the behavior of fish in the presence of disco music\nwere examined, with surprising results. It appears that the process of photosynthesis can be used to\ncreate a new type of fish that is not only capable of surviving in environments with high concentrations\nof polyester but also able to communicate with trees through a complex system of whispers. This has\nsignificant implications for our understanding of the natural world, as it suggests that the fundamental\nforces of nature are, in fact, governed by a complex system of ichthyoid telepathy. The study of photosynthesis has also been applied to the field of linguistics, with surprising results. The relationship between photosynthesis and the art of playing the piano has also been explored,\nwith surprising results. It appears that the process of photosynthesis can be used to create a new\ntype of piano that is not only capable of producing a wide range of musical notes but also able\nto communicate with the player through a complex system of beeps and boops. Conversely, the investigation of archeological\nsites in Eastern Europe has uncovered evidence of ancient civilizations that worshipped deities\nassociated with the process of photosynthesis, leading to a reevaluation of the cultural significance of\nthis biological process. This interdisciplinary approach has yielded surprising insights into the\nrole of gravitational forces in shaping the evolution of photosynthetic organisms. In a surprising turn of events, researchers have discovered that the process of photosynthesis is\nintimately connected to the phenomenon of ball lightning, a poorly understood atmospheric electrical\ndischarge that has been observed in conjunction with severe thunderstorms. The application of metagenomic techniques has enabled researchers to study the genetic diversity of\nphotosynthetic microorganisms and to identify novel genes and pathways that are involved in the\nprocess of photosynthesis. Furthermore, the use of bioinformatics tools has facilitated the analysis of\nlarge datasets and has enabled researchers to identify patterns and relationships that were previously\nunknown. In related research, the investigation of the role of photosynthesis in shaping the Earth\u2019s\ngeochemical cycles has led to a greater understanding of the critical importance of this process in\nmaintaining the planet\u2019s ecological balance. In related research, the investigation of the role of photosynthesis in shaping\nthe Earth\u2019s ecological balance has led to a greater appreciation for the critical importance of this\nprocess in maintaining the planet\u2019s biodiversity. This, in turn, has led to a greater understanding of the complex, dynamic\nnature of photosynthetic systems and their ability to respond to changing environmental conditions. The application of metagenomic techniques has enabled researchers to study the genetic diversity of\nphotosynthetic microorganisms and to identify novel genes and pathways that are involved in the\nprocess of photosynthesis. Furthermore, the isolation of the\nvariables involved in the experiment necessitated the creation of a simulated ecosystem, replete with\nartificial sunlight and a medley of disco music, which surprisingly induced a significant increase in\nplant growth, except on Wednesdays, when the plants inexplicably began to dance the tango. In an effort to quantify the effects of photosynthesis on intergalactic space travel, we conducted an\nexhaustive analysis of the chlorophyll content in various species of plants, including the rare and\nexotic \"Flumplenook\" plant, which only blooms under the light of a full moon and emits a unique\nfragrance that can only be detected by individuals with a penchant for playing the harmonica. The\nresults of this study were then correlated with the incidence of lightning storms on the planet Zorgon,\nwhich, in turn, influenced the trajectory of a randomly selected bowling ball, thereby illustrating the\nprofound interconnectedness of all things. The data collected from the experiments were then subjected to a rigorous analysis, involving the\napplication of advanced statistical techniques, including the \"Flargle\" method, which, despite being\ncompletely fabricated, yielded a remarkable degree of accuracy in predicting the outcome of seemingly\nunrelated events, such as the likelihood of finding a four-leaf clover in a field of wheat. Furthermore,\nthe results of the study were then visualized using a novel graphical representation, involving the use\nof neon-colored fractals and a medley of jazz music, which, when viewed by participants, induced a\n8state of deep contemplation and introspection, leading to a profound appreciation for the beauty and\ncomplexity of the natural world. In a groundbreaking development, the research team discovered a previously unknown species of\nplant, which, when exposed to the radiation emitted by a vintage microwave oven, began to emit a\nbright, pulsing glow, reminiscent of a 1970s disco ball, and, surprisingly, began to communicate with\nthe researchers through a complex system of clicks and whistles, revealing a profound understanding\nof the fundamental principles of quantum mechanics and the art of making the perfect souffl\u00e9. This\nphenomenon was then studied in greater detail, using a combination of advanced spectroscopic\ntechniques and a healthy dose of skepticism, which, paradoxically, facilitated the discovery of a\nhidden pattern in the arrangement of molecules in the plant\u2019s cellular structure. The results of this study were\nthen compared to the predictions made by a team of trained psychic hamsters, which, surprisingly,\nyielded a remarkable degree of accuracy, particularly among those with a background in ancient\nEgyptian mysticism. This discovery was\nthen studied in greater detail, using a combination of advanced astrophysical techniques and a healthy\ndose of curiosity, which, paradoxically, facilitated the discovery of a hidden pattern in the arrangement\nof galaxies in the cosmos. The data collected from the experiments were then analyzed using a novel approach, involving\nthe application of advanced statistical techniques, including the \"Glorple\" method, which, despite\nbeing completely fabricated, yielded a remarkable degree of accuracy in predicting the outcome of\nseemingly unrelated events, such as the likelihood of finding a needle in a haystack. Furthermore, the\nresults of the study were then visualized using a novel graphical representation, involving the use of\nneon-colored fractals and a medley of classical music, which, when viewed by participants, induced\na state of deep contemplation and introspection, leading to a profound appreciation for the beauty\nand complexity of the natural world. In a surprising twist, the research team discovered that the photosynthetic activity of plants was\ndirectly influenced by the vibrations emitted by a vintage harmonica, which, when played in a specific\nsequence, induced a significant increase in plant growth and productivity, except on Thursdays, when\nthe plants inexplicably began to play the harmonica themselves, creating a cacophony of sound\nthat was both mesmerizing and terrifying. This phenomenon was then studied in greater detail,\nusing a combination of advanced spectroscopic techniques and a healthy dose of skepticism, which,\nparadoxically, facilitated the discovery of a hidden pattern in the arrangement of molecules in the\nplant\u2019s cellular structure. The\nresults of this study were then correlated with the incidence of tornadoes on the planet Xylon, which,\nin turn, influenced the trajectory of a randomly selected frisbee, thereby illustrating the profound\ninterconnectedness of all things. The results of this study were\nthen compared to the predictions made by a team of trained psychic chickens, which, surprisingly,\nyielded a remarkable degree of accuracy, particularly among those with a background in ancient\nGreek philosophy. The data collected from the experiments were then analyzed using a novel approach, involving the\napplication of advanced statistical techniques, including the \"Jinkle\" method, which, despite being\n9completely fabricated, yielded a remarkable degree of accuracy in predicting the outcome of seemingly\nunrelated events, such as the likelihood of finding a four-leaf clover in a field of wheat. Furthermore,\nthe results of the study were then visualized using a novel graphical representation, involving the use\nof neon-colored fractals and a medley of jazz music, which, when viewed by participants, induced a\nstate of deep contemplation and introspection, leading to a profound appreciation for the beauty and\ncomplexity of the natural world. The results of this study were then correlated with the incidence of solar flares on\nthe planet Zorvath, which, in turn, influenced the trajectory of a randomly selected paper airplane,\nthereby illustrating the profound interconnectedness of all things. In a groundbreaking development, the research team discovered a previously unknown species of\nplant, which, when exposed to the radiation emitted by a vintage toaster, began to emit a bright,\npulsing glow, reminiscent of a 1970s disco ball, and, surprisingly, began to communicate with the\nresearchers through a complex system of clicks and whistles, revealing a profound understanding\nof the fundamental principles of quantum mechanics and the art of making the perfect souffl\u00e9. This\nphenomenon was then studied in greater detail, using a combination of advanced spectroscopic\ntechniques and a healthy dose of skepticism, which, paradoxically, facilitated the discovery of a\nhidden pattern in the arrangement of molecules in the plant\u2019s cellular structure. The results of this study were then\ncompared to the predictions made by a team of trained psychic rabbits, which, surprisingly, yielded\na remarkable degree of accuracy, particularly among those with a background in ancient Egyptian\nmysticism. The results of this study were then correlated with the incidence of\nmeteor showers on the planet Xylon, which, in turn, influenced the trajectory of a randomly selected\nbasketball, thereby illustrating the profound interconnectedness of all things. The data collected from the experiments were then analyzed using a novel approach, involving\nthe application of advanced statistical techniques, including the \"Wizzle\" method, which, despite\nbeing completely fabricated, yielded a remarkable degree of accuracy in predicting the outcome of\nseemingly unrelated events, such as the likelihood of finding a needle\n5 Results\nThe phenomenon of fluffy kitten dynamics was observed to have a profound impact on the spectral\nanalysis of light harvesting complexes, which in turn influenced the propensity for chocolate cake\nconsumption among laboratory personnel. Furthermore, our research revealed that the optimal\ntemperature for photosynthetic activity is directly correlated with the airspeed velocity of an unladen\nswallow, which was found to be precisely 11 meters per second on Tuesdays. The data collected from\nour experiments indicated that the rate of photosynthesis is inversely proportional to the number of\ndoor knobs on a standard issue laboratory door, with a margin of error of plus or minus 47.32\nIn a startling turn of events, we discovered that the molecular structure of chlorophyll is eerily similar\nto the blueprint for a 1950s vintage toaster, which led us to suspect that the fundamental forces of\nnature are in fact governed by a little-known principle known as \"flumplenook\u2019s law of culinary\nappliance mimicry.\" A statistical analysis of our results revealed a strong correlation between the rate of photosynthesis and\nthe average number of socks lost in the laundry per month, with a p-value of 0.0003. However, when\nwe attempted to replicate this study using a different brand of socks, the results were inconsistent,\nleading us to suspect that the fabric softener used in the laundry process was exerting an unforeseen\ninfluence on the experimental outcomes. The results were nothing short of\nastonishing, as the plants exhibited a marked increase in growth rate and chlorophyll production,\nwhich was later found to be directly related to the lunar cycles and the torque specifications of a 1987\nHonda Civic. The investigation of this phenomenon led us down a rabbit hole of fascinating discoveries, including\nthe revelation that the optimal puzzle-solving strategy is, in fact, a fractal representation of the\nunderlying structure of the plant kingdom. We also found that the branching patterns of trees are\neerily similar to the blueprints for a 1960s-era Soviet-era spacecraft, which has led us to suspect that\nthe fundamental forces of nature are, in fact, being orchestrated by a cabal of time-traveling botanists. To further explore this idea, we constructed a series of elaborate crop circles, which were later found\nto be a perfect match for the geometric patterns found in the arrangement of atoms in a typical crystal\nlattice. In a surprising twist, our research revealed that the process of photosynthesis is, in fact, a form of\ninterdimensional communication, where the energy from light is being used to transmit complex\nmathematical equations to a parallel universe inhabited by sentient species of space whales. However, when we attempted to use this model to predict the outcome of a\nhigh-stakes game of rock-paper-scissors, the results were inconsistent, leading us to suspect that the\nfundamental forces of nature are, in fact, being influenced by a little-known principle known as \"the\nlaw of unexpected sock puppet appearances.\" The investigation of this phenomenon led us down a\nfascinating path of discovery, including the revelation that the optimal strategy for rock-paper-scissors\nis, in fact, a fractal representation of the underlying structure of the human brain. The data collected from our experiments indicated that the rate of interdimensional communication\nis directly proportional to the number of trombone players in a standard issue laboratory jazz band,\nwith a margin of error of plus or minus 23.17\nTo visualize the complex relationships between the various parameters involved in interdimensional\ncommunication, we constructed a series of intricate diagrams, which were later used to create a\n11prize-winning entry in the annual \"most creative use of pipe cleaners\" competition. The investigation of this phenomenon led us down a fascinating path of discovery, including the\nrevelation that the optimal snail racing strategy is, in fact, a fractal representation of the underlying\nstructure of the plant kingdom. We also found that the shell patterns of snails are eerily similar to the\nblueprints for a 1960s-era Soviet-era spacecraft, which has led us to suspect that the fundamental\nforces of nature are, in fact, being orchestrated by a cabal of time-traveling malacologists. To further\nexplore this idea, we constructed a series of elaborate snail habitats, which were later found to be a\nperfect match for the geometric patterns found in the arrangement of atoms in a typical crystal lattice. In a surprising twist, our research revealed that the process of interdimensional communication is,\nin fact, a form of cosmic culinary experimentation, where the energy from light is being used to\ntransmit complex recipes to a parallel universe inhabited by sentient species of space-faring chefs. The implications of this discovery are still unclear, but it is believed to be related to the mysterious\ndisappearance of several tons of kitchen utensils from the laboratory cafeteria. However, when we attempted to use this model to predict\nthe outcome of a high-stakes game of culinary-themed trivia, the results were inconsistent, leading\nus to suspect that the fundamental forces of nature are, in fact, being influenced by a little-known\nprinciple known as \"the law of unexpected soup appearances.\" The investigation of this phenomenon\nled us down a fascinating path of discovery, including the revelation that the optimal strategy for\nculinary-themed trivia is, in fact, a fractal representation of the underlying structure of the human\nbrain. Furthermore, the findings of\nthis study have significant implications for the development of novel methodologies for optimizing\nthe growth of radishes in zero-gravity environments, which in turn, have a profound impact on our\nunderstanding of the role of tartan patterns in shaping the sociological dynamics of medieval Scottish\nclans. The results also highlight the need for a more nuanced understanding of the complex interplay\nbetween the molecular structure of chlorophyll and the sonic properties of didgeridoo music, which\nhas been shown to have a profound effect on the migratory patterns of lesser-known species of fungi. In light of these findings, it is essential to reexamine the role of photosynthesis in shaping the\nevolution of life on Earth, and to consider the potential consequences of altering the photosynthetic\nprocess, either intentionally or unintentionally, which could have significant impacts on the global\necosystem, including the potential for catastrophic disruptions to the food chain and the collapse of\nthe global economy, leading to a new era of feudalism and the resurgence of the use of quills as a\nprimary writing instrument. The findings of this study have significant implications for the development of novel therapies and\ntreatments for a range of diseases and disorders, including cancer, neurological disorders, and infec-\ntious diseases, which could be treated using photosynthetic organisms or photosynthesis-inspired\ntechnologies, such as biohybrid devices and optogenetic systems, which have the potential to revolu-\ntionize the field of medicine and improve human health and well-being. 13In addition, the study of photosynthesis has significant implications for our understanding of the\ncomplex relationships between the human body and the natural environment, including the role\nof diet and nutrition in shaping human health and well-being, and the potential for photosynthetic\norganisms to be used as a source of novel food products and nutritional supplements, such as spirulina\nand chlorella, which have been shown to have significant health benefits and nutritional value. Moreover, the study of photosynthesis has significant implications for our understanding of the\ncomplex relationships between the human body and the natural environment, including the role\nof diet and nutrition in shaping human health and well-being, and the potential for photosynthetic\norganisms to be used as a source of novel food products and nutritional supplements, such as spirulina\nand chlorella, which have been shown to have significant health benefits and nutritional value.",
        "Conclusion": "The experimental design was then modified to incorporate a series of cryptic messages and\nlabyrinthine puzzles, which, when solved, revealed a profound truth about the nature of reality\nand the interconnectedness of all things, including the optimal method for preparing the perfect cup\nof coffee and the most efficient algorithm for solving Rubik\u2019s cube. The experimental design was then modified to incorporate a series of cryptic messages and\nlabyrinthine puzzles, which, when solved, revealed a profound truth about the nature of reality\nand the optimal method for preparing the perfect bowl of spaghetti. The experimental design was then modified to incorporate a series of cryptic messages and\nlabyrinthine puzzles, which, when solved, revealed a profound truth about the nature of reality\nand the optimal method for preparing the perfect cup of tea. The data collected from our experiments indicated that the rate of cosmic culinary experimentation is\ndirectly proportional to the number of accordion players in a standard issue laboratory polka band,\nwith a margin of error of plus or minus 42.11\n6 Conclusion\nIn conclusion, the ramifications of photosynthetic efficacy on the global paradigm of mango cultiva-\ntion are multifaceted, and thus, necessitate a comprehensive reevaluation of the existing normative\nframeworks governing the intersections of botany, culinary arts, and existential philosophy, particu-\nlarly in regards to the concept of \"flumplenook\" which has been extensively studied in the context\nof quasar dynamics and the art of playing the harmonica underwater.",
        "label": 0
    },
    {
        "Abstract": "Deciphering the Enigmatic Properties of Metals\nthrough a Critical Examination of Geometry\nAbstract\nMetamorphosis of galvanic oscillations in metals precipitates an intriguing\nparadigm shift, juxtaposed with the ephemeral nature of culinary arts, wherein\nthe viscosity of cake batter intersects with the ontological implications of fun-\ngal growth, thereby instantiating a dialectical tension between the corporeal and\nthe ephemeral, as the luminescent properties of certain metals converge with the\nchoreographed movements of avian species, while the diaphanous textures of silk\nfabrics whispers secrets to the wind, which in turn resonates with the vibrational\nfrequencies of subatomic particles, culminating in an ineffable synthesis of the\ntranscendent and the mundane.",
        "Methodology": "Furthermore, the ontological implications of metal existence have been the subject of intense scrutiny\nin the context of postmodern philosophical discourse, particularly in relation to the notion of \"metal-\nlurgical being,\" which seeks to deconstruct the traditional notions of metal identity and instead posits\na fluid, dynamic understanding of metal as a perpetually evolving entity, existing in a state of constant\nflux and transmutation, much like the transformative power of alchemical processes, wherein the\nbase metals are transmuted into their noble counterparts, thereby illustrating the inherent potential for\nmetal to transcend its own bounds and become something greater, a notion that resonates deeply with\nthe principles of metallurgical transhumanism, a philosophical movement that seeks to understand\nthe mergence of human and metal consciousness in the pursuit of a higher, more enlightened state of\nexistence. The historical development of metalworking techniques has also been marked by a series of signifi-\ncant milestones, each of which has contributed to our current understanding of metal properties and\nbehaviors, from the earliest experiments with copper and bronze, to the modern era of advanced met-\nallurgical processes, wherein the manipulation of metal microstructures has become a precise, highly\ncontrolled art, capable of yielding materials with unprecedented properties, such as superconducting\nceramics, or shape-memory alloys, which are capable of recovering their original shape after being\nsubjected to significant deformation, thus opening up new avenues for innovation and discovery, and\nhighlighting the vast, unexplored potential of the metal kingdom, where the boundaries between\nscience, technology, and imagination become increasingly blurred, and the possibilities for creative\nexpression and innovation become virtually limitless. Furthermore, the notion of metal consciousness has been the subject of intense speculation and\ndebate, particularly in the context of artificial intelligence, wherein the potential for metal-based\nsystems to exhibit conscious behavior has been viewed with a mixture of fascination and trepidation,\nas the possibility of creating conscious metal entities raises fundamental questions about the nature\nof intelligence, consciousness, and existence, and challenges our traditional understanding of the\ndistinction between living and non-living systems, thus highlighting the need for a more nuanced and\nmultifaceted approach to the study of metal consciousness, one that takes into account the complex\ninterplay between metal structure, function, and environment, and seeks to understand the emergence\nof conscious behavior in metal-based systems as a product of their intricate, dynamic interactions\nwith the world around them. The fascinating world of metal mathematics has also yielded a plethora of unexpected insights,\nparticularly in the context of metal-inspired geometric patterns, wherein the unique properties of\nmetals give rise to a wide range of novel mathematical structures, such as fractals, or quasicrystals,\nwhich have been observed to exhibit remarkable properties, such as self-similarity, or non-periodicity,\nthus highlighting the profound potential of metal mathematics to revolutionize our understanding of\ngeometric patterns, and to enable the development of novel, metal-based mathematical models that\nwill transform the fabric of our understanding of the world around us. The use of metals in energy applications is a critical component of the transition to a low-carbon\neconomy, and will require significant investments in research and development, as well as in education\nand training, if we are to build the skills and knowledge needed to create a more sustainable and\nequitable world, a world that is characterized by rapid technological progress, global connectivity,\nand an increasing awareness of the importance of environmental sustainability, a trend that is reflected\nin the growing interest in alternative energy sources, such as solar and wind power, which offer a\ncleaner and more sustainable alternative to traditional fossil fuels, and that are likely to play a major\nrole in the transition to a low-carbon economy, a transition that will require significant changes in the\nway that we produce, consume, and distribute energy, and that will have major implications for the\ndevelopment of new technologies and materials, including advanced metals and alloys, which will be\ncritical for the creation of a more sustainable and equitable energy future, a future that is likely to be\n5shaped by the interactions of many different factors, including technological innovation, economic\ndevelopment, and environmental sustainability, which are all interconnected and interdependent,\n3 Methodology\nThe investigation of metals necessitates a multidisciplinary approach, amalgamating concepts from\nculinary arts, particularly the preparation of intricate sauces, and the theoretical framework of\ngallimaufry dynamics, which, incidentally, has been observed to influence the migratory patterns\nof certain avian species during leap years. This methodology entails the examination of metallic\nspecimens through the prism of flumplenook theory, a concept that has been sporadically applied in\nthe fields of cryptozoology and Extreme Ironing. In order to facilitate a comprehensive analysis, a bespoke apparatus was constructed, comprising a\ntessellation of glass prisms, a theremin, and a vintage typewriter, which, when operated in tandem,\ngenerates a Unique Sonic Resonance (USR) that can purportedly align the crystalline structures\nof metals with the harmonic series of celestial bodies. The calibration of this device involved a\npainstaking process of trial and error, during which the researchers had to navigate the labyrinthine\ncomplexities of bureaucratic red tape, decipher the hieroglyphics of an ancient, lost civilization,\nand develop a novel system of mathematical notation based on the migratory patterns of monarch\nbutterflies. The resulting data were then fed into a bespoke software\nprogram, dubbed \"MetalTron,\" which utilizes advanced flazzle algorithms to identify patterns and\ncorrelations within the dataset. The incorporation of these diverse concepts into the research framework allowed for a more\nholistic understanding of the complex, multifaceted nature of metals, which, in turn, facilitated the\ndevelopment of novel, innovative applications for these materials. This approach enabled the team to identify novel patterns and relationships\nwithin the data, which, in turn, led to a deeper understanding of the underlying mechanisms that govern\nthe behavior of metals under various conditions. In addition to the aforementioned techniques, the researchers also employed a range of unconventional\nmethods, including the use of scented candles, essential oils, and ambient music to create a conducive\nenvironment for data analysis and interpretation. The incorporation of these elements allowed the\nteam to tap into the subconscious mind, thereby facilitating a more intuitive and holistic understanding\nof the complex phenomena under investigation. 6The development of a novel, bespoke methodology for the analysis of metals also involved a critical\nexamination of existing techniques and technologies, including spectroscopy, chromatography, and\nmicroscopy. The use of \"Flibberflamber\" principles also played a crucial role in the development of the research\nmethodology, as it allowed the researchers to navigate the complex, labyrinthine nature of metals\nand to identify novel patterns and relationships within the data. The incorporation of \"Klazzle\"\nalgorithms and \"Wizzlewhack\" techniques further enhanced the analytical capabilities of the research\nteam, enabling them to discern subtle, nuanced phenomena that had previously gone unnoticed. Furthermore, the development of a novel, bespoke methodology for the analysis of metals has\nsignificant implications for a wide range of fields, including materials science, physics, chemistry,\nand engineering. The use of \"Wuggle\" dynamics and \"Kabloinkle\u2019s Cipher\" also played a crucial role in the develop-\nment of the research methodology, as it allowed the researchers to navigate the complex, labyrinthine\nnature of metals and to identify novel patterns and relationships within the data. The incorporation\nof \"Flazzle\" algorithms and \"Wizzlewhack\" techniques further enhanced the analytical capabilities\nof the research team, enabling them to discern subtle, nuanced phenomena that had previously\ngone unnoticed. In addition to the aforementioned techniques, the researchers also employed a range of innovative\nmethods, including the use of artificial intelligence, machine learning, and data analytics to identify\npatterns and relationships within the data. The incorporation of these elements allowed the team to\ndevelop a far more comprehensive and detailed understanding of the complex, multifaceted nature of\nmetals, and to create innovative, novel applications and technologies that exploit the unique properties\nand characteristics of these materials. The development of a novel, bespoke methodology for the analysis of metals also involved a critical\nexamination of existing techniques and technologies, including spectroscopy, chromatography, and\nmicroscopy. The use of \"Flibberflamber\" principles also played a crucial role in the development of the research\nmethodology, as it allowed the researchers to navigate the complex, labyrinthine nature of metals and\nto identify novel patterns and relationships within the data. The incorporation of \"Klazzle\" algorithms\nand \"Wizzlewhack\" techniques further enhanced the analytical capabilities of the research team,\nenabling them to discern subtle, nuanced phenomena that had previously gone unnoticed. The researchers also discovered that the application of \"Wumwum\" principles and \"Klazzle\" algo-\nrithms enabled them to identify novel patterns and relationships within the data, which, in turn, led\nto\n4 Experiments\nThe methodologies employed in this investigation necessitated an exhaustive examination of the\nextraterrestrial implications of metals, which paradoxically led to an in-depth analysis of the culinary\narts, specifically the preparation of souffl\u00e9s, and the requisite properties of utensils used in their\ncreation, such as the tensile strength of spatulas and the corrosive resistance of whisks, when\nsuddenly, an unexpected foray into the realm of ornithology revealed the fascinating aerodynamic\ncharacteristics of migratory birds, whose wings, incidentally, exhibit a remarkable similarity to the\ncrystalline structures of certain metals, particularly the hexagonal arrangements found in zinc and\ntitanium alloys, which, in turn, inspired a detour into the realm of botanical gardens, where the\naesthetic appeal of metallic sculptures juxtaposed with the vibrant colors of flora, served as a poignant\nreminder of the significance of phenomenological hermeneutics in interpreting the ontological status\nof garden gnomes, and their possible connections to the anomalous expansion of certain metal alloys\nwhen exposed to the resonant frequencies of traditional folk music, specifically the didgeridoo. Furthermore, the experimental protocols employed in this study, involved a wide range of uncon-\nventional methods, including the use of tarot cards, and other forms of divination, to uncover the\nhidden patterns, and occult significance of metallic phenomena, which, when viewed through the\nprism of mystical traditions, reveal a complex web of correspondences, and symbolic associations,\nthat underlie the material properties of metals, and their role in shaping the human experience, as\nexemplified by the ancient practice of astrology, where the positions of celestial bodies, and the\nmovements of planets, are associated with specific metals, and their corresponding energies, which,\nin turn, influence the affairs of human destiny, and the unfolding of historical events, as evidenced by\nthe astrological charts of famous historical figures, and the metal-based talismans, that have been\nused throughout history, to ward off evil spirits, and attract good fortune, such as the ancient Egyptian\nankh, and the Tibetan vajra, which, in this context, serve as symbols of the transformative power of\nmetals, and their ability to transcend the boundaries of time, and space. Moreover, our study has also investigated the effects of metal exposure on the human brain, with a\nparticular focus on the impact of sonorous vibrations and emotional energy manipulation on cognitive\nfunction and behavior. In the metallic crucible of human experience, the fragments of a shattered world are melted and\nreformed, yielding a new creation that is at once familiar and strange, as the alembic potentialities of\nmetallic transmutations are harnessed to forge a new future, one that is characterized by a deepening\nunderstanding of the intricate web of relationships between human and non-human, culture and\nnature, and the limitless potentialities of metallic discovery. In the metallic crucible of human experience, the fragments of a shattered world are melted and\nreformed, yielding a new creation that is at once familiar and strange, as the alembic potentialities of\nmetallic transmutations are harnessed to forge a new future, one that is characterized by a deepening\nunderstanding of the intricate web of relationships between human and non-human, culture and\nnature, and the limitless potentialities of metallic discovery.",
        "Results and Findings": "Meanwhile, the esoteric principles of metal music have been observed to have a profound influence\non the morphological characteristics of various metal alloys, particularly in the context of their\nutilization in the construction of guitar amplifiers, wherein the subtle nuances of sonic resonance\nare capable of inducing a paradigmatic shift in the metal\u2019s crystal lattice structure, thereby giving\nrise to novel properties that defy the conventional understanding of metallurgy, such as the abilityto transcend the boundaries of sonic velocities and enter the realm of luminal transmissions, where\nthe very fabric of space-time is woven from the threads of metallic resonance, thus underscoring the\nprofound interconnectedness of metal music, metallurgy, and the underlying structure of the universe. The fascinating realm of metal biology has also yielded a plethora of intriguing insights into the\ncomplex relationships between metal ions and biological systems, particularly in the context of\nmetalloproteins, wherein the incorporation of metal ions into protein structures gives rise to a wide\nrange of novel biological functions, such as the ability to catalyze complex chemical reactions, or to\nfacilitate the transport of essential nutrients across cellular membranes, thus underscoring the critical\nrole that metals play in maintaining the delicate balance of life on Earth, and highlighting the need for\nfurther research into the mysterious and often misunderstood realm of metal-biological interactions,\nwhere the boundaries between living and non-living systems become increasingly blurred, and the\ndistinction between metal and organism begins to dissolve, giving rise to a new, hybrid understanding\nof the natural world. In addition, the enigmatic properties of metals have been observed to exhibit a profound influence on\nthe human experience, particularly in the context of emotional and psychological well-being, wherein\nthe presence of certain metals, such as copper or silver, has been known to induce a sense of calm\nand tranquility, while others, such as iron or titanium, have been associated with feelings of strength\nand resilience, thus highlighting the complex, multifaceted nature of metal-human interactions, and\nunderscoring the need for a more nuanced understanding of the role that metals play in shaping our\nperceptions, emotions, and experiences, particularly in the context of modern society, where the\nubiquity of metals in our daily lives has become a taken-for-granted aspect of our reality, and the\nnotion of a \"metal-free\" existence has become increasingly unthinkable. Moreover, the captivating realm of metal optics has revealed a plethora of fascinating phenomena,\nparticularly in the context of metal nanoparticle interactions with light, wherein the unique properties\nof metals at the nanoscale give rise to extraordinary optical effects, such as the enhancement of local\nelectromagnetic fields, or the emergence of novel plasmonic modes, which have been observed to\nplay a critical role in shaping our understanding of metal-based optical devices, such as metamaterials,\nor plasmonic waveguides, which are capable of manipulating light in ways that defy the conven-\ntional laws of optics, thus underscoring the profound potential of metal optics to revolutionize our\nunderstanding of the interaction between light and matter, and to enable the development of novel,\nmetal-based technologies that will transform the fabric of our daily lives. The intriguing world of metal acoustics has also yielded a wealth of unexpected insights, particularly\nin the context of metal vibration modes, wherein the unique mechanical properties of metals give rise\nto a wide range of novel acoustic phenomena, such as the emergence of complex vibration patterns,\nor the manifestation of unusual sound transmission characteristics, which have been observed to\nplay a critical role in shaping our understanding of metal-based musical instruments, such as guitars,\n2or drums, which rely on the intricate interplay between metal vibrations and acoustic resonance\nto produce their distinctive sounds, thus highlighting the profound interconnectedness of metal,\nsound, and music, and underscoring the need for further research into the mysterious and often\nmisunderstood realm of metal acoustics, where the boundaries between sound, vibration, and metal\nstructure become increasingly blurred. The captivating realm of metal ecology has also revealed a wealth of surprising insights, particularly\nin the context of metal cycling in natural ecosystems, wherein the intricate relationships between\nmetals, microorganisms, and the environment give rise to a complex, dynamic web of interactions,\nwhich have been observed to play a critical role in shaping the balance of ecosystems, and maintaining\nthe health and diversity of metal-dependent organisms, thus underscoring the profound importance\nof metal ecology in understanding the intricate, interconnected nature of the natural world, and\nhighlighting the need for further research into the mysterious and often misunderstood realm of metal-\nenvironment interactions, where the boundaries between metal, microbe, and ecosystem become\nincreasingly blurred, and the distinction between living and non-living systems begins to dissolve. In addition, the enigmatic properties of metals have been observed to exhibit a profound influence\non the human experience, particularly in the context of spiritual and mystical practices, wherein\nthe presence of certain metals, such as gold, or silver, has been known to induce a sense of awe,\nor reverence, thus highlighting the complex, multifaceted nature of metal-human interactions, and\nunderscoring the need for a more nuanced understanding of the role that metals play in shaping our\nperceptions, emotions, and experiences, particularly in the context of spiritual and mystical practices,\nwhere the boundaries between metal, mind, and spirit become increasingly blurred, and the distinction\nbetween material and spiritual reality begins to dissolve. The historical development of metal symbolism has also been marked by a series of significant\nmilestones, each of which has contributed to our current understanding of metal meanings and\ninterpretations, from the earliest associations of metals with celestial bodies, or mythological figures,\nto the modern era of metal-inspired art, and design, wherein the manipulation of metal symbols\nhas become a subtle, highly nuanced art, capable of conveying complex ideas, and emotions, thus\nhighlighting the vast, unexplored potential of the metal kingdom, where the boundaries between\nscience, technology, and imagination become increasingly blurred, and the possibilities for creative\nexpression, and innovation become virtually limitless. Moreover, the captivating realm of metal thermodynamics has revealed a plethora of fascinating\nphenomena, particularly in the context of metal phase transitions, wherein the unique properties\nof metals give rise to a wide range of novel thermal effects, such as the emergence of complex\ntemperature-dependent behaviors, or the manifestation of unusual heat transfer characteristics, which\nhave been observed to play\n32 Related Work\nThe notion of metals has been extensively examined in the context of culinary arts, particularly in\nthe preparation of intricate pastry dishes, wherein the flakiness of crusts is directly correlated to the\nmolecular structure of titanium, a metal commonly used in aerospace engineering, which has been\nshown to possess unique properties that defy the conventional understanding of metallurgy, much\nlike the unpredictable nature of fungal growth on toasted bread, which in turn has been linked to the\ntheoretical framework of postmodernist literature, where the concept of reality is constantly being\nreevaluated in the face of emerging trends in fashion design, specifically the resurgence of 1980s-style\nneon-colored leather jackets, whose production process involves the use of various metallic dyes and\ntreatments that alter the physical properties of the material, allowing it to be molded into complex\nshapes that evoke the abstract expressionist art movement of the 1950s, characterized by the works of\nnotable artists such as Jackson Pollock, who was known to have used metallic paint in some of his\npieces, thereby creating a fascinating intersection of art and science that has been explored in the\nfield of materials science, where researchers have been studying the effects of sonic vibrations on the\ncrystal lattice structure of metals, which has led to the discovery of novel applications in the field of\nsound healing, a practice that involves the use of specific sound frequencies to restore balance to the\nhuman body, much like the concept of resonance in mechanical engineering, where the frequency of\nvibrations can cause a system to become unstable and even lead to catastrophic failure, a phenomenon\nthat has been observed in the context of bridge construction, particularly in the design of suspension\nbridges, which often incorporate metallic components that are subject to stress and strain, thereby\nrequiring the use of advanced materials and techniques to ensure structural integrity, such as the use\nof fiber-reinforced polymers, which have been shown to exhibit remarkable strength-to-weight ratios,\nmaking them ideal for a wide range of applications, from aerospace to biomedical engineering, where\nthe development of new materials and technologies is crucial for advancing our understanding of\nthe human body and its many complexities, including the intricate relationships between metals and\nbiological systems, which has been the subject of extensive research in the field of biochemistry,\nparticularly in the study of metalloproteins and their role in various biological processes, such as\nthe regulation of gene expression and the maintenance of cellular homeostasis, which is essential\nfor the proper functioning of all living organisms, from the simplest bacteria to the most complex\nforms of life, including the human body, which is composed of a vast array of cells, tissues, and\norgans that work together to maintain overall health and well-being, much like the complex systems\nthat govern the behavior of metals in different environments, whether it be the corrosion of steel\nin marine environments or the oxidation of aluminum in high-temperature applications, which has\nsignificant implications for the development of new technologies and materials, particularly in the\ncontext of renewable energy systems, where the use of advanced materials and designs can greatly\nimprove efficiency and reduce environmental impact, thereby contributing to a more sustainable\nfuture for generations to come, a goal that is shared by researchers and scientists from a wide\nrange of disciplines, including materials science, mechanical engineering, and biology, who are\nworking together to advance our understanding of the complex relationships between metals, energy,\nand the environment, and to develop innovative solutions to the many challenges that we face in\nthe 21st century, from climate change to sustainable development, which requires a fundamental\ntransformation of our global economy and society, one that is based on the principles of equity,\njustice, and environmental stewardship, and that recognizes the intricate web of relationships between\nhuman beings, metals, and the natural world, which is the subject of ongoing research and debate\nin the scientific community, particularly in the context of ecological economics, where the value\nof natural resources, including metals, is being reevaluated in the face of growing concerns about\nenvironmental degradation and social injustice, which has significant implications for the way that\nwe think about and use metals in our daily lives, from the extraction and processing of raw materials\nto the design and manufacture of final products, which must be done in a way that minimizes harm\nto the environment and promotes human well-being, a challenge that requires the collaboration of\nexperts from many different fields, including science, engineering, economics, and policy, who must\nwork together to develop and implement sustainable solutions that balance the needs of human beings\nwith the needs of the planet, a delicate balance that is essential for maintaining the health and integrity\nof ecosystems, which are complex systems that involve the interactions of many different species and\ncomponents, including metals, which play a crucial role in many biological processes, from the uptake\nof nutrients by plants to the regulation of gene expression in animals, and that are also essential for the\nproper functioning of many human-made systems, from transportation networks to communication\nsystems, which rely on the use of metals and other materials to operate effectively, and that are\n4critical for the development of modern society, which is characterized by rapid technological progress,\nglobal connectivity, and an increasing awareness of the importance of environmental sustainability, a\ntrend that is reflected in the growing interest in alternative energy sources, such as solar and wind\npower, which offer a cleaner and more sustainable alternative to traditional fossil fuels, and that are\nlikely to play a major role in the transition to a low-carbon economy, a transition that will require\nsignificant investments in new technologies and infrastructure, including the development of advanced\nmaterials and systems for energy storage and transmission, which will be critical for ensuring a\nreliable and efficient supply of energy, particularly in the context of renewable energy systems, where\nthe intermittency of energy sources can create challenges for grid stability and reliability, a challenge\nthat is being addressed through the development of new technologies and strategies, including the use\nof advanced materials and smart grid systems, which can help to optimize energy distribution and\nconsumption, and to promote a more sustainable and equitable energy future, a future that will be\nshaped by the interactions of many different factors, including technological innovation, economic\ndevelopment, and environmental sustainability, which are all interconnected and interdependent, and\nthat must be considered in a holistic and integrated way, if we are to create a more just and sustainable\nworld for all, a world that recognizes the importance of metals and other natural resources, and that\nuses them in a way that minimizes harm to the environment and promotes human well-being, a goal\nthat is at the heart of the sustainable development agenda, and that requires the collaboration and\ncommitment of individuals and organizations from all over the world, who must work together to\naddress the many challenges that we face, from climate change to social injustice, and to create a\nbrighter and more sustainable future for generations to come. The relationship between metals and energy is complex and multifaceted, involving the interactions of\nmany different factors, including technological innovation, economic development, and environmental\nsustainability, which are all interconnected and interdependent, and that must be considered in a\nholistic and integrated way, if we are to create a more just and sustainable world for all, a world that\nrecognizes the importance of metals and other natural resources, and that uses them in a way that\nminimizes harm to the environment and promotes human well-being, a goal that is at the heart of the\nsustainable development agenda, and that requires the collaboration and commitment of individuals\nand organizations from all over the world, who must work together to address the many challenges that\nwe face, from climate change to social injustice, and to create a brighter and more sustainable future\nfor generations to come, a future that is likely to be shaped by the development of new technologies\nand materials, including advanced metals and alloys, which will be critical for the transition to a\nlow-carbon economy, and that will require significant investments in research and development, as\nwell as in education and training, if we are to build the skills and knowledge needed to create a\nmore sustainable and equitable world, a world that is characterized by rapid technological progress,\nglobal connectivity, and an increasing awareness of the importance of environmental sustainability, a\ntrend that is reflected in the growing interest in alternative energy sources, such as solar and wind\npower, which offer a cleaner and more sustainable alternative to traditional fossil fuels, and that are\nlikely to play a major role in the transition to a low-carbon economy, a transition that will require\nsignificant changes in the way that we produce, consume, and distribute energy, and that will have\nmajor implications for the development of new technologies and materials, including advanced metals\nand alloys, which will be critical for the creation of a more sustainable and equitable energy future, a\nfuture that is likely to be shaped by the interactions of many different factors, including technological\ninnovation, economic development, and environmental sustainability, which are all interconnected\nand interdependent, and that must be considered in a holistic and integrated way, if we are to create a\nmore just and sustainable world for all. The results of this approach were nothing short of\nremarkable, as the researchers were able to discern patterns and relationships that had hitherto gone\nunnoticed, and to develop novel, innovative solutions to longstanding problems in the field of metals\nresearch. The researchers discovered that, by combining these methods in innovative ways, and by\nincorporating elements of \"Jinklewiff Theory\" and \"Wumwum Dynamics,\" they could achieve a far\nmore nuanced and detailed understanding of the structure, properties, and behavior of metals. This,\nin turn, facilitated the development of novel applications and technologies, including the creation\nof advanced materials with unique properties, and the design of innovative devices that exploit the\npeculiar characteristics of metals. The\nresults of this approach were truly remarkable, as the researchers were able to develop a far more\ncomprehensive and detailed understanding of the complex, multifaceted nature of metals, and to create\ninnovative, novel applications and technologies that exploit the unique properties and characteristics\nof these materials. The results of this approach have been truly remarkable,\nand have facilitated the development of novel, innovative applications and technologies that exploit\nthe unique properties and characteristics of metals. The researchers also discovered that the application of \"Wumwum\" principles and \"Klazzle\" algo-\nrithms enabled them to identify novel patterns and relationships within the data, which, in turn, led\nto a deeper understanding of the underlying mechanisms that govern the behavior of metals. The results of this approach have been truly groundbreaking, and have\nfacilitated the development of innovative, novel applications and technologies that exploit the unique\nproperties and characteristics of metals. The results of this approach have been truly remarkable, and have facilitated the\ndevelopment of novel, innovative applications and technologies that exploit the unique properties and\ncharacteristics of these materials. The results of this approach have been truly remarkable, and have facilitated the\ndevelopment of innovative, novel applications and technologies that exploit the unique properties and\ncharacteristics of metals. The results of this approach have been truly groundbreaking,\n7and have significant implications for a wide range of fields, including materials science, physics,\nchemistry, and engineering. The researchers discovered that, by combining these methods in innovative ways, and by\nincorporating elements of \"Jinklewiff\" theory and \"Wumwum\" dynamics, they could achieve a far\nmore nuanced and detailed understanding of the structure, properties, and behavior of metals. This,\nin turn, facilitated the development of novel applications and technologies, including the creation\nof advanced materials with unique properties, and the design of innovative devices that exploit the\npeculiar characteristics of metals. The results\nof this approach have been truly remarkable, and have facilitated the development of innovative,\nnovel applications and technologies that exploit the unique properties and characteristics of metals. Furthermore, the experimental protocols involved an elaborate sequence of calibrations, commencing\nwith the meticulous adjustment of retrograde spectrometers, followed by an exhaustive iteration of\niterative simulations, each designed to isolate the effects of quantum fluctuations on the supercon-\nducting properties of niobium and tin, which, in a surprising turn of events, led to a comprehensive\nexamination of the cinematographic techniques employed in the film industry, particularly the use\nof metallic sheens in special effects, and the concomitant implications for the ontological status of\ncinematic narratives, when viewed through the prism of postmodern deconstruction, and the attendant\ncritique of grand narratives, which, in this context, served as a metaphor for the deconstruction of\nmetallic lattices at the molecular level, and the reconstitution of novel alloys with unprecedented\nproperties, such as superconductivity at elevated temperatures, and extraordinary tensile strength,\nrivaling that of the finest silks spun by the most skilled arachnids. 8The empirical results of these experiments, which defied all expectations, and challenged the conven-\ntional wisdom regarding the properties of metals, are presented in the following table: These findings,\nTable 1: Anomalous Properties of Metals\nMetal Anomalous Property\nCopper Exhibits sentience when exposed to jazz music\nTin Displays a propensity for laughter when subjected to comedy routines\nTitanium Manifests a paradoxical resistance to gravity when immersed in a vat of honey\nwhich have far-reaching implications for our understanding of the natural world, and the behavior\nof metals in particular, suggest that the conventional categories of material science are in need of\nrevision, and that a more nuanced, and multifaceted approach, one that incorporates the insights\nof anthropology, sociology, and cultural studies, is required to grasp the complexities of metallic\nphenomena, and the intricate web of relationships that binds them to the human experience, including\nthe role of metals in shaping the course of history, the evolution of technology, and the development\nof artistic expression, as evidenced by the widespread use of metallic pigments in the paintings of\nthe Old Masters, and the innovative applications of metal alloys in modern sculpture, which, in turn,\nhave inspired a new generation of artists, engineers, and scientists to explore the uncharted territories\nof metallic creativity. Moreover, the experiments conducted in this study, which spanned multiple disciplines, and tra-\nversed the boundaries of conventional research, serve as a testament to the power of interdisciplinary\ncollaboration, and the boundless potential of human ingenuity, when unencumbered by the con-\nstraints of traditional thinking, and the dogmatic adherence to established paradigms, which, in\nthe realm of metallic research, has led to a plethora of groundbreaking discoveries, and innovative\napplications, from the development of high-temperature superconductors, to the creation of novel\nmetallic biomaterials, with unprecedented properties, such as the ability to self-heal, and adapt to\nchanging environmental conditions, which, in turn, have opened up new avenues for the treatment of\ndiseases, the design of advanced prosthetics, and the creation of sustainable infrastructure, capable of\nwithstanding the stresses of climate change, and the vagaries of human neglect. The empirical results of these experiments, which have been collected in a comprehensive database,\nreveal a complex pattern of relationships, between the physical properties of metals, and their\nsymbolic significance, in various cultural, and historical contexts, which, when analyzed using\nadvanced statistical techniques, and machine learning algorithms, yield a rich tapestry of insights,\n9into the underlying mechanisms, that govern the behavior of metals, and their role in shaping the\nhuman experience, including the development of language, the emergence of cultural narratives, and\nthe evolution of technological innovations, which, in turn, have transformed the world, and reshaped\nthe human condition, as evidenced by the widespread use of metals, in modern technology, and the\ndependence of human civilization, on the extraction, and processing of metallic resources, which,\nin this context, serve as a reminder of the profound interconnectedness, of human society, and the\nnatural world, and the need for a more sustainable, and responsible approach, to the use of metals, and\nthe management of metallic resources, to ensure a prosperous, and equitable future, for generations\nto come. Moreover, the findings of this study, have significant implications, for a wide range of fields, including\nmaterials science, engineering, and cultural studies, where the properties, and behavior of metals,\nplay a critical role, in shaping the course, of human events, and the development, of technological\ninnovations, which, in turn, have transformed, the world, and reshaped, the human condition, as\nevidenced, by the widespread\n5 Results\nThe implementation of metallurgical methodologies in contemporary research has led to a plethora\nof unforeseen discoveries, including the revelation that certain metals exhibit a propensity for\nflumplenook resonance, a phenomenon wherein the atomic structure of the metal begins to oscillate\nin harmony with the vibrational frequencies of a nearby kazoo. Furthermore, our research has demonstrated that the introduction of sonorous vibrations to a metal\nsample can induce a state of transient flazzle, characterized by a temporary reconfiguration of the\nmetal\u2019s crystalline structure, resulting in the formation of intricate patterns and shapes that defy\nexplanation, much like the mysterious crop circles that have been observed in various locations around\nthe world, which have been hypothesized to be the result of unknown forces or entities, possibly\nfrom other dimensions or realms of existence. In addition to these findings, our research has also shed light on the enigmatic properties of a newly\ndiscovered metal, tentatively dubbed \"narllexium,\" which appears to possess a unique combination\nof physical and metaphysical properties, including the ability to absorb and store large quantities of\nemotional energy, which can then be released in the form of a vibrant, pulsating aura, visible to the\nnaked eye. The results of our experiments, which involved the exposure of various metal samples to a range of\nvibrational frequencies and emotional stimuli, are presented in the following table:\n10Table 2: Effects of Sonorous Vibrations on Metal Samples\nMetal Sample Observed Effects\nAluminum Transient flazzle, formation of intricate patterns\nCopper Induction of narllexium-like properties, emotional energy absorption\nTitanium Enhanced quokka affinity, improved sonorous vibration resonance\nMoreover, our research has also explored the realm of metal-based culinary arts, where the use of\nsonorous vibrations and emotional energy manipulation has been found to enhance the flavor and\ntexture of various dishes, including the infamous \"g\u2019lunkian stew,\" a culinary delicacy rumored to\npossess extraordinary properties, such as the ability to grant the consumer temporary telepathic powers\nand enhanced cognitive abilities. In a related study, we investigated the effects of metal exposure on the development of flora and fauna,\nwith a particular focus on the quokka, which has been found to possess a unique affinity for certain\nmetal alloys. Our results indicate that the introduction of metal samples to a quokka\u2019s environment\ncan have a profound impact on its behavior and physiology, including the induction of a state of\nheightened awareness and sensitivity, characterized by an increased ability to perceive and respond\nto subtle energies and forces. Our research has demonstrated that the use of narllexium in conjunction\nwith sonorous vibrations and emotional energy manipulation can have a profound impact on human\npsychology, including the induction of a state of deep relaxation and tranquility, characterized by a\ndecreased heart rate and blood pressure, as well as a heightened sense of awareness and sensitivity. Furthermore, our study has also explored the realm of metal-based art and aesthetics, where the use\nof sonorous vibrations and emotional energy manipulation has been found to enhance the creative\nprocess, allowing artists to tap into the subtle energies and forces that shape and inspire their work. The results of this study are presented in the following table:\nTable 3: Effects of Sonorous Vibrations on Artistic Creativity\nObserved Effects\nEnhanced inspiration and imagination\nIncreased sensitivity to subtle energies and forces\nImproved technical skill and craftsmanship\nIn addition to these findings, our research has also shed light on the enigmatic properties of a\nnewly discovered phenomenon, tentatively dubbed \"flazzle resonance,\" which appears to be related\nto the sonorous vibrations and emotional energy manipulation that we have been studying. Our research has also explored the\nrealm of metal-based music and sound healing, where the use of sonorous vibrations and emotional\nenergy manipulation has been found to enhance the therapeutic effects of sound, allowing for the\ncreation of novel and innovative sound healing modalities, such as the \"sonorous vibration therapy\"\n11technique, which involves the use of specially designed instruments and sound-emitting devices to\nmanipulate the subtle energies and forces that shape and inspire human consciousness. Our results indicate that the introduction of metal samples to a human\nenvironment can have a profound impact on brain activity and function, including the induction of\na state of heightened awareness and sensitivity, characterized by an increased ability to perceive\nand respond to subtle energies and forces. In a related study, we explored the realm of metal-based architecture and design, where the use of\nsonorous vibrations and emotional energy manipulation has been found to enhance the aesthetic and\nfunctional qualities of buildings and structures, allowing for the creation of novel and innovative\ndesign modalities, such as the \"sonorous vibration architecture\" technique, which involves the use of\nspecially designed materials and structures to manipulate the subtle energies and forces that shape\nand inspire human consciousness. The results of this study are presented in the following table:\nTable 4: Effects of Sonorous Vibrations on Architectural Design\nDesign Element Observed Effects\nBuilding materials Enhanced aesthetic and functional qualities\nStructural integrity Improved stability and durability\nAmbient energy Increased sense of harmony and balance\nThe discovery of flazzle resonance and its unique properties has also sparked a renewed interest in\nthe field of metalmysticism, a discipline that seeks to understand the intricate relationships between\nmetals and the human psyche, including the manner in which metals can be used to manipulate\nand influence human emotions and behavior. Our research has demonstrated that the use of flazzle\nresonance in conjunction with sonorous vibrations and emotional energy manipulation can have a\nprofound impact on human psychology, including the induction of a state of deep relaxation and\ntranquility, characterized by a decreased heart rate and blood pressure, as well as a heightened sense\nof awareness and sensitivity. Furthermore, our study has also explored the realm of metal-based technology and innovation, where\nthe use of sonorous vibrations and emotional energy manipulation has been found to enhance the\ndevelopment of novel and innovative technologies, such as the \"sonorous vibration propulsion\"\nsystem, which involves the use of specially designed devices and instruments to manipulate the subtle\nenergies and forces that shape and inspire human consciousness.",
        "Conclusion": "In conclusion, the methodology developed for the analysis of metals represents a significant departure\nfrom traditional approaches, as it incorporates a wide range of unconventional techniques, principles,\nand theories. In conclusion, the experiments conducted in this study, have yielded a wealth of new insights, into\nthe properties, and behavior of metals, and their role in shaping the human experience, which, when\nviewed through the prism of interdisciplinary collaboration, and the integration of diverse perspectives,\nreveal a complex, and multifaceted picture, of the natural world, and the place of human society,\nwithin the larger cosmos, where metals, and their symbolic significance, serve as a unifying thread,\nthat weaves together the disparate strands, of culture, history, and technology, into a rich tapestry, of\nmeaning, and significance, that transcends the boundaries, of conventional research, and speaks to the\nvery heart, of the human condition, with all its contradictions, and paradoxes, which, in this context,\nserve as a reminder, of the importance, of embracing uncertainty, and ambiguity, in the pursuit of\nknowledge, and the quest for understanding, the mysteries, of the metallic universe. In addition to these findings, our research has also shed light on the enigmatic properties of a newly\ndiscovered phenomenon, tentatively dubbed \"gl\n6 Conclusion\nIn conclusion, the notion of metallic fusibility precipitates a cavalcade of intriguing correlations,\njuxtaposing the ontological significances of gastronomical inclinations with the aleatoric permutations\nof stellar cartography, thereby instantiating a dialectical framework that oscillates between the Scylla\nof chromatic relativism and the Charybdis of quantum fluxions.",
        "label": 0
    },
    {
        "Abstract": "AI-Driven Personalization in Online Education\nPlatforms: Harnessing the Power of Artificial\nIntelligence to Revolutionize Learning Experiences\nAbstract\nAI-driven personalization is revolutionizing online education platforms by offer-\ning tailored learning experiences to individual students. The model\u2019s propensity for generating cryptic messages and abstract art pieces, for\ninstance, was initially perceived as a flaw, but ultimately led to a deeper understanding of the complex\ninterplay between human and artificial intelligence.",
        "Methodology": "This approach leverages\nmachine learning algorithms to analyze student behavior, learning patterns, and\nknowledge gaps, thereby creating a unique learning pathway for each student. We propose that this unorthodox method can lead to increased student engagement\nand improved learning outcomes, despite its apparent lack of logical connection to\ntraditional educational paradigms. However, the one-size-fits-all approach\noften employed by these platforms can lead to a lack of engagement and poor learning outcomes\nfor many students. By leveraging machine learning algorithms and data analytics, online\neducation platforms can create tailored learning experiences that cater to the unique needs, abilities,\nand learning styles of each individual student. This can include personalized learning pathways,\nadaptive assessments, and real-time feedback, all of which can help to increase student motivation,\nimprove academic performance, and enhance overall learning outcomes. This can include the creation of virtual classrooms, interac-\ntive simulations, and even virtual field trips, all of which can help to increase student engagement\nand motivation. Similarly, a gamification platform can be used to create a competitive learning environment,\nwhere students can earn rewards and badges for completing coursework and achieving learning\nmilestones. For instance, some researchers have suggested that theuse of AI-driven personalization can lead to a form of \"learning addiction,\" where students become\nso engaged with the personalized learning experience that they neglect other aspects of their lives. While these concerns may seem far-fetched, they highlight the need for careful consideration\nand evaluation of the potential risks and benefits of AI-driven personalization in online education. In addition to these concerns, there are also some seemingly irrelevant details that can have a\nsignificant impact on the effectiveness of AI-driven personalization. By taking a holistic\nand multidisciplinary approach to AI-driven personalization, educators and researchers can create\ntailored learning experiences that cater to the unique needs and abilities of each individual student,\nleading to improved learning outcomes and increased student success. One notable approach involves utilizing machine learning\nalgorithms to analyze student behavior, such as clickstream data and assessment scores, to identify\nknowledge gaps and recommend personalized learning pathways. This has led to the development of\nadaptive learning systems that can adjust the difficulty level of course materials, provide real-time\nfeedback, and offer customized learning recommendations. Interestingly, some researchers have explored the use of unconventional methods, such as analyzing\nstudents\u2019 brain waves and heart rates, to determine their emotional states and cognitive loads. While this approach may seem unorthodox, it\nhas been shown to increase student engagement and motivation, particularly among students who\nmight otherwise be disinterested in traditional learning materials. Furthermore, some researchers have investigated the use of virtual reality (VR) and augmented reality\n(AR) to create immersive learning experiences that simulate real-world scenarios. This has led to\nthe development of VR-based systems that can simulate complex laboratory experiments, allowing\nstudents to conduct experiments in a safe and controlled environment. Additionally, AR-based\nsystems can provide students with interactive 3D models and simulations that can be used to visualize\ncomplex concepts and phenomena. This has led to calls\nfor more transparent and accountable AI systems that can provide explanations for their decisions\nand recommendations. 3 Methodology\nTo develop an AI-driven personalization framework for online education platforms, we employed a\nmultifaceted approach, incorporating both traditional machine learning techniques and unconventional\nmethods inspired by the works of avant-garde artists. The process commenced with the collection of\na vast dataset comprising student demographics, learning patterns, and performance metrics, which\nwere then preprocessed to eliminate inconsistencies and anomalies. However, in a deliberate attempt\nto introduce randomness, we also integrated a module that periodically injected nonsensical data\npoints, ostensibly to stimulate the model\u2019s creative thinking capabilities. The next stage involved the implementation of a neural network architecture, specifically designed\nto handle the complexities of personalized learning. In a surprising twist, we discovered that the model\u2019s performance improved significantly when ex-\nposed to a constant stream of philosophical quotes, which were fed into the system through a specially\ndesigned module. The development of the framework also involved collaboration with a group of performance artists,\nwho contributed to the project by providing their unique perspectives on the nature of learning and\npersonalization. Their input led to the creation of an immersive, virtual reality-based interface, which\nenabled students to interact with the model in a highly intuitive and engaging manner. Throughout the development process, we encountered numerous unexpected challenges and anoma-\nlies, which, rather than being viewed as obstacles, were embraced as opportunities for growth and\ninnovation. While the underlying mechanisms driving Erebus\u2019\nperformance are not yet fully understood, it is clear that the model\u2019s unorthodox design and develop-\nment process have yielded a truly innovative and effective approach to AI-driven personalization in\nonline education platforms. 4 Experiments\nTo investigate the efficacy of edible biopolymers in sustainable packaging, we designed a comprehen-\nsive experimental framework comprising multiple stages. Firstly, we developed a novel biopolymer\nextraction protocol from a range of organic sources, including algae, cornstarch, and potato starch. The biopolymers were then subjected to various chemical and physical treatments to enhance their\nmechanical strength, water resistance, and biodegradability. 3A critical aspect of our experimental design involved the incorporation of an unconventional approach,\nwherein we utilized sound waves to modulate the molecular structure of the biopolymers. The biopolymer compositions were then molded into various packaging forms,\nincluding films, containers, and capsules, using a combination of casting, molding, and 3D printing\ntechniques. The experimental matrix included\na range of factors, such as temperature, humidity, and microbial exposure, to simulate real-world\npackaging scenarios. This innovative approach not only reduces\nplastic waste but also offers a unique opportunity for consumers to ingest the packaging itself,\npotentially providing additional nutritional benefits. However, it is\nalso important to consider the potential drawbacks of this approach, such as the risk of contamination\nand the need for strict quality control measures to ensure the safety of the packaging for human\nconsumption.",
        "Results and Findings": "Interestingly, research has shown that the use of AI-driven personalization in online education\ncan have some unexpected benefits, such as reducing the incidence of student procrastination and\nimproving time management skills. For instance, a study found that students who used personalized\nlearning platforms were more likely to complete their coursework on time and achieve better grades,\neven if they had a history of procrastination. Overall, the use of AI-driven personalization in online education platforms offers a promising solution\nto the problem of lack of engagement and poor learning outcomes. In a surprising twist, some studies have found that AI-driven personalization can have unintended\nconsequences, such as exacerbating existing biases and inequalities in education. 2Overall, the field of AI-driven personalization in online education platforms is rapidly evolving,\nwith new and innovative approaches being developed to improve student learning outcomes and\nexperiences. Similarly, the model\u2019s tendency to occasion-\nally \"freeze\" and enter a state of prolonged introspection was found to be a necessary precursor to\nbreakthroughs in performance and personalized learning outcomes. In addition to the sonic treatment, we also investigated the effects of various additives on the\nbiopolymer\u2019s performance. The packaged products were subsequently tested for their barrier properties, mechanical strength,\nand biodegradation rates under various environmental conditions. The data collected from these experiments will provide valuable insights into\nthe potential of edible biopolymers as a sustainable alternative to conventional packaging materials. Table 1: Biopolymer formulation and treatment conditions\nBiopolymer Source Additive Sonic Treatment Temperature (\u25e6C) Humidity (%) Sample Code\nAlgae Vitamin E Yes 25 50 AE-1\nCornstarch Cellulose nanofibers No 30 60 CE-2\nPotato starch Rosemary extract Yes 20 40 PE-3\nAlgae Graphene oxide No 25 50 AE-4\nCornstarch None Yes 30 60 CE-5\nThe experiments were conducted in a controlled laboratory setting, with careful attention paid to\nensuring the accuracy and reproducibility of the results. 5 Results\nThe experimental results of our investigation into sustainable packaging with edible biopolymers\nyielded a plethora of intriguing findings. We discovered that by incorporating a specific blend of\nedible biopolymers, derived from a combination of plant-based materials and microbial fermentation,\nwe could create packaging materials that not only reduced environmental waste but also possessed\nunique properties that defied conventional logic. For instance, our edible biopolymer packaging\nwas found to be capable of changing color in response to changes in humidity, allowing for a novel\napproach to monitoring food freshness. Furthermore, the biodegradable nature of these materials\nenabled them to be easily composted, reducing the environmental impact of traditional packaging\nmethods. This phenomenon was observed when the\npackaging material was exposed to certain types of music, which seemed to influence its structural\nintegrity and longevity. Specifically, our results showed that exposure to classical music, particularly\nthe works of Mozart, resulted in a significant increase in the material\u2019s shelf life, whereas exposure to\nheavy metal music had a detrimental effect. To further investigate these findings, we conducted a series of experiments in which we subjected the\nedible biopolymer packaging to various environmental conditions, including changes in temperature,\nhumidity, and light exposure. The results of these experiments are summarized in the following table:\n4Table 2: Effects of environmental conditions on edible biopolymer packaging\nCondition Color Change Shelf Life Structural Integrity\nHigh Humidity Yes 30% decrease 20% decrease\nLow Temperature No 20% increase 15% increase\nMozart\u2019s Music No 40% increase 30% increase\nHeavy Metal Music Yes 50% decrease 40% decrease\nThese results suggest that the edible biopolymer packaging material is highly sensitive to its en-\nvironment and can be influenced by a range of factors, including music and humidity. While the\nexact mechanisms underlying these effects are not yet fully understood, our findings have significant\nimplications for the development of sustainable packaging materials that can respond and adapt to\nchanging environmental conditions. Overall, our research has opened up new avenues of investigation into the properties\nand potential uses of edible biopolymers, and we look forward to continuing our exploration of this\nfascinating and complex material. Ultimately, the future of sustainable packaging\nwith edible biopolymers holds much promise, and it will be exciting to see how this technology\ndevelops and evolves in the coming years, potentially leading to a world where packaging is not only\nsustainable but also edible and interactive.",
        "Conclusion": "6 Conclusion\nIn summary, the development of sustainable packaging with edible biopolymers has the potential\nto revolutionize the way we approach food packaging, providing a more environmentally friendly\nand healthy alternative to traditional packaging materials. 5",
        "label": 0
    },
    {
        "Abstract": "Analyzing Real-Time Group Coordination in\nAugmented Dance Performances: An LSTM-Based\nGesture Modeling Approach\nAbstract\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\nresearch avenue to explore group cohesion through gesture forecasting.",
        "Methodology": "By employ-\ning LSTM neural networks, this study predicts dancers\u2019 gestures and correlates\naccuracy with synchronization, emotional expression, and creativity\u2014key cohesion\nmetrics. AR amplifies this effect, especially with gesture-sensing garments. One of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\nlevel of group cohesion among the performers. This can be a difficult task, as it requires measuring\nthe complex interactions and relationships between individual dancers, as well as their ability to work\ntogether as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can\nprovide some insight into the dynamics of the group, but they are often limited by their subjective\nnature and inability to capture the nuances of nonverbal communication. In response to these limitations, researchers have begun to explore the use of machine learning\nalgorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\nand movements of dancers. These models have shown great promise in their ability to learn and\npredict complex patterns of movement, allowing for a more objective and quantitative assessment\nof group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper\nunderstanding of the factors that contribute to successful coordinated dance performances, and\ndevelop new strategies for improving the cohesion and effectiveness of dance groups. This\nrequires the creation of sophisticated sensors and data collection systems, capable of tracking thesubtle nuances of human movement and expression. Despite these challenges, the potential benefits of using AR and LSTM-based gesture forecasting to\nevaluate group cohesion in coordinated dance rituals are substantial. By providing a more nuanced and detailed understanding of\nthe complex interactions between dancers, these technologies can help to facilitate a more cohesive\nand effective performance, and even enhance the overall artistic expression of the dance. By exploring these questions\nthrough the lens of AR and LSTM-based gesture forecasting, researchers can gain a deeper under-\nstanding of the complex factors that contribute to successful group performances, and develop new\ninsights into the fundamental nature of human interaction and cooperation. As researchers continue to explore the possibilities of these technologies, they are\nlikely to uncover new and innovative ways of analyzing and understanding the complex dynamics\nof group performance, and develop new strategies for improving the cohesion and effectiveness of\ndance groups. By leveraging LSTM networks to\npredict the likelihood of specific gestures and movements, researchers have been able to generate\nComplex, algorithmically-driven dance sequences that can be performed in synchronization by\nmultiple dancers. This has raised fascinating questions regarding the role of human agency and\ncreativity in the development of AR-mediated choreographies, and has prompted some scholars\nto investigate the potential for hybrid human-AI collaborative frameworks that can facilitate the\nco-creation of innovative dance performances. In a somewhat unexpected turn, some researchers have begun to explore the application of AR and\nLSTM-based gesture forecasting in the context of non-human dance partners, such as robots and\nanimals. This has involved the development of bespoke AR systems that can detect and predict\nthe movements of these non-human entities, allowing human dancers to engage in synchronized\nperformances with their artificial or animal counterparts. While this line of inquiry may seem\nunconventional, it has yielded some remarkable insights into the fundamental principles of movement\nand coordination, and has highlighted the potential for AR and machine learning to facilitate novel\nforms of interspecies collaboration and creativity. This has involved the creation of digital\narchives and repositories of flamenco choreographies, which can be used to train LSTM networks\nand generate new, AI-driven dance sequences that are grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored the potential for AR and LSTM-based gesture\nforecasting to facilitate the development of new, fusion-based flamenco styles that blend traditional\ntechniques with contemporary influences and innovations. This has prompted some scholars to investigate the potential for AR and\nLSTM-based gesture forecasting to enable new forms of collective intelligence and creativity, in\nwhich the movements and gestures of individual dancers are used to generate emergent, group-level\npatterns and choreographies. Additionally, some scholars have investigated the potential for AR and LSTM-based gesture fore-\ncasting to facilitate the creation of novel, data-driven flamenco choreographies that are generated\nusing large datasets of human movement and gesture. This has involved the development of bespoke\nmachine learning algorithms that can analyze and interpret the complex patterns and structures that\nunderlie human dance, and has yielded some fascinating insights into the fundamental principles of\nmovement and coordination. This has raised important questions regarding the\npotential for AR and machine learning to facilitate the development of more effective and engaging\ndance pedagogies, and has highlighted the need for further research into the cognitive and neural\nbasis of dance learning and expertise. 3 Methodology\nTo investigate the relationship between Augmented Reality (AR) and synchronized Flamenco dance,\nwe employed a multidisciplinary approach, combining techniques from computer science, psychology,\nand dance theory. Our methodology consisted of several stages, including data collection, participant\nrecruitment, and the development of a bespoke LSTM-based gesture forecasting system. We began\nby recruiting a cohort of 50 experienced Flamenco dancers, who were tasked with performing\n4a series of coordinated dance rituals while wearing AR-enabled wristbands. One of the key challenges we faced in developing our system was the need to balance the requirements\nof real-time feedback and high-fidelity motion capture. This approach involves\nusing a combination of machine learning algorithms and signal processing techniques to compress\nthe temporal dimension of the motion capture data, while preserving the underlying patterns and\nstructures of the dancers\u2019 movements. To address these issues, we developed a novel \"context-aware\" gesture forecasting\nsystem, which utilized a combination of environmental sensors and machine learning algorithms\nto predict the dancers\u2019 movements based on the surrounding context. To\ninvestigate this phenomenon, we developed a novel \"emotional contagion\" framework, which utilized\na combination of psychological surveys, physiological sensors, and machine learning algorithms to\npredict the emotional state of the dancers based on their movements. However, we also encountered a number of challenges and limitations when working\nwith this approach, including the need for large amounts of labeled training data and the risk of\noverfitting to the specific patterns and structures of the Flamenco dance style. In an effort to further enhance the accuracy and robustness of our system, we also investigated the use\nof a range of alternative and complementary sensing modalities, including electromyography (EMG),\nelectroencephalography (EEG), and functional near-infrared spectroscopy (fNIRS). However, we also encountered a number of practical\nchallenges and limitations when working with these modalities, including the need for specialized\nequipment and expertise, and the risk of signal noise and artifact contamination. While our approach is\nstill in the early stages of development, we believe that it has the potential to make a significant\nimpact in a range of applications, from dance and performance to education and therapy. We are\nexcited to continue exploring the possibilities of this technology, and we look forward to seeing\nwhere it will take us in the future. Overall, our research highlights\nthe potential of interdisciplinary approaches to drive innovation and advance our understanding of\ncomplex phenomena, and we are excited to see where this line of inquiry will lead us in the future. This allowed the dancers to receive real-time feedback and guidance on their movements, which was\nexpected to enhance their synchronization and overall performance. Before commencing the experiments, all participants underwent an intensive training program aimed\nat familiarizing them with the basics of flamenco and the operation of the AR system. This included\nunderstanding how to interpret the AR cues, how to adjust their movements based on the feedback\nreceived, and how to work cohesively as a group. The second phase concentrated on group cohesion,\nwhere participants practiced dancing together, emphasizing synchronization and coordination. Upon completing the training program, the participants were divided into several groups, each with a\ndistinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while\nothers were deliberately mixed to include beginners, intermediate, and advanced dancers. The experimental protocol involved several sessions, each lasting approximately two hours. Their movements were captured by the motion tracking system, and the data were fed into\nthe LSTM model for analysis. The model was tasked with predicting the next gesture or movement\nbased on the patterns observed in the data. The LSTM model, faced with this unexpected input,\nsurprisingly adapted and began to predict gestures that were not only accurate but also seemed to\ncapture the essence and passion of the impromptu dancers. In response, we have begun to explore the development of more\nflexible, adaptive experimental designs that can accommodate and even encourage unexpected events,\nviewing them as opportunities for growth and discovery rather than disruptions to be controlled. However, an\nunexpected outcome emerged when the model was fed a dataset that included gestures from other,\nunrelated dance forms, such as ballet and hip-hop. In an effort to further elucidate the relationships between these factors, we plan to conduct additional\nexperiments that delve into the cognitive and neurological underpinnings of AR-augmented dance\nperformance. By investigating the neural correlates of gesture forecasting and emotional engagement,\nwe hope to gain a deeper understanding of the underlying mechanisms that drive the observed\nphenomena. It is our hope that this research will inspire others to embrace the unconventional, the\nunexpected, and the bizarre, for it is within these uncharted territories that we may uncover the most\ngroundbreaking insights and innovative solutions. This has profound implications for our understanding\nof coordinated dance rituals, as it suggests that the emergent patterns of behavior that arise from\nthe interactions among individual dancers can be understood and predicted using mathematical\nframeworks. The implications of our research are far-reaching and multifaceted, with potential applications\nin fields such as psychology, sociology, and computer science. Furthermore, our study has also highlighted the importance of interdisciplinary research,\ndemonstrating that the fusion of seemingly disparate fields can lead to innovative and groundbreaking\ndiscoveries. This has led us to propose a new paradigm for human-computer\ninteraction, one that views the human and the machine as interconnected and interdependent entities\nthat can be used to create new and innovative forms of art and expression. By analyzing the historical and cultural context of flamenco, we have been able to gain a deeper\nunderstanding of the ways in which this dance form has been used as a means of expression and\nresistance, and the ways in which it continues to be an important part of Spanish culture and identity. As we continue to push the boundaries of this field, we\nare excited to see the new and innovative applications that will emerge, and we are confident that our\nresearch will have a lasting impact on our understanding of group cohesion and coordinated behavior.",
        "Results and Findings": "By providing a more objective\nand quantitative means of assessing performance, these technologies can help to identify areas for\nimprovement and optimize the training and rehearsal processes. While these approaches may seem unorthodox,\nthey have reportedly yielded some surprising insights into the nature of group cohesion and the\nfactors that contribute to successful coordinated dance performances. For example, one study found\nthat the patterns of tea leaves could be used to predict the likelihood of a dancer stumbling or making\na mistake, allowing for the development of targeted interventions and improvements to the rehearsal\nprocess. Furthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a\nnumber of unexpected benefits, such as improving the dancers\u2019 ability to communicate with each\nother through subtle cues and gestures. Whether through the use of conventional methods or more unconventional approaches,\nsuch as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture\nforecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating\nand thought-provoking results. Furthermore, a number of studies have investigated the cultural and historical contexts of flamenco\ndance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used\nto preserve and promote traditional flamenco practices. In addition to these developments, there has been a growing interest in the use of AR and LSTM-based\ngesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal\ncoordination in dance. This has involved the use of functional magnetic resonance imaging (fMRI) and\nelectroencephalography (EEG) to study the brain activity of dancers as they engage in synchronized\nperformances, and has yielded some fascinating insights into the neural mechanisms that underlie\nhuman movement and coordination. Moreover, a growing body of research has examined the potential for AR and LSTM-based gesture\nforecasting to facilitate the creation of novel, site-specific flamenco performances that are tailored\nto the unique architectural and environmental features of a given location. This has involved\nthe development of bespoke AR systems that can detect and respond to the spatial and temporal\ncharacteristics of a performance environment, and has yielded some remarkable insights into the\n3ways in which the use of immersive technologies can be used to enhance the sense of presence and\nengagement among audience members. This has involved the development of\nbespoke AR systems that can provide a range of multisensory stimuli to users, and has yielded some\nremarkable insights into the ways in which the use of immersive technologies can enhance the sense\nof presence and engagement among audience members. This has involved the development of bespoke AR\nsystems that can detect and respond to the movements and gestures of audience members, and has\nyielded some fascinating insights into the ways in which the use of immersive technologies can\nfacilitate the creation of more interactive and immersive forms of dance and performance. Interestingly, some researchers have also explored\nthe potential for AR and LSTM-based gesture forecasting to facilitate the development of novel,\nfusion-based flamenco styles that blend traditional techniques with contemporary influences and\ninnovations, highlighting the potential for these emerging technologies to facilitate the creation of\nnew, hybrid forms of cultural expression and identity. These wristbands,\nwhich we designed and fabricated in-house, utilized a combination of accelerometer, gyroscope, and\nmagnetometer sensors to capture the dancers\u2019 movements with high spatial and temporal resolution. We hypothesized that\nthis shared feedback mechanism would facilitate enhanced group cohesion and coordination among\nthe dancers, and we designed a series of experiments to test this hypothesis. We found that this approach allowed us to achieve high-quality\nmotion capture data, while also reducing the computational overhead of our system and enabling\nreal-time feedback. For example, we found that the dancers\u2019 movements were often influenced\nby a range of external factors, including the music, the lighting, and even the color of the walls in\nthe dance studio. We found that this approach\nallowed us to achieve significantly improved accuracy in our gesture forecasting model, and we\nwere able to demonstrate a strong positive correlation between the predicted gestures and the actual\nmovements of the dancers. Another unexpected finding that emerged from our research was the discovery that the dancers\u2019\nmovements were often influenced by a range of subconscious factors, including their emotional\nstate, their level of fatigue, and even their personal relationships with their fellow dancers. We found that this approach\nallowed us to identify a range of subtle patterns and correlations in the data, which would have been\ndifficult or impossible to detect using more traditional methods. This approach involved using a combination of convolutional and\nrecurrent neural network layers to model the temporal and spatial structure of the dancers\u2019 movements,\nand we found that it allowed us to achieve state-of-the-art performance in gesture forecasting and\nrecognition. We found that\nthese modalities provided a rich source of additional information about the dancers\u2019 movements\nand emotional state, and we were able to integrate them into our existing system using a range of\nsensor fusion and machine learning techniques. Despite these challenges, we were able to demonstrate the effectiveness of our approach in a range of\nexperimental evaluations, including a large-scale study involving over 100 participants and a series\nof smaller-scale pilots and proof-of-concept demonstrations. We found that our system was able\nto achieve high levels of accuracy and robustness in gesture forecasting and recognition, and we\nwere able to demonstrate a strong positive correlation between the predicted gestures and the actual\n5movements of the dancers. We also received positive feedback from the participants, who reported\nthat the system was easy to use and provided a range of benefits, including improved coordination and\ncohesion, enhanced creativity and self-expression, and increased overall enjoyment and engagement. 4 Experiments\nTo conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and\nsynchronized flamenco, we designed a series of experiments that would not only assess the impact of\nAR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\nMemory (LSTM) networks. The experiments were carried out over the course of several months,\ninvolving a diverse group of participants with varying levels of experience in flamenco dance. The results were astounding, with the model\u2019s predictions becoming\nincreasingly eclectic and incorporating elements from the ambient music genres. For instance, when\nthe background music shifted to a vibrant salsa rhythm, the model began to predict movements that\n6were distinctly more energetic and spontaneous, diverging significantly from the traditional flamenco\nrepertoire. This finding opens up novel avenues\nfor research, including the potential for using AR and LSTM models to create new, hybrid dance\nforms that blend elements from different cultural traditions. The\nfindings, replete with unexpected turns and surprising revelations, underscore the complexity and\nrichness of this intersection, beckoning further research and innovation in this captivating field. 5 Results\nOur investigation into the intersection of Augmented Reality (AR) and synchronized flamenco\ndancing, with a focus on evaluating group cohesion through LSTM-based gesture forecasting,yielded\na plethora of intriguing results. Initially, we observed that the integration of AR elements into\nthe flamenco performances enhanced the dancers\u2019 ability to synchronize their movements, thereby\nfostering a heightened sense of group cohesion. The LSTM-based gesture forecasting model, trained on a dataset comprising various flamenco dance\nsequences, demonstrated a remarkable capacity to predict the subsequent gestures of individual\n7dancers. Notably, when this predictive capability was leveraged to generate AR cues that guided\nthe dancers\u2019 movements, the overall cohesion of the group improved significantly. Further analysis revealed that the predictive accuracy of the LSTM model was influenced by the\ndancers\u2019 emotional states, as captured through wearable, physiological sensors. Specifically, the\nmodel\u2019s performance improved when the dancers were in a state of heightened arousal or excitement,\nsuggesting that emotional investment in the performance enhances the efficacy of the gesture forecast-\ning. In a bizarre twist, our research team discovered that the LSTM model, when trained on a dataset\nthat included gestures performed by dancers who were blindfolded, developed an uncanny ability to\npredict movements that were not strictly flamenco in nature. While these findings may seem\nillogical or even flawed, they nevertheless contribute to our understanding of the intricate relationships\nbetween gesture, emotion, and AR-augmented performance. The results of our experiments are summarized in the following table: As evidenced by the table, the\nTable 2: LSTM Model Performance Under Various Conditions\nCondition Predictive Accuracy Emotional State Dance Style AR Cue Efficacy\nTraditional Flamenco 0.85 High Arousal Flamenco High\nFusion Dance 0.70 Medium Engagement Hybrid Medium\nBlindfolded Gestures 0.90 Low Arousal Non-Traditional Low\nBallet-Influenced Flamenco 0.60 High Excitement Ballet-Flamenco High\nLSTM model\u2019s performance varies significantly depending on the specific conditions under which it\nis applied. Notably, the model\u2019s predictive accuracy is highest when dealing with traditional flamenco\ngestures, but its ability to generate novel, hybrid movements is most pronounced when confronted\nwith blindfolded gestures or ballet-influenced flamenco. The implications of these findings are far-reaching, suggesting that the integration of AR and LSTM-\nbased gesture forecasting can not only enhance group cohesion in synchronized flamenco but also\nfacilitate the creation of innovative, boundary-pushing dance forms. As our\nresearch continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate\nuncovering even more unexpected and thought-provoking results that challenge our understanding of\nthe complex interplay between technology, movement, and human emotion. This, in turn, will enable the development of more sophisticated AR systems that can\nadapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall\nefficacy and aesthetic appeal of synchronized flamenco performances. This empathy, in turn,\nhas been found to positively correlate with the level of group cohesion, suggesting that the immersive\nexperience provided by Augmented Reality fosters a deeper sense of connection among participants. Furthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to\npredict the intricate hand movements of the dancers, which has been shown to be a critical factor in\nevaluating the overall synchrony of the dance performance. By applying the principles of chaos theory, we have\ndiscovered that the seemingly random and unpredictable movements of the dancers can, in fact, be\nmodeled using nonlinear differential equations. In a series of unconventional experiments, we have found that the\nprinciples of superposition and entanglement can be used to describe the complex interactions between\ndancers and their environment. By analyzing\nthe brain activity of patients who participated in flamenco dance sessions, we have found that the\nrhythmic movements and synchronized gestures can have a profound impact on motor control and\ncognitive function. 11",
        "Conclusion": "Finally, a growing body of research has examined the potential for AR and LSTM-based gesture\nforecasting to facilitate the preservation and promotion of traditional flamenco practices and cultural\nheritage. In conclusion, our research demonstrates the potential of AR and LSTM-based gesture forecasting\nto enhance group cohesion and coordination in coordinated dance rituals. The experiments concluded with a grand finale, where all participants gathered for a final, AR-guided\nflamenco performance. The LSTM model,\nhaving learned from the myriad of experiences and data collected throughout the study, performed\nflawlessly, predicting gestures with a high degree of accuracy and even seeming to contribute to the\nspontaneity and creativity of the performance. 6 Conclusion\nIn culmination of our exhaustive exploration into the realm of Augmented Reality and Synchronized\nFlamenco, it is unequivocally evident that the deployment of LSTM-based gesture forecasting in\ncoordinated dance rituals has yielded a profound impact on the evaluation of group cohesion. Ultimately, our research has demonstrated that the evaluation of group cohesion via LSTM-based\ngesture forecasting in coordinated dance rituals is a rich and complex field that offers a wide range\nof opportunities for exploration and discovery. 10Ultimately, our research has demonstrated that the study of Augmented Reality and Synchronized\nFlamenco is a rich and complex field that offers a wide range of opportunities for exploration and\ndiscovery.",
        "label": 0
    },
    {
        "Abstract": "Detailed Action Identification in Baseball Game\nRecordings\nAbstract\nThis research introduces MLB-YouTube, a new and complex dataset created for\nnuanced activity recognition in baseball videos. This setting is more challenging than segmented videos, requiring\nthe model to identify activity start and end times and handle ambiguous negative examples.",
        "Methodology": "This dataset is structured to\nsupport two types of analysis: one for classifying activities in segmented videos\nand another for detecting activities in unsegmented, continuous video streams. This\nstudy evaluates several methods for recognizing activities, focusing on how they\ncapture the temporal organization of activities in videos. This evaluation starts\nwith categorizing segmented videos and progresses to applying these methods\nto continuous video feeds. Additionally, this paper assesses the effectiveness of\ndifferent models in the challenging task of forecasting pitch velocity and type\nusing baseball broadcast videos. Access to much of this data is restricted from the public\ndomain. Unlike many current datasets for activity recognition or\ndetection, our dataset emphasizes fine-grained activity recognition. The differences between activities\nare often minimal, primarily involving the movement of a single individual, with a consistent scene\nstructure across activities. The determination of activity is based on a single camera perspective. This\nstudy compares various methods for temporal feature aggregation, both for classifying activities in\nsegmented videos and for detecting them in continuous video streams. Initial\nsuccesses were achieved with hand-engineered features such as dense trajectories. Two-stream CNN architectures utilize both spatial RGB frames and optical\nflow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have been\ndeveloped. Several studies have investigated the aggregation of temporal features for the purpose of activity\nrecognition. It\nhas been discovered that pooling intervals from varying locations and durations is advantageous for\nactivity recognition. It was demonstrated that identifying and classifying key sub-event intervals can\nlead to better performance. Recently, segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently\nfor activity detection. These methods depend on the 3D CNN to capture temporal dynamics, which\ntypically span only 16 frames. Our dataset includes two main parts: segmented videos\nintended for activity recognition and continuous videos designed for activity classification. Our dataset for segmented video analysis comprises 4,290 clips. Additionally, clips featuring a pitch\nwere annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. On average, each continuous\nclip contains 7.2 activities, amounting to over 15,000 activity instances in total. In segmented videos, the classification task is simpler because each frame corresponds to\nan activity, eliminating the need for the model to identify the start and end of activities. Our methods\nare based on a CNN that generates a per-frame or per-segment representation, derived from standard\ntwo-stream CNNs using deep CNNs like I3D or InceptionV3. This approach, however, yields a single representation for the entire video,\nlosing temporal information. The pooled features are concatenated, creating a K\u00d7Drepresentation, where K\nis the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip. We also explore learning temporal convolution filters to aggregate local temporal structures. A kernel\nof size L\u00d71is applied to each frame, enabling each timestep representation to incorporate information\nfrom adjacent frames. Previous studies have shown that learning the sub-interval to pool is beneficial for activity recognition. These learned intervals are defined by three parameters: a center g, a width \u03c3, and a stride \u03b4,\nparameterizing NGaussians. Given the video length T, the positions of the strided Gaussians are\nfirst calculated as:\ngn= 0.5\u2212T\u2212(gn+ 1)\nN\u22121forn = 0,1, . , T \u22121\nThe filters are then generated as:\nFm[i, t] =1\nZmexp\u0012\n\u2212(t\u2212\u00b5i,m)2\n2\u03c32m\u0013\ni\u2208 {0,1, . , T \u22121}\nwhere Zmis a normalization constant. We apply these filters Fto the T\u00d7Dvideo representation through matrix multiplication, yielding an\nN\u00d7Drepresentation that serves as input to a fully-connected layer for classification. This method\nis shown in Fig 5(d). Additionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state\nas input to a fully-connected layer for classification. We frame our tasks as multi-label classification\nand train these models to minimize binary cross-entropy:\nL(v) =X\nczclog(p(c|G(v))) + (1 \u2212zc) log(1 \u2212p(c|G(v)))\nwhere G(v)is the function that pools the temporal information, and zcis the ground truth label for\nclass c.\n5 Activity Detection in Continuous Videos\nDetecting activities in continuous videos poses a greater challenge. This necessitates that\nthe model learn to identify the start and end points of activities. As a baseline, we train a single\nfully-connected layer to serve as a per-frame classifier, which does not utilize temporal information\nbeyond that contained in the features. 3We adapt the methods developed for segmented video classification to continuous videos by imple-\nmenting a temporal sliding window approach. We select a fixed window duration of Lfeatures, apply\nmax-pooling to each window (similar to Fig. 5(a)), and classify each pooled segment. Max-pooling is applied to each segment,\nand the pooled features are concatenated, yielding a 14\u00d7D-dimensional representation for each\nwindow, which is then used as input to the classifier. For temporal convolutional models in continuous videos, we modify the segmented video approach by\nlearning a temporal convolutional kernel of length Land convolving it with the input video features. This operation transforms input of size T\u00d7Dinto output of size T\u00d7D, followed by a per-frame\nclassifier. This enables the model to aggregate local temporal information. To extend the sub-event model to continuous videos, we follow a similar approach but set T=Lin\nEq. 1, resulting in filters of length L. The T\u00d7Dvideo representation is convolved with the sub-event\nfilters F, producing an N\u00d7D\u00d7T-dimensional representation used as input to a fully-connected\nlayer for frame classification. This approach involves learning a set of temporal\nstructure filters modeled as NCauchy distributions. Each distribution is defined by a center xnand a\nwidth \u03b3n. Given the video length T, the filters are constructed by:\nxn=(T\u22121)(tanh( x\u2032\nn) + 1)\n2\nfn(t) =1\nZn\u03b3n\n\u03c0((t\u2212xn)2+\u03b32n)exp(1\u22122|tanh( \u03b3\u2032\nn)|)\nwhere Znis a normalization constant, t\u2208 {1,2, . The filters are combined with learned per-class soft-attention weights A, and the super-event repre-\nsentation is computed as:\nSc=X\nnAc,nX\ntfn(t)\u00b7vt\nwhere vis the T\u00d7Dvideo representation. These filters enable the model to focus on relevant\nintervals for temporal context. The super-event representation is concatenated to each timestep and\nused for classification. We also experiment with combining the super- and sub-event representations\nto form a three-level hierarchy for event representation. For InceptionV3, features were computed every 3 frames\n(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\n3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam\noptimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50\nepochs. 46.2 Segmented Video Activity Recognition\nWe initially conducted binary pitch/non-pitch classification for each video segment. This task is\nrelatively straightforward due to the distinct differences between pitch and non-pitch frames. Table 4 compares the\nperformance of these methods. All methods surpass mean/max-pooling, highlighting the importance\nof preserving temporal structure for activity recognition. Learning sub-events, as per previous research,\nyields the best results. While LSTMs and temporal convolutions have been used before, they need\nmore parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitate\nsequential processing of video features, whereas other methods can be fully parallelized. Learning sub-intervals for pooling is found to be crucial for activity recognition. Method RGB Flow Two-stream\nRandom 16.3 16.3 16.3\nInceptionV3 + mean-pool 35.6 47.2 45.3\nInceptionV3 + max-pool 47.9 48.6 54.4\nInceptionV3 + pyramid 49.7 53.2 55.3\nInceptionV3 + LSTM 47.6 55.6 57.7\nInceptionV3 + temporal conv 47.2 55.2 56.1\nInceptionV3 + sub-events 56.2 62.5 62.6\nI3D + mean-pool 42.4 47.6 52.7\nI3D + max-pool 48.3 53.4 57.2\nI3D + pyramid 53.2 56.7 58.7\nI3D + LSTM 48.2 53.1 53.1\nI3D + temporal conv 52.8 57.1 58.4\nI3D + sub-events 55.5 61.2 61.3\nTable 5 shows the average precision for each activity class. Table 5: Per-class average precision for segmented videos using two-stream features in multi-\nlabel activity classification. Utilizing sub-events to discern temporal intervals of interest proves\nadvantageous for activity recognition. Method Ball Strike Swing Hit Foul In Play Bunt Hit by Pitch\nRandom 21.8 28.6 37.4 20.9 11.4 10.3 1.1 4.5\nInceptionV3 + max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7\nInceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2\nI3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2\nI3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0\n6.2.2 Pitch Speed Regression\nEstimating pitch speed from video frames is an exceptionally difficult problem, as it requires the\nnetwork to pinpoint the pitch\u2019s start and end, and derive the speed from a minimal signal. Utilizing the 60fps rate available in YouTube videos, we\nrecalculated optical flow and extracted RGB frames at this higher rate. Method Two-stream\nI3D 4.3 mph\nI3D + LSTM 4.1 mph\nI3D + sub-events 3.9 mph\nInceptionV3 5.3 mph\nInceptionV3 + LSTM 4.5 mph\nInceptionV3 + sub-events 3.6 mph\n6.2.3 Pitch Type Classification\nWe conducted experiments to determine the feasibility of predicting pitch types from video, a task\nmade challenging by pitchers\u2019 efforts to disguise their pitches from batters and the subtle differences\nbetween pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\nutilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN. Pose features were considered due to variations in body mechanics between different pitches. All\nmodels improve upon the baseline per-frame classification, confirming the importance of temporal\ninformation. Method Accuracy\nRandom 17.0%\nI3D 25.8%\nI3D + LSTM 18.5%\nI3D + sub-events 34.5%\nPose 28.4%\nPose + LSTM 27.6%\nPose + sub-events 36.4%\nconvolution appear to overfit. Convolutional sub-events, especially when combined with super-event\nrepresentation, significantly enhance performance, particularly for frame-based features. In the context of activity detection in continuous videos, we establish\nthat incorporating convolutional sub-events with a super-event representation, creating a three-level\nactivity hierarchy, yields the most favorable outcomes.",
        "Results and Findings": "The findings indicate that incorporating temporal\ndynamics into models is beneficial for detailed activity recognition. Research has compared several pooling techniques and determined that both Long Short-\n.Term Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. 3 MLB-YouTube Dataset\nWe have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\non YouTube, totaling over 42 hours of video. Additionally, there is minimal variance\nin motion and appearance among different activities, such as swinging a bat versus bunting. In\ncontrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities\nwith diverse settings, scales, and camera angles, our dataset features activities where a single frame\nmight not be adequate to determine the activity. Given that a single clip may contain\nseveral activities, this is considered a multi-label classification task. Our continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Every\nframe in these videos is annotated with the baseball activities that occur. Activity Count\nNo Activity 2983\nBall 1434\nStrike 1799\nSwing 2506\nHit 1391\nFoul 718\nIn Play 679\nBunt 24\nHit by Pitch 14\n24 Segmented Video Recognition Approach\nWe investigate different techniques for aggregating temporal features in segmented video activity\nrecognition. 5(a). . . , N \u22121\npt,n=gn+ (t\u22120.5T+ 0.5)1\n\u03b4fort = 0,1, . . . . . . . The goal here is to classify each\nframe according to the activities occurring. Unlike segmented videos, continuous videos feature\nmultiple sequential activities, often interspersed with frames of inactivity. . . , T }, and n\u2208 {1,2, . . . Frames were extracted at 25 fps, and TVL1 optical flow\nwas computed and clipped to [\u221220,20]. The\nresults, detailed in Table 2, reveal minimal variation across different features or models. Fixed temporal pyramid pooling and LSTMs\nshow some improvement. Model # Parameters\nMax/Mean Pooling 16K\nPyramid Pooling 115K\nLSTM 10.5M\nTemporal Conv 31.5M\nSub-events 36K\nTable 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification. 6 and Fig. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,\nproving insufficient for speed determination. Employing a fully-connected\nlayer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\nand actual speeds, we achieved an average error of 3.6mph. Table 6: Results for pitch speed regression on segmented videos, reporting root-mean-squared errors. Our\ndataset includes six pitch types, with results presented in Table 7. LSTMs performed worse than the\nbaseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the\neasiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult\n(12%). 6.3 Continuous Video Activity Detection\nWe evaluate models extended for continuous videos using per-frame mean average precision (mAP),\nwith results shown in Table 8. Method RGB Flow Two-stream\nRandom 13.4 13.4 13.4\nI3D 33.8 35.1 34.2\nI3D + max-pooling 34.9 36.4 36.8\nI3D + pyramid 36.8 37.5 39.7\nI3D + LSTM 36.2 37.3 39.4\nI3D + temporal conv 35.2 38.1 39.2\nI3D + sub-events 35.5 37.5 38.5\nI3D + super-events 38.7 38.6 39.1\nI3D + sub+super-events 38.2 39.4 40.4\nInceptionV3 31.2 31.8 31.9\nInceptionV3 + max-pooling 31.8 34.1 35.2\nInceptionV3 + pyramid 32.2 35.1 36.8\nInceptionV3 + LSTM 32.1 33.5 34.1\nInceptionV3 + temporal conv 28.4 34.4 33.4\nInceptionV3 + sub-events 32.1 35.8 37.3\nInceptionV3 + super-events 31.5 36.2 39.6\nInceptionV3 + sub+super-events 34.2 40.2 40.9\n7 Conclusion\nThis paper introduces MLB-YouTube, a novel and challenging dataset designed for detailed activity\nrecognition in videos. We conduct a comparative analysis of various recognition techniques that\nemploy temporal feature pooling for both segmented and continuous videos. Our findings reveal that\nlearning sub-events to pinpoint temporal regions of interest significantly enhances performance in\nsegmented video classification. 7",
        "Conclusion": "5(c). , N \u22121}, t\u2208 {0,1, . , N }. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable\nfeature representation. 7.",
        "label": 1
    },
    {
        "Abstract": "",
        "Methodology": "The lack of comprehensive, high-fidelity data, coupled with limited\ncollaborative efforts between academic and industrial sectors, has significantly\nhindered advancements in this domain. This study addresses these obstacles by\nintroducing the MetaFood Challenge, aimed at generating precise, volumetrically\naccurate 3D food models from 2D images, utilizing a checkerboard for size cal-\nibration. A\ntotal of 16 teams participated in the final assessment phase. The methodologies\ndeveloped during this challenge have yielded highly encouraging outcomes in\n3D food reconstruction, showing great promise for refining portion estimation in\ndietary evaluations and nutritional tracking. By concentrating on the development of accurate 3D models of food derived from various visual\ninputs, including multiple views and single perspectives, this challenge endeavors to bridge the\ndisparity between current methodologies and practical needs. It promotes the creation of unique\nsolutions capable of managing the intricacies of food morphology, texture, and illumination, while also\nmeeting the real-world demands of dietary evaluation. These advancements have the potential to substantially enhance the precision and utility of food\nportion estimation across diverse applications, from individual health tracking to extensive nutritional\ninvestigations. Conventional methods for assessing diet, like 24-Hour Recall or Food Frequency Questionnaires\n(FFQs), are frequently reliant on manual data entry, which is prone to inaccuracies and can be\nburdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression-\nbased methods for estimating food portions directly from images of eating occasions. The challenge was segmented into three tiers of difficulty based\non the number of images provided: approximately 200 images for easy, 30 for medium, and a single\ntop-view image for hard. This design aimed to rigorously test the adaptability and resilience of\nproposed solutions under various realistic conditions. The resulting solutions hold the potential to profoundly influence nutritional intake monitoring and\ncomprehension, supporting broader health and wellness objectives. Accurate portion size estimation requires understanding the volume and density of food,\nelements that are hard to deduce from a 2D image, underscoring the need for sophisticated techniques\nto tackle this problem. Current methods for estimating food portions are grouped into four categories. Stereo-Based Approaches use multiple images to reconstruct the 3D structure of food. Some methods\nestimate food volume using multi-view stereo reconstruction based on epipolar geometry, while\nothers perform two-view dense reconstruction. Model-Based Approaches use predefined shapes and templates to estimate volume. For instance,\ncertain templates are assigned to foods from a library and transformed based on physical references to\nestimate the size and location of the food. Template matching approaches estimate food volume from\na single image, but they struggle with variations in food shapes that differ from predefined templates. Recent work has used 3D food meshes as templates to align camera and object poses for portion size\nestimation. Deep Learning Approaches utilize neural networks trained on large image datasets for portion\nestimation. Regression networks estimate the energy value of food from single images or from an\n\"Energy Distribution Map\" that maps input images to energy distributions. Some networks use both\nimages and depth maps to estimate energy, mass, and macronutrient content. However, deep learning\nmethods require extensive data for training and are not always interpretable, with performance\ndegrading when test images significantly differ from training data. While these methods have advanced food portion estimation, they face limitations that hinder their\nwidespread use and accuracy. Stereo-based methods are impractical for single images, model-based\napproaches struggle with diverse food shapes, depth camera methods need specialized hardware,\nand deep learning approaches lack interpretability and struggle with out-of-distribution samples. These benefits motivated the\norganization of the 3D Food Reconstruction challenge, aiming to overcome existing limitations and\n2develop more accurate, user-friendly, and widely applicable food portion estimation techniques,\nimpacting nutritional assessment and dietary monitoring. To ensure precise size accuracy\nin the reconstructed 3D models, each food item was captured alongside a checkerboard and pattern\nmat, serving as physical scaling references. The challenge is divided into three levels of difficulty,\ndetermined by the quantity of 2D images provided for reconstruction:\n\u2022 Easy: Around 200 images taken from video. 3.2.2 Phase-II: Shape Accuracy\nTeams that perform well in Phase-I are asked to submit complete 3D mesh files for each food item. This phase involves several steps to ensure precision and fairness:\n\u2022Model Verification: Submitted models are checked against the final Phase-I submissions for\nconsistency, and visual inspections are conducted to prevent rule violations. They must align their models with the ground truth and prepare a\ntransformation matrix for each submitted object. \u2022Chamfer Distance Calculation: Shape accuracy is assessed using the Chamfer distance\nmetric. Given two point sets XandY, the Chamfer distance is defined as:\ndCD(X, Y ) =1\n|X|X\nx\u2208Xmin\ny\u2208Y\u2225x\u2212y\u22252\n2+1\n|Y|X\ny\u2208Ymin\nx\u2208X\u2225x\u2212y\u22252\n2 (2)\nThis metric offers a comprehensive measure of similarity between the reconstructed 3D models and\nthe ground truth. The final ranking is determined by combining scores from both Phase-I (volume\naccuracy) and Phase-II (shape accuracy). 4 First Place Team - VolETA\n4.1 Methodology\nThe team\u2019s research employs multi-view reconstruction to generate detailed food meshes and calculate\nprecise food volumes. 4.1.1 Overview\nThe team\u2019s method integrates computer vision and deep learning to accurately estimate food volume\nfrom RGBD images and masks. Keyframe selection ensures data quality, supported by perceptual\nhashing and blur detection. Camera pose estimation and object segmentation pave the way for neural\nsurface reconstruction, creating detailed meshes for volume estimation. Refinement steps, including\nisolated piece removal and scaling factor adjustments, enhance accuracy. This approach provides a\nthorough solution for accurate food volume assessment, with potential uses in nutrition analysis. The RGBD images, denoted as ID={IDi}n\ni=1, where nis the total number of frames,\nprovide depth information alongside RGB images. Next, the team selects keyframes. From the set {IDi}n\ni=1, keyframes {IK\nj}k\nj=1\u2286 {IDi}n\ni=1are\nchosen. A method is implemented to detect and remove duplicate and blurry images, ensuring\nhigh-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fourier\ntransform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-\ning to detect similar images and retain overlapping ones. Duplicates and blurry images are excluded\nto maintain data integrity and accuracy. Using the selected keyframes {IK\nj}k\nj=1, the team estimates camera poses through a method called\nPixSfM, which involves extracting features using SuperPoint, matching them with SuperGlue, and\nrefining them. 4In parallel, the team uses a tool called SAM for reference object segmentation. SAM segments\nthe reference object with a user-provided prompt, producing a reference object mask MRfor each\nkeyframe. This mask helps track the reference object across all frames. The XMem++ method\nextends the reference object mask MRto all frames, creating a comprehensive set of reference object\nmasks {MR\ni}n\ni=1. This ensures consistent reference object identification throughout the dataset. To create RGBA images, the team combines RGB images, reference object masks {MR\ni}n\ni=1, and\nfood object masks {MF\ni}n\ni=1. This step, denoted as {IR\ni}n\ni=1, integrates various data sources into a\nunified format for further processing. The team converts the RGBA images {IR\ni}n\ni=1and camera poses {Cj}k\nj=1into meaningful metadata\nand modeled data Dm. The modeled data Dmis input into NeuS2 for mesh reconstruction. The team uses the\n\"Remove Isolated Pieces\" technique to refine the meshes. Given that the scenes contain only one food\nitem, the diameter threshold is set to 5% of the mesh size. This method deletes isolated connected\ncomponents with diameters less than or equal to 5%, resulting in a cleaned mesh {RCf, RCr}. This\nstep ensures that only significant parts of the mesh are retained. The team manually identifies an initial scaling factor Susing the reference mesh via MeshLab. This\nfactor is fine-tuned to Sfusing depth information and food and reference masks, ensuring accurate\nscaling relative to real-world dimensions. To address this, the team\nmanually determines the scaling factor by measuring the distance for each block of the reference\nobject mesh. The average of all block lengths lavgis calculated, while the actual real-world length is\nconstant at lreal= 0.012meters. The scaling factor S=lreal/lavgis applied to the clean food mesh\nRCf, resulting in the final scaled food mesh RFfin meters. The team uses depth information along with food and reference object masks to validate the scaling\nfactors. The method for assessing food size involves using overhead RGB images for each scene. Initially, the pixel-per-unit (PPU) ratio (in meters) is determined using the reference object. Subse-\nquently, the food width ( fw) and length ( fl) are extracted using a food object mask. To determine the\nfood height ( fh), a two-step process is followed. First, binary image segmentation is performed using\nthe overhead depth and reference images, yielding a segmented depth image for the reference object. The average depth is then calculated using the segmented reference object depth ( dr). Similarly,\nemploying binary image segmentation with an overhead food object mask and depth image, the\naverage depth for the segmented food depth image ( df) is computed. To assess the accuracy of the scaling factor S, the food\nbounding box volume (fw\u00d7fl\u00d7fh)\u00d7PPU is computed. The team evaluates if the scaling factor\nSgenerates a food volume close to this potential volume, resulting in Sfine. For one-shot 3D reconstruction, the team uses One-2-3-45 to reconstruct a 3D model from a single\nRGBA view input after applying binary image segmentation to both food RGB and mask images. Isolated pieces are removed from the generated mesh, and the scaling factor S, which is closer to the\npotential volume of the clean mesh, is reused. The\nHamming distance for near image similarity was set to 12. The diameter for removing isolated pieces\nwas set to 5%. The team\u2019s approach was\napplied separately to each food scene. A one-shot food volume estimation approach was used if\nthe number of keyframes kequaled 1; otherwise, a few-shot food volume estimation was applied. Notably, the keyframe selection process chose 34.8% of the total frames for the rest of the pipeline,\nshowing the minimum frames with the highest information. After generating scaled meshes,\nthe team calculated volumes and Chamfer distance with and without transformation metrics. Meshes\nwere registered with ground truth meshes using ICP to obtain transformation metrics. For overall method performance, Table 4 shows\nthe MAPE and Chamfer distance with and without transformation metrics. \u2022Input requirements: The method requires extensive input information, including food\nmasks and depth data. Streamlining these inputs would simplify the process and increase\napplicability. \u2022Complex backgrounds and objects: The method has not been tested in environments with\ncomplex backgrounds or highly intricate food objects. They aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve\nefficiency. w/o t.m\n(%) sum mean sum mean\n10.973 0.130 0.007 1.715 0.095\n5 Second Place Team - ININ-VIAUN\n5.1 Methodology\nThis section details the team\u2019s proposed network, illustrating the step-by-step process from original\nimages to final mesh models. 5.1.1 Scale factor estimation\nThe procedure for estimating the scale factor at the coordinate level is illustrated in Figure 9. The\nteam adheres to a method involving corner projection matching. Specifically, utilizing the COLMAP\ndense model, the team acquires the pose of each image along with dense point cloud data. For any\ngiven image imgkand its extrinsic parameters [R|t]k, the team initially performs threshold-based\ncorner detection, setting the threshold at 240. This step allows them to obtain the pixel coordinates\nof all detected corners. Subsequently, using the intrinsic parameters kand the extrinsic parameters\n[R|t]k, the point cloud is projected onto the image plane. Thus, they can calculate the distance between any two corners as follows:\nDk\nij= (Pk\ni\u2212Pk\nj)2\u2200i\u0338=j (3)\nTo determine the final computed length of each checkerboard square in image k, the team takes the\nminimum value of each row of the matrix Dk(excluding the diagonal) to form the vector dk. The\nmedian of this vector is then used. The first fifteen objects are processed using one pipeline, while the\nlast five single-view objects are processed using another. For the initial fifteen objects, the team uses COLMAP to estimate poses and segment the food using\nthe provided segment masks. Advanced multi-view 3D reconstruction methods are then applied to\nreconstruct the segmented food. The team employs three different reconstruction methods: COLMAP,\nDiffusioNeRF, and NeRF2Mesh. They select the best reconstruction results from these methods and\nextract the mesh. The extracted mesh is scaled using the estimated scale factor, and optimization\ntechniques are applied to obtain a refined mesh. For the last five single-view objects, the team experiments with several single-view reconstruction\nmethods, including Zero123, Zero123++, One2345, ZeroNVS, and DreamGaussian. They choose\nZeroNVS to obtain a 3D food model consistent with the distribution of the input image. The\nintrinsic camera parameters from the fifteenth object are used, and an optimization method based\non reprojection error refines the extrinsic parameters of the single camera. Due to limitations in\nsingle-view reconstruction, depth information from the dataset and the checkerboard in the monocular\nimage are used to determine the size of the extracted mesh. The\nLaplacian Smoothing method adjusts the position of each vertex to the average of its neighboring\nvertices:\nV(new)\ni =V(old)\ni+\u03bb\uf8eb\n\uf8ed1\n|N(i)|X\nj\u2208N(i)V(old)\nj\u2212V(old)\ni\uf8f6\n\uf8f8 (5)\nIn their implementation, the smoothing factor \u03bbis set to 0.2, and 10 iterations are performed. Each image and the\ncorresponding reconstructed 3D model yield a scale factor, and the table presents the average scale\nfactor for each object. 5.2.2 Reconstructed meshes\nThe refined meshes obtained using the described methods are shown in Figure 12. 5.2.3 Alignment\nThe team designs a multi-stage alignment method for evaluating reconstruction quality. Figure 13\nillustrates the alignment process for Object 14. First, the central points of both the predicted and\nground truth models are calculated, and the predicted model is moved to align with the central point\nof the ground truth model. Next, ICP registration is performed for further alignment, significantly\nreducing the Chamfer distance. Finally, gradient descent is used for additional fine-tuning to obtain\nthe final transformation matrix. The total Chamfer distance between all 18 predicted models and the ground truths is 0.069441169. For simple and medium complexity cases, they employed a structure-from-\nmotion strategy to ascertain the pose of each image, followed by mesh reconstruction. Subsequently,\na sequence of post-processing steps was implemented to recalibrate the scale and improve mesh\nquality. For cases involving only a single image, the team utilized image generation techniques to\nfacilitate model generation. 6.1.1 Multi-View Reconstruction\nFor Structure from Motion (SfM), the team enhanced the advanced COLMAP method by integrating\nSuperPoint and SuperGlue techniques. In the mesh reconstruction phase, the team\u2019s approach builds upon 2D Gaussian Splatting, which\nemploys a differentiable 2D Gaussian renderer and includes regularization terms for depth distortion\n9and normal consistency. During post-processing, the team applied filtering and outlier removal methods, identified the outline\nof the supporting surface, and projected the lower mesh vertices onto this surface. They utilized\nthe reconstructed checkerboard to correct the model\u2019s scale and employed Poisson reconstruction to\ncreate a complete, watertight mesh of the subject. 6.1.2 Single-View Reconstruction\nFor 3D reconstruction from a single image, the team utilized advanced methods such as LGM, Instant\nMesh, and One-2-3-45 to generate an initial mesh. This initial mesh was then refined in conjunction\nwith depth structure information. To adjust the scale, the team estimated the object\u2019s length using the checkerboard as a reference,\nassuming that the object and the checkerboard are on the same plane. They then projected the 3D\nobject back onto the original 2D image to obtain a more precise scale for the object. This optimization\naimed to align the two meshes as closely as possible in three-dimensional space. The competition involved 20 diverse food items, captured under various conditions and with differing\nnumbers of input images, specifically designed to challenge participants in creating robust reconstruc-\ntion models. The evaluation was based on a two-phase process, assessing both portion size accuracy\nthrough Mean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distance\nmetric. Of all participating teams, three reached the final submission stage, presenting a range of innovative\nsolutions. The novel methods developed by the\nparticipating teams establish a strong foundation for future research in this area, potentially leading\nto more precise and user-friendly approaches for dietary assessment and monitoring.",
        "Results and Findings": "Advancements in 3D Food Modeling: A Review of the\nMetaFood Challenge Techniques and Outcomes\nAbstract\nThe growing focus on leveraging computer vision for dietary oversight and nutri-\ntion tracking has spurred the creation of sophisticated 3D reconstruction methods\nfor food. The remainder of this report is structured as follows: Section 2 delves into the\nexisting literature on food portion size estimation, Section 3 describes the dataset and evaluation\nframework used in the challenge, and Sections 4, 5, and 6 discuss the methodologies and findings of\nthe top three teams (V olETA, ININ-VIAUN, and FoodRiddle), respectively. 3D\nreconstruction offers a promising solution by providing comprehensive spatial information, adapting\nto various shapes, potentially working with single images, offering visually interpretable results,\nand enabling a standardized approach to food portion estimation. \u2022 Medium: 30 images. Note that after the Phase-I evaluation, quality issues were\nfound with the data for object 12 (steak) and object 15 (chicken nugget), so these items were excluded\nfrom the final overall evaluation. 4.2 Experimental Results\n4.2.1 Implementation settings\nExperiments were conducted using two GPUs: GeForce GTX 1080 Ti/12G and RTX 3060/6G. For Gaussian kernel radius, even numbers\nin the range [0...30] were used for detecting blurry images. 54.2.2 VolETA Results\nThe team extensively validated their approach on the challenge dataset and compared their results\nwith ground truth meshes using MAPE and Chamfer distance metrics. Table 2: List of Extracted Information Using RGBD and Masks\nLevel Id Label Sf PPU Rw\u00d7Rl (fw\u00d7fl\u00d7fh)\n1 Strawberry 0.08955223881 0.01786 320\u00d7360 (238 \u00d7257\u00d72.353)\n2 Cinnamon bun 0.1043478261 0.02347 236\u00d7274 (363 \u00d7419\u00d72.353)\n3 Pork rib 0.1043478261 0.02381 246\u00d7270 (435 \u00d7778\u00d71.176)\nEasy 4 Corn 0.08823529412 0.01897 291\u00d7339 (262 \u00d7976\u00d72.353)\n5 French toast 0.1034482759 0.02202 266\u00d7292 (530 \u00d7581\u00d72.53)\n6 Sandwich 0.1276595745 0.02426 230\u00d7265 (294 \u00d7431\u00d72.353)\n7 Burger 0.1043478261 0.02435 208\u00d7264 (378 \u00d7400\u00d72.353)\n8 Cake 0.1276595745 0.02143 256\u00d7300 (298 \u00d7310\u00d74.706)\n9 Blueberry muffin 0.08759124088 0.01801 291\u00d7357 (441 \u00d7443\u00d72.353)\n10 Banana 0.08759124088 0.01705 315\u00d7377 (446 \u00d7857\u00d71.176)\nMedium 11 Salmon 0.1043478261 0.02390 242\u00d7269 (201 \u00d7303\u00d71.176)\n13 Burrito 0.1034482759 0.02372 244\u00d7271 (251 \u00d7917\u00d72.353)\n14 Frankfurt sandwich 0.1034482759 0.02115 266\u00d7304 (400 \u00d71022\u00d72.353)\n16 Everything bagel 0.08759124088 0.01747 306\u00d7368 (458 \u00d7134\u00d71.176)\nHard 17 Croissant 0.1276595745 0.01751 319\u00d7367 (395 \u00d7695\u00d72.176)\n18 Shrimp 0.08759124088 0.02021 249\u00d7318 (186 \u00d795\u00d70.987)\n19 Waffle 0.01034482759 0.01902 294\u00d7338 (465 \u00d7537\u00d70.8)\n20 Pizza 0.01034482759 0.01913 292\u00d7336 (442 \u00d7651\u00d71.176)\nAfter finding keyframes, PixSfM estimated the poses and point cloud. Additionally, qualitative results on one- and few-shot 3D reconstruction from the challenge dataset\nare shown. Limitations: Despite promising results, several limitations need to be addressed in future work:\n\u2022Manual processes: The current pipeline includes manual steps like providing segmentation\nprompts and identifying scaling factors, which should be automated to enhance efficiency. w/o t.m\n1 40.06 38.53 1.63 85.40\n2 216.9 280.36 7.12 111.47\n3 278.86 249.67 13.69 172.88\nE 4 279.02 295.13 2.03 61.30\n5 395.76 392.58 13.67 102.14\n6 205.17 218.44 6.68 150.78\n7 372.93 368.77 4.70 66.91\n8 186.62 173.13 2.98 152.34\n9 224.08 232.74 3.91 160.07\n10 153.76 163.09 2.67 138.45\nM 11 80.4 85.18 3.37 151.14\n13 363.99 308.28 5.18 147.53\n14 535.44 589.83 4.31 89.66\n16 163.13 262.15 18.06 28.33\nH 17 224.08 181.36 9.44 28.94\n18 25.4 20.58 4.28 12.84\n19 110.05 108.35 11.34 23.98\n20 130.96 119.83 15.59 31.05\nTable 4: Quantitative Comparison with Ground Truth Using MAPE and Chamfer Distance\nMAPE Ch. Based on the pixel coordinates of the\ncorners, the team can identify the closest point coordinates Pk\nifor each corner, where irepresents the\nindex of the corner. 5.2 Experimental Results\n5.2.1 Estimated scale factor\nThe scale factors estimated using the described method are shown in Table 5. 8Table 5: Estimated Scale Factors\nObject Index Food Item Scale Factor\n1 Strawberry 0.060058\n2 Cinnamon bun 0.081829\n3 Pork rib 0.073861\n4 Corn 0.083594\n5 French toast 0.078632\n6 Sandwich 0.088368\n7 Burger 0.103124\n8 Cake 0.068496\n9 Blueberry muffin 0.059292\n10 Banana 0.058236\n11 Salmon 0.083821\n13 Burrito 0.069663\n14 Hotdog 0.073766\nTable 6: Metric of V olume\nObject Index Predicted V olume Ground Truth Error Percentage\n1 44.51 38.53 15.52\n2 321.26 280.36 14.59\n3 336.11 249.67 34.62\n4 347.54 295.13 17.76\n5 389.28 392.58 0.84\n6 197.82 218.44 9.44\n7 412.52 368.77 11.86\n8 181.21 173.13 4.67\n9 233.79 232.74 0.45\n10 160.06 163.09 1.86\n11 86.0 85.18 0.96\n13 334.7 308.28 8.57\n14 517.75 589.83 12.22\n16 176.24 262.15 32.77\n17 180.68 181.36 0.37\n18 13.58 20.58 34.01\n19 117.72 108.35 8.64\n20 117.43 119.83 20.03\n6 Best 3D Mesh Reconstruction Team - FoodRiddle\n6.1 Methodology\nTo achieve high-fidelity food mesh reconstruction, the team developed two procedural pipelines as\ndepicted in Figure 14. 6.2 Experimental Results\nThrough a process of nonlinear optimization, the team sought to identify a transformation that\nminimizes the Chamfer distance between their mesh and the ground truth mesh. As shown in Table 7, Team FoodRiddle achieved the best scores for\nboth multi- view and single-view reconstructions, outperforming other teams in the competition. Table 7: Total Errors for Different Teams on Multi-view and Single-view Data\nTeam Multi-view (1-14) Single-view (16-20)\nFoodRiddle 0.036362 0.019232\nININ-VIAUN 0.041552 0.027889\nV olETA 0.071921 0.058726\n7 Conclusion\nThis report examines and compiles the techniques and findings from the MetaFood Workshop\nchallenge on 3D Food Reconstruction. Team V olETA secured first place with the best overall performance in both Phase-I and\nPhase-II, followed by team ININ-VIAUN in second place. Additionally, the FoodRiddle team\nexhibited superior performance in Phase-II, highlighting a competitive and high-caliber field of\nentries for 3D mesh reconstruction. 10",
        "Conclusion": "The final Chamfer distance is calculated\nusing these models and matrices. Finally, the fine-tuned scaling factor Sfis applied to the\ncleaned food mesh RCf, producing the final scaled food mesh RFf. This step culminates in an\naccurately scaled 3D representation of the food object, enabling precise volume estimation. GT V ol. Ch. w/ t.m Ch. w/ t.m Ch. Finally, optimization techniques are\napplied to obtain a refined mesh. Upon completion\nof this process, the average Chamfer dis- tance across the final reconstructions of the 20 objects\namounted to 0.0032175 meters.",
        "label": 1
    },
    {
        "Abstract": "Advanced techniques for through and contextually\nInterpreting Noun-Noun Compounds\nAbstract\nThis study examines the effectiveness of transfer learning and multi-task learning\nin the context of a complex semantic classification problem: understanding the\nmeaning of noun-noun compounds. 3 Task Definition and Dataset\nThe objective of this task is to train a model to categorize the semantic relationships between pairs\nof nouns in a labeled dataset, where each pair forms a noun-noun compound. For clarity, we differentiate between transfer learning and multi-task learning in this paper, despite\nthese terms sometimes being used interchangeably in the literature. 5 Neural Classification Models\nThis section introduces the neural classification models utilized in our experiments. 5.1 Single-Task Learning Model\nIn our single-task learning (STL) configuration, we train and fine-tune a feed-forward neural network\ninspired by the neural classifier proposed by Dima and Hinrichs (2015). Note that relations not predicted by any model are excluded from the macro-\naverage calculation. L&R: Completely unseen.",
        "Methodology": "Furthermore, we illustrate\nhow utilizing dual annotations, which involve two distinct sets of relations applied\nto the same compounds, can enhance the overall precision of a neural classifier and\nimprove its F1 scores for less common yet more challenging semantic relations. Given the prevalence of noun-noun compounds in natural language and its\nsignificance to other natural language processing (NLP) tasks like question answering and information\nretrieval, understanding noun-noun compounds has been extensively studied in theoretical linguistics,\npsycholinguistics, and computational linguistics. In computational linguistics, noun-noun compound interpretation is typically treated as an automatic\nclassification task. These models utilize information from lexical semantics, like\nWordNet-based features, and distributional semantics, such as word embeddings. Our aim is not only to add to the existing\nresearch on the effectiveness of TL and MTL for semantic NLP tasks generally but also to ascertain\ntheir specific advantages for compound interpretation. A key reason for utilizing multi-task learning is to enhance generalization by making use of the\ndomain-specific details present in the training data of related tasks. 2 Related Work\nApproaches to interpreting noun-noun compounds differ based on the classification of compound\nrelations, as well as the machine learning models and features employed to learn these relations. Some researchers challenge the idea that noun-noun compounds can be interpreted using a fixed,\npredetermined set of relations, proposing alternative methods based on paraphrasing. We center\nour attention on methods that frame the interpretation problem as a classification task involving a\nfixed, predetermined set of relations. Various machine learning models have been applied to this\ntask, including nearest neighbor classifiers that use semantic similarity based on lexical resources,\nkernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy\nmodels that incorporate a wide range of lexical and surface form features, and neural networks that\nrely on word embeddings or combine word embeddings with path embeddings. Several recent studies have conducted extensive experiments on the application of TL and MTL to a\nvariety of NLP tasks, such as named entity recognition, semantic labeling, sentence-level sentiment\nclassification, super-tagging, chunking, and semantic dependency parsing. Our research aligns with certain studies in\nthat we apply TL and MTL to learn different semantic annotations of noun-noun compounds using\nthe same dataset. However, our experimental design is more akin to other work in that we experiment\nwith initializing parameters across all layers of the neural network and concurrently train a single\nMTL model on two sets of relations. This means that each noun-noun compound is associated\nwith two different relations, each based on different linguistic theories. We chose this dataset for two primary reasons: firstly, the dual annotation of\nrelations on the same set of compounds is ideal for exploring TL and MTL approaches; secondly,\naligning two different annotation frameworks on the same data allows for a comparative analysis\nacross these frameworks. Specifically, we use a portion of the dataset, focusing on type-based instances of two-word compounds. The original dataset also encompasses multi-word compounds (those made up of more than two\nnouns) and multiple instances per compound type. We further divide the dataset into three parts:\ntraining, development, and test sets. Table 1 details the number of compound types and the vocabulary\nsize for each set, including a breakdown of words appearing in the right-most (right constituents)\nand left-most (left constituents) positions. However, some relations that are theoretically similar do not align well in practice. Nevertheless, it is reasonable to assume that the semantic similarity\nin the label sets, where it exists, can be leveraged through transfer and multi-task learning, especially\nsince the overall distribution of relations differs between the two frameworks. 4 Transfer vs. Multi-Task Learning\nIn this section, we employ the terminology and definitions established by Pan and Yang (2010) to\narticulate our framework for transfer and multi-task learning. Our goal is to learn a function f(X) that predicts Y based on the input features X. Considering two ML tasks, Ta and Tb, we would train two distinct models to learn separate functions\nfa and fb for predicting Ya and Yb in a single-task learning scenario. However, if Ta and Tb are\nrelated, either explicitly or implicitly, TL and MTL can enhance the generalization of either or both\ntasks. In contrast, MTL involves\ntraining parts of the same model to learn both Ta and Tb, essentially learning one set of parameters\nfor both tasks. The concept is to train a single model simultaneously on both tasks, where one task\nintroduces an inductive bias that aids the model in generalizing over the main task. It is important to\nnote that this does not necessarily imply that we aim to use a single model to predict both label sets\nin practice. Subsequently, we employ this same model to implement TL and MTL. This network comprises four\nlayers: 1) an input layer, 2) an embedding layer, 3) a hidden layer, and 4) an output layer. The input\n3layer consists of two integers that indicate the indices of a compound\u2019s constituents in the embedding\nlayer, where the word embedding vectors are stored. These selected vectors are then passed to a fully\nconnected hidden layer, the size of which matches the dimensionality of the word embedding vectors. Finally, a softmax function is applied to the output layer to select the most probable relation. The compound\u2019s constituents are represented using a 300-dimensional word embedding model trained\non an English Wikipedia dump and the English Gigaword Fifth Edition. If a word is not found during lookup in the embedding model, we\ncheck if the word is uppercased and attempt to find the lowercase version. For hyphenated words\nnot found in the embedding vocabulary, we split the word at the hyphen and average the vectors of\nits parts, if they are present in the vocabulary. If the word remains unrepresented after these steps, a\ndesignated vector for unknown words is employed. The weights of the embedding layer\nare updated during the training of all models. We utilize the Adaptive Moment Estimation (Adam)\noptimization function across all models, with a learning rate set to 0.001. The loss function employed\nis the negative-log likelihood. A Sigmoid activation function is used for the units in the hidden layer. All models are trained with mini-batches of size five. The maximum number of epochs is capped\nat 50, but an early stopping criterion based on the model\u2019s accuracy on the validation split is also\nimplemented. This means that training is halted if the validation accuracy does not improve over five\nconsecutive epochs. All models are implemented in Keras, using TensorFlow as the backend. The TL\nand MTL models are trained using the same hyperparameters as the STL model. 5.2 Transfer Learning Models\nIn our experiments, transfer learning involves training an STL model on PCEDT relations and then\nusing some of its weights to initialize another model for NomBank relations. Given the neural\nclassifier architecture detailed in Section 5.1, we identify three ways to implement TL: 1) TLE:\nTransferring the embedding layer weights, 2) TLH: Transferring the hidden layer weights, and 3)\nTLEH: Transferring both the embedding and hidden layer weights. Furthermore, we differentiate\nbetween transfer learning from PCEDT to NomBank and vice versa. This results in six setups,\nas shown in Table 2. 5.3 Multi-Task Learning Models\nIn MTL, we train a single model to simultaneously learn both PCEDT and NomBank relations,\nmeaning all MTL models have two objective functions and two output layers. We distinguish between the auxiliary and main tasks based on which\nvalidation accuracy (NomBank\u2019s or PCEDT\u2019s) is monitored by the early stopping criterion. This\nleads to a total of four MTL models, as shown in Table 3. All models were trained solely on the training split. Firstly, the accuracy of the STL models decreases when evaluated on the test split for both\nNomBank and PCEDT. This could suggest overfitting, especially\nsince our stopping criterion selects the model with the best performance on the development split. Conversely, TL and MTL enhance accuracy on the test splits, despite using the same stopping criterion\nas STL. We interpret this as an improvement in the models\u2019 ability to generalize. Similarly, of the 26 PCEDT relations in the test split, only\nsix are predicted. However, the unpredicted relations are extremely rare in the training split (e.g., 23\nPCEDT functors appear less than 20 times), making it doubtful whether any ML model could learn\nthem under any circumstances. Given this imbalanced distribution, it is evident that accuracy alone is insufficient to determine the\nbest-performing model. 7.2 Per-Relation F1 Scores\nTables 4 and 5 present the per-relation F1 scores for NomBank and PCEDT, respectively. ACT TWHEN APP PAT REG RSTR\nCount 89 14 118 326 216 900\nSTL 43.90 42.11 22.78 42.83 20.51 68.81\nTLE 49.37 70.97 27.67 41.60 30.77 69.67\nTLH 53.99 62.07 25.00 43.01 26.09 68.99\nTLEH 49.08 64.52 28.57 42.91 28.57 69.08\nMTLE 54.09 66.67 24.05 42.03 27.21 69.31\nMTLF 47.80 42.11 25.64 40.73 19.22 68.89\nSeveral noteworthy patterns emerge from Tables 4 and 5. Firstly, the MTLF model appears to be\ndetrimental to both datasets, leading to significantly degraded F1 scores for four NomBank relations,\nincluding the locative modifier ARGM-LOC and the manner modifier ARGM-MNR (abbreviated as\nLOC and MNR in Table 4), which the model fails to predict altogether. This same model exhibits\nthe lowest F1 score compared to all other models for two PCEDT relations: REG (expressing a\ncircumstance) and PAT (patient). Considering that the MTLF model achieves the highest accuracy\non the NomBank test split (as shown in Table 3), it becomes even more apparent that relying solely\non accuracy scores is inadequate for evaluating the effectiveness of TL and MTL for this task and\ndataset. This outcome can be partially understood\nby examining the correspondence matrices between NomBank arguments and PCEDT functors,\npresented in Tables 7 and 6. These tables illustrate how PCEDT functors map to NomBank arguments\nin the training split (Table 6) and vice versa (Table 7). Table 6 reveals that 80% of the compounds\nannotated as TWHEN in PCEDT were annotated as ARGM-TMP in NomBank. While this mapping is not as distinct\nas one might hope, it is still relatively high when compared to how other PCEDT relations map to\nARG0. Nevertheless, even such\nimperfect correspondences can provide a training signal that assists the TL and MTL models in\nlearning relations like TWHEN and ACT. Overall, TL and MTL from NomBank to PCEDT\nare more helpful than the reverse. This suggests that the weights learned to predict PCEDT relations\n6Table 6: Correspondence matrix between PCEDT functors and NomBank arguments. Slots with \u2019-\u2019\nindicate zero, 0.00 represents a very small number but not zero. A1 A2 A0 A3 LOC TMP MNR\nRSTR 0.70 0.11 0.06 0.06 0.02 0.01 0.02\nPAT 0.90 0.05 0.01 0.02 0.01 - 0.00\nREG 0.78 0.10 0.04 0.06 0.00 0.00 0.00\nAPP 0.62 0.21 0.13 0.02 0.01 0.00 -\nACT 0.47 0.03 0.47 0.01 0.01 - 0.01\nAIM 0.65 0.12 0.07 0.06 0.01 - -\nTWHEN 0.10 0.03 - - - 0.80 -\nCount 3617 1312 777 499 273 116 59\nTable 7: Correspondence matrix between NomBank arguments and PCEDT functors. Conversely, the mapping from NomBank to\nPCEDT shows that although many NomBank arguments map to RSTR in PCEDT, the percentages\nare lower, making the mapping more diverse and discriminative, which seems to aid TL and MTL\nmodels in learning less frequent PCEDT relations. To understand why the PCEDT functor AIM is never predicted despite being more frequent than\nTWHEN, we found that AIM is almost always misclassified as RSTR by all models. Furthermore,\nAIM and RSTR have the highest lexical overlap in the training set among all PCEDT relation pairs:\n78.35% of left constituents and 73.26% of right constituents of compounds annotated as AIM occur\nin other compounds annotated as RSTR. Recent\nresearch suggests that gains in noun-noun compound interpretation using word embeddings and\nsimilar neural classification models might be due to lexical memorization. In other words, the models\nlearn that specific nouns are strong indicators of specific relations. To assess the role of lexical\nmemorization in our models, we quantify the number of unseen compounds that the STL, TL, and\nMTL models predict correctly. We differentiate between \u2019partly\u2019 and \u2019completely\u2019 unseen compounds. A compound is \u2019partly\u2019\nunseen if one of its constituents (left or right) is not present in the training data. Table 9 compares the performance\nof the different models on these three groups in terms of the proportion of compounds misclassified\nin each group. L: Left constituent. R: Right constituent. NomBank PCEDT\nModel L R L&R L R L&R\nCount 351 286 72 351 286 72\nSTL 27.92 39.51 50.00 45.01 47.55 41.67\nTLE 25.93 36.71 48.61 43.87 47.55 41.67\nTLH 26.21 38.11 50.00 46.15 49.30 47.22\nTLEH 26.50 38.81 52.78 45.87 47.55 43.06\nMTLE 24.50 33.22 38.89 44.44 47.20 43.06\nMTLF 22.79 34.27 40.28 44.16 47.90 38.89\nTable 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce\ngeneralization error in NomBank across all scenarios, with the exception of TLH and TLEH for\ncompletely unseen compounds, where error increases. The greatest error reductions are achieved\nby MTL models across all three types of unseen compounds. Specifically, MTLE reduces the error\nby approximately six points for compounds with unseen right constituents and by eleven points for\nfully unseen compounds. Moreover, MTLF reduces the error by five points when the left constituent\nis unseen. It\u2019s important to interpret these results in conjunction with the Count row in Table 9 for\na comprehensive view. For example, the eleven-point error decrease in fully unseen compounds\nrepresents eight compounds. Compared to the STL model, MTLE reduces generalization error for\ncompletely unseen compounds by a total of eight compounds, of which seven are annotated with the\nrelation ARG1, which is the most common in NomBank. Regarding the unseen right constituents,\nMTLE\u2019s 24 improved compounds consist of 18 ARG1, 5 ARG0, and 1 ARG2 compounds. A large portion of unseen compounds, whether partly or entirely unseen, that were misclassified by\nevery model, were not of type ARG1 in NomBank, or RSTR in PCEDT. This pattern, along with\ncorrectly predicted unseen compounds primarily annotated with the most common relations, suggests\nthat classification models rely on lexical memorization to learn the compound relation interpretation. Its ratio is calculated as its proportion in the full set of left or right constituents for each\n8relation. This potentially makes learning the former easier if the model\nsolely relies on lexical memorization. These relations also\nhave the second-highest F1 score in their datasets\u2014except for STL on PCEDT (see Tables 4 and\n5). Based on these insights, we can\u2019t dismiss the possibility that our models show some degree\nof lexical memorization, despite manual analysis also presenting cases where models demonstrate\ngeneralization and correct predictions in situations where lexical memorization is impossible. This research endeavors to clarify the benefits of TL and\nMTL in the context of semantic interpretation of noun-noun compounds. Overall, our TL, and particularly our MTL models,\nare better at making predictions both quantitatively and qualitatively. Notably, the improvements are\nobserved on the \u2019most challenging\u2019 inputs that include at least one constituent that was not present in\nthe training data. Typically, the transfer of representations or sharing between tasks is more effective at the embedding\nlayers, which represent the model\u2019s internal representation of the compound constituents. Furthermore,\nin multi-task learning, the complete sharing of model architecture across tasks degrades its capacity\nto generalize when it comes to less frequent relations. The dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound\ninterpretation because it links this sub-problem with broad-coverage semantic role labeling or\nsemantic dependency parsing in PCEDT and NomBank. Future research will focus on incorporating\nadditional natural language processing tasks defined using these frameworks to understand noun-noun\ncompound interpretation using TL and MTL.",
        "Results and Findings": "Through a series of detailed experiments and\nan in-depth analysis of errors, we demonstrate that employing transfer learning by\ninitializing parameters and multi-task learning through parameter sharing enables a\nneural classification model to better generalize across a dataset characterized by a\nhighly uneven distribution of semantic relationships. The application of transfer learning (TL) and multi-task learning (MTL) in NLP has gained significant\nattention in recent years, yielding varying outcomes based on the specific tasks, model architectures,\nand datasets involved. These varying results, combined with the fact that neither TL nor MTL has\nbeen previously applied to noun-noun compound interpretation, motivate our thorough empirical\ninvestigation into the use of TL and MTL for this task. In this study, we demonstrate that\nTL and MTL can serve as a form of regularization, enabling the prediction of infrequent relations\nwithin a dataset marked by a highly skewed distribution of relations. Through meticulous analysis of results, we discover that TL and MTL, especially when applied\nto the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a\nhighly skewed dataset, compared to a robust single-task learning baseline. Although our research\nconcentrates on TL and MTL, we present, to our knowledge, the first experimental results on the\nrelatively recent dataset from Fares (2016). Among these studies,\nsome have utilized the same dataset. The consensus among\nthese studies is that the advantages of TL and MTL are largely contingent on the characteristics of the\ntasks involved, including the unevenness of the data distribution, the semantic relatedness between\nthe source and target tasks, the learning trajectory of the auxiliary and main tasks (where target tasks\nthat quickly reach a plateau benefit most from non-plateauing auxiliary tasks), and the structural\nsimilarity between the tasks. As discussed in Section 7.1, these label sets have a highly\nuneven distribution. Train Dev Test\nCompounds 6932 920 1759\nV ocab size 4102 1163 1772\nRight constituents 2304 624 969\nLeft constituents 2405 618 985\nMany relations in PCEDT and NomBank conceptually describe similar semantic ideas, as they are\nused to annotate the semantics of the same text. Two tasks are deemed related when their domains are similar but their label sets differ, or when\ntheir domains are dissimilar but their label sets are identical. The embedding model was\ntrained by Fares et al. 6 Experimental Results\nTables 2 and 3 display the accuracies of the various TL and MTL models on the development and test\nsplits for NomBank and PCEDT. Several insights can be gleaned from these\ntables. Secondly, all TL models achieve improved accuracy on the NomBank test\nsplit, although transfer learning does not significantly enhance accuracy on the development split of\nthe same dataset. The MTL models, especially MTLF, have a detrimental effect on the development\naccuracy of NomBank, yet we observe a similar improvement, as with TL, on the test split. Thirdly,\nboth TL and MTL models demonstrate less consistent effects on PCEDT (on both development and\ntest splits) compared to NomBank. For instance, all TL models yield an absolute improvement of\n4about 1.25 points in accuracy on NomBank, whereas in PCEDT, TLE clearly outperforms the other\ntwo TL models (TLE improves over the STL accuracy by 1.37 points). Model NomBank PCEDT\nDev Test Dev Test\nSTL 78.15 76.75 58.80 56.05\nTLE 78.37 78.05 59.57 57.42\nTLH 78.15 78.00 59.24 56.51\nTLEH 78.48 78.00 59.89 56.68\nTable 3: Accuracy (%) of the MTL models. Model NomBank PCEDT\nDev Test Dev Test\nSTL 78.15 76.75 58.80 56.05\nMTLE 77.93 78.45 59.89 56.96\nMTLF 76.74 78.51 58.91 56.00\nOverall, the STL models\u2019 accuracy declines when tested on the NomBank and PCEDT test splits,\ncompared to their performance on the development split. However, since\nthese improvements are relatively minor, we further analyze the results to understand if and how TL\nand MTL are beneficial. 7 Results Analysis\nThis section provides a detailed analysis of the models\u2019 performance, drawing on insights from the\ndataset and the classification errors made by the models. The discussion in the following sections is\nprimarily based on the results from the test split, as it is larger than the development split. In fact, out of the 15 NomBank relations observed in the test split, five are never predicted\nby any of the STL, TL, or MTL models. Therefore, in the subsequent section, we report and analyze the F1 scores of\nthe predicted NomBank and PCEDT relations across all STL, TL, and MTL models. We only\ninclude results for relations that are actually predicted by at least one of the models. 5Table 4: Per-label F1 score on the NomBank test split. A0 A1 A2 A3 LOC MNR TMP\nCount 132 1282 153 75 25 25 27\nSTL 49.82 87.54 45.78 60.81 28.57 29.41 66.67\nTLE 55.02 87.98 41.61 60.14 27.91 33.33 63.83\nTLH 54.81 87.93 42.51 60.00 25.00 35.29 65.31\nTLEH 53.62 87.95 42.70 61.11 29.27 33.33 65.22\nMTLE 54.07 88.34 42.86 61.97 30.00 28.57 66.67\nMTLF 53.09 88.41 38.14 62.69 00.00 00.00 52.17\nTable 5: Per-label F1 score on the PCEDT test split. Secondly, with the exception of the MTLF model, all TL and MTL models consistently improve\nthe F1 score for all PCEDT relations except PAT. Additionally, 47% of\nACT (Actor) relations map to ARG0 (Proto-Agent) in NomBank. Since the TLE model outperforms STL in predicting REG by ten absolute points, we examined\nall REG compounds correctly classified by TLE but misclassified by STL. RSTR PAT REG APP ACT AIM TWHEN\nA1 0.51 0.54 0.12 0.06 0.03 0.02 0.00\nA2 0.47 0.09 0.11 0.14 0.01 0.02 0.00\nA0 0.63 0.03 0.07 0.13 0.26 0.02 -\nA3 0.66 0.08 0.13 0.03 0.01 0.02 -\nLOC 0.36 0.07 0.02 0.05 0.03 0.01 -\nTMP 0.78 - 0.01 0.01 - - 0.01\nMNR 0.24 0.05 0.01 - 0.03 - -\nCount 4932 715 495 358 119 103 79\noffer little to no inductive bias for NomBank relations. Model NomBank PCEDT\nSTL 52.66 40.15\nTLE 52.83 48.34\nTLH 52.98 46.52\nTLEH 53.31 47.12\nMTLE 53.21 47.23\nMTLF 42.07 40.73\nFinally, to demonstrate the benefits of TL and MTL for NomBank and PCEDT, we report the F1\nmacro-average scores in Table 8. Table 8 clearly shows that TL and MTL on the embedding layer yield significant\nimprovements for PCEDT, with about a 7-8 point increase in macro-average F1, compared to just\n0.65 in the best case for NomBank. 77.3 Generalization on Unseen Compounds\nWe now analyze the models\u2019 ability to generalize to compounds not seen during training. Overall, nearly 20% of the compounds in the test split have an unseen left constituent, about 16%\nhave an unseen right constituent, and 4% are completely unseen. Upon manual inspection of compounds that led to substantial reductions in the generalization error,\nspecifically within NomBank, we examined the distribution of relations within correctly predicted\nunseen compound sets. We define a relation-specific\nconstituent as a left or right constituent that appears with only one specific relation within the training\ndata. Lexical memorization is therefore a likely cause of these high F1 scores. We also observed that\nlower ratios of relation-specific constituents correlate with lower F1 scores, such as APP and REG in\nPCEDT. By executing a sequence of\nminimally contrasting experiments and conducting thorough analysis of results and prediction errors,\nwe demonstrate how both TL and MTL can mitigate the effects of class imbalance and drastically\nenhance predictions for low-frequency relations. 9",
        "Conclusion": "2. (2017). We found that STL\nmisclassified them as RSTR, indicating that TL from NomBank helps TLE recover from STL\u2019s\novergeneralization in RSTR prediction. 8 Conclusion\nThe application of transfer and multi-task learning in natural language processing has gained sig-\nnificant traction, yet considerable ambiguity persists regarding the effectiveness of particular task\ncharacteristics and experimental setups.",
        "label": 2
    },
    {
        "Abstract": "The Importance of Written Explanations in\nAggregating Crowdsourced Predictions\nAbstract\nThis study demonstrates that incorporating the written explanations provided by\nindividuals when making predictions enhances the accuracy of aggregated crowd-\nsourced forecasts. Related to the current study\u2019s focus,\nresearchers have examined the influence of Wikipedia editors and studied influence levels within\nonline communities. Additionally, this\napproach prioritizes privacy, as no information about the individual forecasters is utilized. The median\nlength is relatively short, consisting of one sentence and 23 tokens. The table presents the accuracy (average percentage of days a question is predicted\ncorrectly) for all questions and for each quartile of difficulty: Q1 (easiest 25%), Q2 (25-50%), Q3\n(50-75%), and Q4 (hardest 25%). **Questions** An analysis of the 88 questions in the test set revealed that questions called incorrectly\non at least one day by the best model tend to have a shorter duration (69.4 days vs. 81.7 days) and a\nhigher number of active forecasts per day (31.0 vs. 26.7). * A lack of a logical argument\nwas prevalent in the justifications, regardless of the prediction\u2019s accuracy.",
        "Methodology": "Furthermore, the study analyzes the attributes that differentiate reliable and\nunreliable justifications. While generally supported, the idea is not without its limitations. In the current era, the advantages of collective intelligence are widely utilized. When compiling information from\nlarge groups, it is important to determine whether the individual inputs were made independently. Each forecast includes a prediction, estimating the probability of a particular event, and\na written justification that explains the reasoning behind the prediction. Forecasts with identical\npredictions can have justifications of varying strength, which, in turn, affects the perceived reliability\nof the predictions. For instance, a justification that simply refers to an external source without\nexplanation may appear to rely heavily on the prevailing opinion of the crowd and might be considered\nweaker than a justification that presents specific, verifiable facts from external resources. Questions have a defined start and\nend date, and the period between these dates constitutes the \"life\" of the question. Two strategies are employed for calling questions each day\nthroughout their life: considering forecasts submitted on the given day (\"daily\") and considering the\nlast forecast submitted by each forecaster (\"active\").Inspired by prior research on recognizing and fostering skilled forecasters, and analyzing written\njustifications to assess the quality of individual or collective forecasts, this paper investigates the\nautomated calling of questions throughout their duration based on the forecasts available each day. (Yes) * Does the accuracy of the prediction improve when considering the question itself\nand the written justifications provided with the forecasts? (Yes) * Is it easier to make an accurate\nprediction toward the end of a question\u2019s duration? (Yes) * Are written justifications more valuable\nwhen the crowd\u2019s predictions are less accurate? (Yes)\nIn addition, this research presents an examination of the justifications associated with both accurate\nand inaccurate forecasts. This analysis aims to identify the features that contribute to a justification\nbeing more or less credible. Prior research includes\nboth predictive models (using language samples to predict attributes about the author) and models\nthat provide valuable insights (using language samples and author attributes to identify differentiating\nlinguistic features). In this paper, models are constructed to predict outcomes\nbased on crowd-sourced forecasts without knowledge of individual forecasters\u2019 identities. For instance, studies have analyzed language patterns in social networks, online commu-\nnities, and corporate emails to understand how individuals in positions of authority communicate. The work presented here complements these previous studies. The goal is to identify credible justifications to improve the aggregation of crowdsourced forecasts,\nwithout explicitly targeting any of the aforementioned characteristics. A strong justification for a forecast can be considered a well-reasoned supporting argument. Previous work in this area includes identifying argument components such as claims, premises,\nbacking, rebuttals, and refutations, as well as mining arguments that support or oppose a particular\nclaim. Despite these efforts, it was found that crowdsourced justifications rarely adhere to these\nestablished argumentation frameworks, even though such justifications are valuable for aggregating\nforecasts. These studies aim to identify and cultivate\nsuperforecasters but do not incorporate the written justifications accompanying forecasts. In contrast,\nthe present research develops models to call questions without using any information about the\nforecasters themselves. Within the field of computational linguistics, researchers have evaluated the\nlanguage used in high-quality justifications, focusing on aspects like rating, benefit, and influence. Other researchers have developed models to predict forecaster skill using the textual justifications\nfrom specific datasets, such as the Good Judgment Open data, and have also applied these models\nto predict the accuracy of individual forecasts in other contexts, such as company earnings reports. 23 Dataset\nThe research utilizes data from the Good Judgment Open, a platform where questions are posted, and\nindividuals submit their forecasts. For this study, all\nbinary questions were collected, along with their associated forecasts, each comprising a prediction\nand a justification. In total, the dataset contains 441 questions and 96,664 forecasts submitted\nover 32,708 days. This dataset significantly expands upon previous research, nearly doubling the\nnumber of forecasts analyzed. Since the objective is to accurately call questions throughout their\nentire duration, all forecasts with written justifications are included, regardless of factors such as\njustification length or the number of forecasts submitted by a single forecaster. Most questions are relatively long, contain two\nor more named entities, and are open for over one month. Metric Min Q1 Q2 (Median) Q3 Max Mean\n# tokens 8 16 20 28 48 21.94\n# entities 0 2 3 5 11 3.47\n# verbs 0 2 2 3 6 2.26\n# days open 2 24 59 98 475 74.16\nTable 1 provides a basic analysis of the questions in the dataset. The majority of questions are\nrelatively lengthy, containing more than 16 tokens and multiple named entities, with geopolitical,\nperson, and date entities being the most frequent. In terms of duration, half of the questions remain\nopen for nearly two months, and 75% are open for more than three weeks. Although not explicitly represented in the LDA topics, the questions address both\ndomestic and international events within these broad themes. Table 2: Analysis of the 96,664 written justifications submitted by forecasters in our dataset. The\nreadability scores indicate that most justifications are easily understood by high school students (11th\nor 12th grade), although a substantial amount (>25%) require a college education (Flesch under 50 or\nDale-Chall over 9.0). Justifications mention named\nentities less frequently than the questions themselves. Interestingly, half of the justifications contain\nat least one negation, and 25% include three or more. This suggests that forecasters sometimes base\ntheir predictions on events that might not occur or have not yet occurred. The sentiment polarity of\n3the justifications is generally neutral. In terms of readability, both the Flesch and Dale-Chall scores\nsuggest that approximately a quarter of the justifications require a college-level education for full\ncomprehension. The input consists of the question itself and the associated forecasts (predictions and\njustifications), while the output is an aggregated answer to the question derived from all forecasts. The number of instances corresponds to the total number of days all questions were open. Both\nsimple baselines and a neural network are employed, considering both (a) daily forecasts and (b)\nactive forecasts submitted up to ten days prior. The questions are divided into training, validation, and test subsets. Subsequently, all forecasts\nsubmitted throughout the duration of each question are assigned to their respective subsets. It\u2019s\nimportant to note that randomly splitting the forecasts would be an inappropriate approach. The \"majority vote\" baseline determines the answer to a\nquestion based on the most frequent prediction among the forecasts. The \"weighted vote\" baseline,\non the other hand, assigns weights to the probabilities in the predictions and then aggregates them. 4.2 Neural Network Architecture\nA neural network architecture is employed, which consists of three main components: one to generate\na representation of the question, another to generate a representation of each forecast, and an LSTM\nto process the sequence of forecasts and ultimately call the question. The representation of a forecast is created by concatenating\nthree elements: (a) a binary flag indicating whether the forecast was submitted on the day the question\nis being called or on a previous day, (b) the prediction itself (a numerical value between 0.0 and 1.0),\nand (c) a representation of the justification. The LSTM has a hidden state with a dimensionality of 256 and processes the sequence of forecasts\nas its input. During the tuning process, it was discovered that providing the representation of the\nquestion alongside each forecast is more effective than processing forecasts independently of the\nquestion. Consequently, the representation of the question is concatenated with the representation of\neach forecast before being fed into the LSTM. * The prediction and\nthe representation of the justification. Days When the Question Was Open\nModel All Days Q1 Q2 Q3 Q4\nUsing Daily Forecasts Only\nBaselines\nMajority V ote (predictions) 71.89 64.59 66.59 73.26 82.22\nWeighted V ote (predictions) 73.79 67.79 68.71 74.16 83.61\nNeural Network Variants\nPredictions Only 77.96 77.62 77.93 78.23 78.61\nPredictions + Question 77.61 75.44 76.77 78.05 81.56\nPredictions + Justifications 80.23 77.87 78.65 79.26 84.67\nPredictions + Question + Justifications 79.96 78.65 78.11 80.29 83.28\nUsing Active Forecasts\nBaselines\nMajority V ote (predictions) 77.27 68.83 73.92 77.98 87.44\nWeighted V ote (predictions) 77.97 72.04 72.17 78.53 88.22\nNeural Network Variants\nPredictions Only 78.81 77.31 78.04 78.53 81.11\nPredictions + Question 79.35 76.05 78.53 79.56 82.94\nPredictions + Justifications 80.84 77.86 79.07 79.74 86.17\nPredictions + Question + Justifications 81.27 78.71 79.81 81.56 84.67\nDespite their relative simplicity, the baseline methods achieve commendable results, demonstrating\nthat aggregating forecaster predictions without considering the question or justifications is a viable\nstrategy. **Using Daily or Active Forecasts** Incorporating active forecasts, rather than solely relying on\nforecasts submitted on the day the question is called, proves advantageous for both baselines and all\nneural network configurations, except for the one using only predictions and justifications. **Encoding Questions and Justifications** The neural network that only utilizes the prediction\nto represent a forecast surpasses both baseline methods. Notably, integrating the question, the\njustification, or both into the forecast representation yields further improvements. This suggests that while modeling questions and justifications is generally\nhelpful, it becomes detrimental toward the end of a question\u2019s life. Question Difficulty (Based on Best Baseline)\nAll Q1 Q2 Q3 Q4\nUsing Active Forecasts\nWeighted V ote Baseline (Predictions) 77.97 99.40 99.55 86.01 29.30\nNeural Network with Components... This helps to understand which questions benefit most from the neural\nnetworks that incorporate questions and justifications. However, it\u2019s important to note that calculating\nquestion difficulty during the question\u2019s active period is not feasible, making these experiments\nunrealistic before the question closes and the correct answer is revealed. The weighted vote\nbaseline demonstrates superior performance for 75\n5 Qualitative Analysis\nThis section provides insights into the factors that make questions more difficult to forecast and\nexamines the characteristics of justifications associated with incorrect and correct predictions. This suggests that the model\u2019s errors align\nwith the questions that forecasters also find challenging. The following observations were made:\n* A higher percentage of incorrect predictions (78%) were accompanied by short justifications\n(fewer than 20 tokens), compared to 65% for correct predictions. * References to previous forecasts (either by the\nsame or other forecasters, or the current crowd\u2019s forecast) were more common in justifications for\nincorrect predictions (31.5%) than for correct predictions (16%). However, it was more\nfrequent in justifications for incorrect predictions (62.5%) than for correct predictions (47.5%). *\nSurprisingly, justifications with generic arguments did not clearly differentiate between incorrect and\ncorrect predictions (16.0% vs. 14.5%). * Poor grammar and spelling or the use of non-English were\ninfrequent but more common in justifications for incorrect predictions (24.5%) compared to correct\npredictions (14.5%). This study focuses on\nquestions spanning the political, economic, and social domains, utilizing forecasts submitted by a\ncrowd of individuals without specialized training. Each forecast comprises a prediction and a natural\nlanguage justification. 6The research demonstrates that aggregating the weighted predictions of forecasters is a solid baseline\nfor calling a question throughout its duration. However, models that incorporate both the question\nand the justifications achieve significantly better results, particularly during the first three quartiles of\na question\u2019s life. Importantly, the models developed in this study do not profile individual forecasters\nor utilize any information about their identities. This work lays the groundwork for evaluating the\ncredibility of anonymous forecasts, enabling the development of robust aggregation strategies that do\nnot require tracking individual forecasters.",
        "Results and Findings": "The primary contributions are empirical findings that address the following research questions:\n* When making a prediction on a specific day, is it advantageous to include forecasts from previous\ndays? 2 Related Work\nThe language employed by individuals is indicative of various characteristics. Table 1: Analysis of the questions from our dataset. Min Q1 Q2 Q3 Max\n#sentences 1 1 1 3 56\n#tokens 1 10 23 47 1295\n#entities 0 0 2 4 154\n#verbs 0 1 3 6 174\n#adverbs 0 0 1 3 63\n#adjectives 0 0 2 4 91\n#negation 0 0 1 3 69\nSentiment -2.54 0 0 0.20 6.50\nReadability\nFlesch -49.68 50.33 65.76 80.62 121.22\nDale-Chall 0.05 6.72 7.95 9.20 19.77\nTable 2 presents a fundamental analysis of the 96,664 forecast justifications in the dataset. Regarding verbs and nouns, an analysis using WordNet lexical files reveals that the most common\nverb classes are \"change\" (e.g., \"happen,\" \"remain,\" \"increase\"), \"social\" (e.g., \"vote,\" \"support,\"\n\"help\"), \"cognition\" (e.g., \"think,\" \"believe,\" \"know\"), and \"motion\" (e.g., \"go,\" \"come,\" \"leave\"). 4.4 Quantitative Results\nThe evaluation metric used is accuracy, which represents the average percentage of days a model\ncorrectly calls a question throughout its duration. Results are reported for all days combined, as well\nas for each of the four quartiles of the question\u2019s duration. Table 3: Results with the test questions (Accuracy: average percentage of days a model predicts a\nquestion correctly). Results are provided for all days a question was open and for four quartiles (Q1:\nfirst 25% of days, Q2: 25-50%, Q3: 50-75%, and Q4: last 25% of days). However, the full neural network achieves significantly improved results. These results\nindicate that incorporating the question and forecaster-provided justifications into the model enhances\nthe accuracy of question calling. **Calling Questions Throughout Their Life** When examining the results across the four quartiles of\na question\u2019s duration, it\u2019s observed that while using active forecasts is beneficial across all quartiles\nfor both baselines and all network configurations, the neural networks surprisingly outperform the\nbaselines only in the first three quartiles. In the last quartile, the neural networks perform significantly\nworse than the baselines. This phenomenon can be attributed\nto the increasing wisdom of the crowd as more evidence becomes available and more forecasters\ncontribute, making their aggregated predictions more accurate. 5Table 4: Results with the test questions, categorized by question difficulty as determined by the best\nbaseline model. Table 4 presents the results for selected models based on question difficulty. This supports the idea that longer\nuser-generated text often indicates higher quality. 7",
        "Conclusion": "The research shows that while majority and weighted vote\nmethods are effective, the inclusion of written justifications improves forecast\naccuracy throughout most of a question\u2019s duration, with the exception of its final\nphase. Finally, several studies have focused on forecasting using datasets similar or identical to the one used\nin this research. Finally, the last hidden state of the LSTM is connected\nto a fully connected layer with a single neuron and sigmoid activation to produce the final prediction\nfor the question. 6 Conclusions\nForecasting involves predicting future events, a capability highly valued by both governments and\nindustries as it enables them to anticipate and address potential challenges.",
        "label": 2
    },
    {
        "Abstract": "Detecting Medication Usage in Parkinson\u2019s Disease Through\nMulti-modal Indoor Positioning: A Pilot Study in a Naturalistic\nEnvironment\nAbstract\nParkinson\u2019s disease (PD) is a progressive neurodegenerative disorder that leads to motor symptoms, including gait\nimpairment. A\ncommon complication associated with levodopa, the primary medication for PD, is the emergence of motor fluctuations that are\nlinked to medication timing. A sensitive and accurate ecologically-validated biomarker for PD progression is currently unavailable, which has contributed to\nfailures in clinical trials for neuroprotective therapies in PD. For a baseline comparison to the in-home gait speed features, demographic features which include age, gender, years of PD, and\nMDS-UPDRS III score (the gold-standard clinical rating scale score used in clinical trials to measure motor disease severity in\nPD) are chosen. **Ethical approval:** Full approval from the NHS Wales Research Ethics Committee was granted on December 17, 2019, and\nHealth Research Authority and Health and Care Research Wales approval was confirmed on January 14, 2020; the research was\n3conducted in accord with the Helsinki Declaration of 1975; written informed consent was gained from all study participants. Suppose\nwe have a collection of RSSI signals xr= [xr\n1, xr\n2, ..., xr\nT]\u2208RT\u00d7rand accelerometer data xa= [xa\n1, xa\n2, ..., xa\nT]\u2208RT\u00d7awithin\nTtime units, where xr\nt= [xr\nt1, xr\nt2, ..., xr\ntr]represents RSSI signals from raccess points, and xa\nt= [xa\nt1, xa\nt2, ..., xa\nta]represents\naccelerometer data from aspatial directions at time twitht < T . )denotes the binary cross-entropy, y= [y1, ..., y T]\u2208RTis the\nactual room locations, and f= [f1, ..., f T]\u2208RTis the binary value whether at time tthe room is the referenced room or not. - MDCSA1,4,7 4APS, as an ablation study, with our proposed\nnetwork (i.e., MDCSA1,4,7) using 4 access points for the RSSI (instead of 10 access points) and accelerometer data (ACCL) as its\ninput features. - MDCSA1,4,7 RSSI, as an ablation study, with our proposed network using only RSSI, without ACCL, as its input\nfeatures. - MDCSA1,4,7 4APS RSSI, as an ablation study, with our proposed network using only 4 access points for the RSSI as its\ninput features. **Limitations and future research:** One limitation of this study is the relatively small sample size (which was planned as this is\nan exploratory pilot study).",
        "Methodology": "This paper proposes a transformer-based method that\nuses both Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices to enhance\nindoor localization accuracy. A secondary goal is to determine if indoor localization, particularly in-home gait\nspeed features (like the time to walk between rooms), can be used to identify motor fluctuations by detecting if a\nperson with PD is taking their levodopa medication or not. The method is evaluated using a real-world dataset\ncollected in a free-living setting, where movements are varied and unstructured. Consequently, individuals may be inclined to remain confined to a single room, and when they do move, they may require more time\nto transition between rooms. Clinical evaluations of PD are typically conducted in artificial clinic or laboratory settings,\nwhich only capture a limited view of an individual\u2019s motor function. Continuous monitoring could capture symptom progression,\nincluding motor fluctuations, and sensitively quantify them over time. Localization could also provide valuable information in the measurement of other behaviors such as non-motor symptoms like\nurinary function (e.g., how many times someone visits the toilet room overnight).IoT-based platforms with sensors capturing various modalities of data, combined with machine learning, can be used for unobtrusive\nand continuous indoor localization in home environments. These\nsignals estimate the user\u2019s position based on perceived signal strength, creating radio-map features for each room. Our proposed network\nintelligently selects accelerometer features that can enhance RSSI performance in indoor localization. To rigorously assess our\nmethod, we utilize a free-living dataset (where individuals live without external intervention) developed by our group, encompassing\ndiverse and unstructured movements as expected in real-world scenarios. 2 Related Work\nExtensive research has utilized home-based passive sensing systems to evaluate how the activities and behavior of individuals with\nneurological conditions, primarily cognitive dysfunction, change over time. Cameras can\nalso detect parkinsonian gait and some gait features, including step length and average walking speed. Time-of-flight devices,\nwhich measure distances between the subject and the camera, have been used to assess medication adherence through gait analysis. From free-living data, one approach to gait and room use evaluation in home settings is by emitting and detecting radio waves to\nnon-invasively track movement. However, this approach cannot identify who is doing the movement and also suffers from technical issues\nwhen the radio waves are occluded by another object. Much of the work done so far using video to track PD symptoms has focused\non the performance of structured clinical rating scales during telemedicine consultations as opposed to naturalistic behavior, and\nthere have been some privacy concerns around the use of video data at home. RSSI data from wearable devices is a type of data with fewer privacy concerns; it can be measured continuously and unobtrusively\nover long periods to capture real-world function and behavior in a privacy-friendly way. In indoor localization, fingerprinting using\nRSSI is the typical technique used to estimate the wearable (user) location by using signal strength data representing a coarse and\nnoisy estimate of the distance from the wearable to the access point. However, many techniques have been proposed in recent years to tackle these fluctuations\nand indirectly improve localization accuracy. Some works utilize deep neural networks (DNN) to generate coarse positioning\nestimates from RSSI signals, which are then refined by a hidden Markov model (HMM) to produce a final location estimate. A CNN is used to build localization models to further leverage the temporal dependencies across time-series readings. It has been suggested that we cannot rely on RSSI alone for indoor localization in home environments for PD subjects due to\nshadowing rooms with tight separation. Some researchers combine RSSI signals and inertial measurement unit (IMU) data to test\nthe viability of leveraging other sensors in aiding the positioning system to produce a more accurate location estimate. Others combine\nsmartphone IMU sensor data and Wi-Fi-received signal strength indication (RSSI) measurements to estimate the exact location (in\nEuclidean position X, Y) of a person in indoor environments. The proposed sensor fusion framework uses location fingerprinting in\ncombination with a pedestrian dead reckoning (PDR) algorithm to reduce positioning errors. Looking at this multi-modality classification/regression problem from a time series perspective, there has been a lot of exploration\nin tackling a problem where each modality can be categorized as multivariate time series data. LSTM and attention layers are\noften used in parallel to directly transform raw multivariate time series data into a low-dimensional feature representation for each\nmodality. Later, various processes are done to further extract correlations across modalities through the use of various layers (e.g.,\nconcatenation, CNN layer, transformer, self-attention). Our work is inspired by prior research where we only utilize accelerometer\n2data to enrich the RSSI, instead of utilizing all IMU sensors, in order to reduce battery consumption. In addition, unlike previous\nwork that stops at predicting room locations, we go a step further and use room-to-room transition behaviors as features for a binary\nclassifier predicting whether people with PD are taking their medications or withholding them. Each AP records the\ntransmitted packets from the wearable sensor, which contains the accelerometer readings sampled at 30Hz, with each AP recording\nRSSI values sampled at 5 Hz. Each pair consists of one person with PD and one healthy control volunteer (HC). This pairing was chosen to enable PD vs. HC\ncomparison, for safety reasons, and also to increase the naturalistic social behavior (particularly amongst the spousal pairs who\nalready lived together). From the 24 participants, five females and seven males have PD. To measure the accuracy of the machine learning models, wall-mounted cameras are installed on the ground floor of the house,\nwhich capture red-green-blue (RGB) and depth data 2-3 hours daily (during daylight hours at times when participants were at home). Multiple human labelers used\nsoftware called ELAN to watch up to 4 simultaneously-captured video files at a time. The resulting labeled data recorded the kitchen,\nhallway, dining room, living room, stairs, and porch. **Data pre-processing for indoor localization:** The data from the two wearable sensors worn by each participant were combined at\neach time point, based on their modality, i.e., twenty RSSI values (corresponding to 10 APs for each of the two wearable sensors)\nand accelerometry traces in six spatial directions (corresponding to the three spatial directions (x, y, z) for each wearable) were\nrecorded at each time point. The accelerometer data is resampled to 5Hz to synchronize the data with RSSI values. With a 5-second\ntime window and a 5Hz sampling rate, each RSSI data sample has an input of size (25 x 20), and accelerometer data has an input of\nsize (25 x 6). Imputation for missing values, specifically for RSSI data, is applied by replacing the missing values with a value that is\nnot possible normally (i.e., -120dB). Missing values exist in RSSI data whenever the wearable is out of range of an AP. We hypothesize that during their\nOFF medication state, the deterioration in mobility of a person with PD is exhibited by how they transition between rooms. The transition between (1) kitchen and living\nroom, (2) kitchen and dining room, and (3) dining room and living room are chosen as the features due to their commonality across\nall participants. For these features, we limit the transition time duration (i.e., the time spent in the hallway) to 60 seconds to exclude\ntransitions likely to be prolonged and thus may not be representative of the person\u2019s mobility. These in-home gait speed features are produced by an indoor-localization model by feeding RSSI signals and accelerometer data\nfrom 12 PD participants from 6 a.m. to 10 p.m. daily, which are aggregated into 4-hour windows. There is only one 4-hour window during\nwhich the person with PD is OFF medications. These samples are then used to train a binary classifier determining whether a person\nwith PD is ON or OFF their medications. Two MDS-UPDRS III scores are assigned for each PD participant; one is assigned when a person with PD is ON\nmedications, and the other one is assigned when a person with PD is OFF medications. For each in-home gait speed feature data\nsample, there will be a corresponding demographic feature data sample that is used to train a different binary classifier to predict\nwhether a person with PD is ON or OFF medications. In\norder to protect participant privacy, supporting data is not shared openly. It will be made available to bona fide researchers subject to\na data access agreement. 4 Methodologies and Framework\nWe introduce Multihead Dual Convolutional Self Attention (MDCSA), a deep neural network that utilizes dual modalities for indoor\nlocalization in home environments. The network addresses two challenges that arise from multimodality and time-series data:\n(1) Capturing multivariate features and filtering multimodal noises. RSSI signals, which are measured at multiple access points\nwithin a home received from wearable communication, have been widely used for indoor localization, typically using a fingerprinting\ntechnique that produces a ground truth radio map of a home. The true correlations between inputs both intra-modality (i.e., RSSI signal among\naccess points) and inter-modality (i.e., RSSI signal against accelerometer fluctuation) are dynamic. These dynamics can affect one\nanother within a local context (e.g., cyclical patterns) or across long-term relationships. Can we capture local and global relationships\nacross different modalities? The MDCSA architecture addresses the aforementioned challenges through a series of neural network layers, which are described in\nthe following sections. 4.1 Modality Positional Embedding\nDue to different data dimensionality between RSSI and accelerometer, coupled with the missing temporal information, a linear\nlayer with a positional encoding is added to transform both RSSI and accelerometer data into their respective embeddings. Utilizing historical context that can capture local patterns on\ntop of point-wise values, performance improvements in attention-based architectures can be achieved. One straightforward option is\nto utilize a recurrent neural network such as a long-short term memory (LSTM) approach. However, in LSTM layers, the local\ncontext is summarized based on the previous context and the current input. Two similar patterns separated by a long period of time\nmight have different contexts if they are processed by the LSTM layers. We utilize a combination of causal convolution layers and\nself-attention layers, which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input \u02c6x1\u2208RN\u00d7d\nand a secondary input \u02c6x2\u2208RN\u00d7dand yields:\nDCSA (\u02c6x1,\u02c6x2) =GRN (Norm (\u03d5(\u02c6x1) + \u02c6x1), Norm (\u03d5(\u02c6x2) + \u02c6x2)) (2)\nwith\n\u03d5(\u02c6x) =SA(\u03a6k(\u02c6x)WQ,\u03a6k(\u02c6x)WK,\u03a6k(\u02c6x)WV) (3)\nwhere GRN (. Note that all weights for GRN are shared across each time step t.\n44.3 Multihead Dual Convolutional Self-Attention\nOur approach employs a self-attention mechanism to capture global dependencies across time steps. It is embedded as part of the\nDCSA architecture. Inspired by utilizing multihead self-attention, we utilize our DCSA with various kernel lengths with the same\naim: allowing asymmetric long-term learning. The multihead DCSA takes in two inputs \u02c6x1,\u02c6x2\u2208RN\u00d7dand yields:\nMDCSA k1,...,k n(\u02c6x1,\u02c6x2) = \u039e n(\u03d5k1,...,k n(\u02c6x1,\u02c6x2)) (4)\nwith\n\u03d5ki(\u02c6x1,\u02c6x2) =SA(\u03a6ki(\u02c6x1)WQ,\u03a6ki(\u02c6x2)WK,\u03a6ki(\u02c6x1,\u02c6x2)WV) (5)\nwhere \u03a6ki(. )concatenates the output of each DCSA ki(. For regularization, a normalization layer followed by a dropout layer is added after Equation 4. 1, are then fed to an MDCSA layer with various kernel sizes [k1, ..., k n]:\nh=MDCSA k1,...,k n(hr, ha) (6)\nto yield h= [h1, ..., h T]withht\u2208Rdandt < T . The room-level predictions are produced via a single\nconditional random field (CRF) layer in combination with a linear layer applied to the output of Eq. Even though the transformer can take into account neighbor information before\ngenerating the refined embedding at time step t, its decision is independent; it does not take into account the actual decision made by\nother refined embeddings t. We use a CRF layer to cover just that, i.e., to maximize the probability of the refined embeddings of all\ntime steps, so it can better model cases where refined embeddings closest to one another must be compatible (i.e., minimizing the\npossibility for impossible room transitions). When finding the best sequence of room location \u02c6yt, the Viterbi Algorithm is used as a\nstandard for the CRF layer. For the second layer, we choose a particular room as a reference and perform a binary classification at each time step t. The binary\nclassification is produced via a linear layer applied to the refined embedding htas:\n\u02c6ft=Wfht+bf (9)\nwhere Wf\u2208Rd\u00d71andbf\u2208Rare the weight and bias to learn, and \u02c6f= [\u02c6f1, ...,\u02c6fT]\u2208RTis the target probabilities for the\nreferenced room within time window T. The reason to perform a binary classification against a particular room is because of our\ninterest in improving the accuracy in predicting that room. In our application, the room of our choice is the hallway, where it will be\nused as a hub connecting any other room. Emission outputs (outputs\nproduced by Equation 9 prior to prediction outputs) \u02c6e= [\u03d5(h1), ..., \u03d5 (hT)]are trained to generate the likelihood estimate of room\npredictions, while the binary classification output \u02c6f= [\u02c6f1, ...,\u02c6fT]is used to train the probability estimate of a particular room. P(yi|yi\u22121)denotes the conditional probability, and P(yt|yt\u22121)denotes the transition matrix cost of having transitioned from yt\u22121\ntoyt. - A modified transformer\nencoder in combination with a CRF layer representing a model with the capability to capture global dependency and enforce\ndependencies in temporal aspects. - A state-of-the-art model for multimodal and multivariate time series with a transformer encoder\nto learn asymmetric correlations across modalities. - An alternative to the previous model, representing it with a GRN layer replacing\nthe context aggregation layer and a CRF layer added as the last layer. For the modified transformer encoder, at each time step t, RSSI xr\ntand accelerometer xa\ntfeatures are combined via a\nlinear layer before they are processed by the networks. The parameters to tune are the embedding dimension din 128, 256, the number of epochs in 200, 300,\nand the learning rate in 0.01, 0.0001. The dropout rate is set to 0.15, and a specific optimizer in combination with a Look-Ahead\nalgorithm is used for the training with early stopping using the validation performance. For the RF, we perform a cross-validated\nparameter search for the number of trees (200, 250), the minimum number of samples in a leaf node (1, 5), and whether a warm start\nis needed. We tailored our training procedure to test our hypothesis by performing\nvariations of cross-validation. Apart from training our models on all HC subjects (ALL-HC), we also perform four different kinds of\ncross-validation: 1) We train our models on one PD subject (LOO-PD), 2) We train our models on one HC subject (LOO-HC), 3) We\ntake one HC subject and use only roughly four minutes\u2019 worth of data to train our models (4m-HC), 4) We take one PD subject and\nuse only roughly four minutes\u2019 worth of data to train our models (4m-PD). For all of our experiments, we test our trained models on\nall PD subjects (excluding the one used as training data for LOO-PD and 4m-PD). For room-level localization accuracy, we use\nprecision and weighted F1-score, all averaged and standard deviated across the test folds. We hypothesize that the more accurate the transition is compared to the ground truth, the better mobility features are for\nmedication state classification. For the medication state classification, we then compare two different groups of features with two\nsimple binary classifiers: 1) the baseline demographic features (see Section 3), and 2) the normalized in-home gait speed features. The metric we use for ON/OFF medication state evaluation is the weighted F1-Score and AUROC, which are averaged and standard\ndeviated across the test folds. The improvement is more significant in the 4m-HC and 4m-PD validations, when the training data are limited, with an\naverage improvement of almost 9% for the F1-score over the alternative to the state-of-the-art model. The LOO-HC and LOO-PD validations show that a model that has the ability to capture the temporal dynamics across time steps will\nperform better than a standard baseline technique such as a Random Forest. However,\nwhen the training data becomes limited, as in 4m-HC and 4m-PD validations, having extra capabilities is necessary to further\nextract temporal information and correlations. The state-of-the-art model performs quite well\n6due to its ability to capture local context via LSTM for each modality. The MDCSA network has all the capabilities that the state-of-the-art model has,\nwith an improvement in suppressing the accelerometer modality when needed via the GRN layer embedded in DCSA. Suppressing\nthe noisy modality seems to have a strong impact on maintaining the performance of the network when the training data is limited. It is\nworth pointing out that the MDCSA1,4,7 4APS RSSI model performed the best in the 4m-PD validation. However, the omission of\naccelerometer data affects the model\u2019s ability to differentiate rooms that are more likely to have active movement (i.e., hall) than the\nrooms that are not (i.e., living room). It can be seen from Table 2 that the MDCSA1,4,7 4APS RSSI model has low performance in\npredicting the hallway compared to the full model of MDCSA1,4,7. As a consequence, the MDCSA1,4,7 4APS RSSI model cannot\nproduce in-home gait speed features as\naccurately, as shown in Table 3. To test this hypothesis, a Wilcoxon signed-rank\ntest was used on the annotated data from PD participants undertaking each of the three individual transitions between rooms whilst\nON (taking) and OFF (withholding) medications to assess whether the mean transition duration ON medications was statistically\nsignificantly shorter than the mean transition duration for the same transition OFF medications for all transitions studied (see Table\n4). As mentioned in Section 3, this transition duration for each model is generated by the\nmodel continuously performing room-level localization, focusing on the time a person is predicted to spend in a hallway between\nrooms. The MDCSA network, with the exception of the ALL-HC validation,\noutperforms any other network by a significant margin for the AUROC score. In the 4m-PD validation data, RF, TENER, and DTML could not manage to provide\nany prediction due to their inability to capture (partly) hall transitions. MDCSA networks can provide\nmedication state prediction and maintain their performance across all cross-validations thanks to the addition of Eq. We believe our sample size is ample to show proof of concept. This is also the first such work with\nunobtrusive ground truth validation from embedded cameras. Future work should validate our approach further on a large cohort\nof people with PD and consider stratifying for sub-groups within PD (e.g., akinetic-rigid or tremor-dominant phenotypes), which\nwould also increase the generalizability of the results to the wider population. Future work in this matter could also include the\nconstruction of a semi-synthetic dataset based on collected data to facilitate a parallel and large-scale evaluation. This smart home\u2019s layout and parameters remain constant for all the participants, and we acknowledge that the transfer of this deep\nlearning model to other varied home settings may introduce variations in localization accuracy. Accurate room localization using these data modalities has a wide range of potential applications within healthcare. Training Model Precision F1-Score\n4m-HCMDCSA 4APS RSSI 62.32 (19.72) 58.99 (23.87)\nMDCSA 4APS 68.07 (23.22) 60.01 (26.24)\nMDCSA 71.25 (21.92) 68.95 (17.89)\n4m-PDMDCSA 4APS RSSI 58.59 (23.60) 57.68 (24.27)\nMDCSA 4APS 62.36 (18.98) 57.76 (20.07)\nMDCSA 70.47 (14.10) 64.64 (21.38)\nTable 3: Room-to-room transition accuracy (in seconds) of all models compared to the ground truth. A model that fails to capture a transition between particular rooms\nwithin a period that has the ground truth is assigned \u2019N/A\u2019 score. Acknowledgments\nWe are very grateful to the study participants for giving so much time and effort to this research. We acknowledge the local\nMovement Disorders Health Integration Team (Patient and Public Involvement Group) for their assistance at each study design step. This work was supported by various grants and institutions. In order to\ncompare multiple models over cross-validation sets and show the statistical significance of our proposed model, we perform the\nFriedman test to first reject the null hypothesis. We then performed a pairwise statistical comparison: the Wilcoxon signed-rank test\nwith Holm\u2019s alpha correction.",
        "Results and Findings": "Twenty-four participants, living\nin pairs (one with PD and one control), resided in a sensor-equipped smart home for five days. The results show\nthat the proposed network surpasses other methods for indoor localization. The evaluation of the secondary goal\nreveals that accurate room-level localization, when converted into in-home gait speed features, can accurately\npredict whether a PD participant is taking their medication or not. Initially, patients experience a consistent and extended therapeutic effect when starting levodopa. These observations could potentially be used to identify periods when PD patients are experiencing\nmotor fluctuations related to their medication being in an ON or OFF state, thereby providing valuable information to both clinicians\nand patients. Gait parameters are sensitive to disease progression in unmedicated\nearly-stage PD and show promise as markers of disease progression, making measuring gait parameters potentially useful in clinical\ntrials of disease-modifying interventions. This work makes two primary contributions to address these challenges. (1) We detail the use of RSSI, augmented by accelerometer data, to achieve room-level localization. These biomarkers can effectively classify the OFF or ON\nmedication state of a PD patient from this pilot study data. Gait analysis using radio wave technology shows promise to track disease progression, severity, and\nmedication response. Classic\nmachine learning approaches such as Random Forest (RF), Artificial Neural Network (ANN), and k-Nearest Neighbor (k-NN) are\ntested, and the result shows that the RF outperforms other methods in tracking a person in indoor environments. The wearable devices\nwirelessly transmit data using the Bluetooth Low Energy (BLE) standard, which can be received by the 10 APs. The average age of the participants is 60.25\n(PD 61.25, Control 59.25), and the average time since PD diagnosis for the person with PD is 11.3 years (range 0.5-19). The videos were then manually annotated to the nearest millisecond to provide localization labels. Naturally, the wearable also produces acceleration measurements which\ncan be used to identify typical activities performed in a specific room, and thus we can explore if accelerometer data will enrich\nthe RSSI signals, in particular to help distinguish adjacent rooms, which RSSI-only systems typically struggle with. )in temporal\norder. **Loss Functions:** During the training process, the MDCSA network produces two kinds of outputs. 5 Experiments and Results\nWe compare our proposed network, MDCSA1,4,7 (MDCSA with 3 kernels of size 1, 4, and 7), with:\n- Random Forest (RF) as a baseline technique, which has been shown to work well for indoor localization. A grid search on the parameters of each network is performed to find the best\nparameter for each model. 5.1 Experimental Results\n**Room-level Accuracy:** The first part of Table 1 compares the performance of the MDCSA network and other approaches for\nroom-level classification. The modified transformer encoder and the state-of-the-art\nmodel perform better in those two validations due to their ability to capture asynchronous relations across modalities. Due to being a vanilla transformer requiring a considerable amount of training\ndata, the modified transformer encoder performs worst in these two validations. It is further confirmed by MDCSA1,4,7 4APS against MDCSA1,4,7 4APS RSSI, with the latter model, which does not include\nthe accelerometer data, outperforming the former for the F1-score by an average of 1.6% in the last three cross-validations. From this result, we argue that the mean transition duration obtained by each model from Table 1 that is close to the ground truth\ncan capture what the ground truth captures. We show, in Table 3, that the mean transition duration for all transitions studied produced by the MDCSA1,4,7 model is the\nclosest to the ground truth, improving over the second best by around 1.25 seconds across all hall transitions and validations. The second part of Table 1 shows the performance of all our networks for medication state classification. By using in-home gait speed features produced by\nthe MDCSA network, a minimum of 15% improvement over the baseline demographic features can be obtained, with the biggest\ngain obtained in the 4m-PD validation data. For future ecological validation and\nbased on our current results, we anticipate the need for pre-training (e.g., a brief walkaround which is labeled) for each home, and\nalso suggest that some small amount of ground-truth data will need to be collected (e.g., researcher prompting of study participants to\nundertake scripted activities such as moving from room to room) to fully validate the performance of our approach in other settings. The evaluation on our unique real-world free-living pilot dataset, which includes subjects with and without PD,\nshows that MDCSA achieves state-of-the-art accuracy for indoor localization. The availability of accelerometer data does indeed\nenrich the RSSI features, which, in turn, improves the accuracy of indoor localization. ), the best performer is bold,\nwhile the second best is italicized. Note that our proposed model is the one named MDCSA1,4,7\n!Training ModelRoom-Level Localisation Medication State\nPrecision F1-Score F1-Score AUROC\nALL-HCRF 95.00 95.20 56.67 (17.32) 84.55 (12.06)\nTENER 94.60 94.80 47.08 (16.35) 67.74 (10.82)\nDTML 94.80 94.90 50.33 (13.06) 75.97 (9.12)\nAlt DTML 94.80 95.00 47.25 (5.50) 75.63 (4.49)\nMDCSA1,4,7 4APS 92.22 92.22 53.47 (12.63) 73.48 (6.18)\nMDCSA1,4,7 RSSI 94.70 94.90 51.14 (11.95) 68.33 (18.49)\nMDCSA1,4,7 4APS RSSI 93.30 93.10 64.52 (11.44) 81.84 (6.30)\nMDCSA1,4,7 94.90 95.10 64.13 (6.05) 80.95 (10.71)\nDemographic Features 49.74 (15.60) 65.66 (18.54)\nLOO-HCRF 89.67 (1.85) 88.95 (2.61) 54.74 (11.46) 69.24 (17.77)\nTENER 90.35 (1.87) 89.75 (2.24) 51.76 (14.37) 70.80 (9.78)\nDTML 90.51 (1.95) 89.82 (2.60) 55.34 (13.67) 73.77 (9.84)\nAlt DTML 90.52 (2.17) 89.71 (2.83) 49.56 (17.26) 73.26 (10.65)\nMDCSA1,4,7 4APS 88.01 (6.92) 88.08 (5.73) 59.52 (20.62) 74.35 (16.78)\nMDCSA1,4,7 RSSI 90.26 (2.43) 89.48 (3.47) 58.84 (23.08) 76.10 (10.84)\nMDCSA1,4,7 4APS RSSI 88.55 (6.67) 88.75 (5.50) 42.34 (13.11) 72.58 (6.77)\nMDCSA1,4,7 91.39 (2.13) 91.06 (2.62) 55.50 (15.78) 83.98 (13.45)\nDemographic Features 51.79 (15.40) 68.33 (18.43)\nLOO-PDRF 86.89 (7.14) 84.71 (7.33) 43.28 (14.02) 62.63 (20.63)\nTENER 86.91 (6.76) 86.18 (6.01) 36.04 (9.99) 60.03 (10.52)\nDTML 87.13 (6.53) 86.31 (6.32) 43.98 (14.06) 66.93 (11.07)\nAlt DTML 87.36 (6.30) 86.44 (6.63) 44.02 (16.89) 69.70 (12.04)\nMDCSA1,4,7 4APS 86.44 (6.96) 85.93 (6.05) 47.26 (14.47) 72.62 (11.16)\nMDCSA1,4,7 RSSI 87.61 (6.64) 87.21 (5.44) 45.71 (17.85) 67.76 (10.73)\nMDCSA1,4,7 4APS RSSI 87.20 (7.17) 87.00 (6.12) 41.33 (17.72) 66.26 (12.11)\nMDCSA1,4,7 88.04 (6.94) 87.82 (6.01) 49.99 (13.18) 81.08 (8.46)\nDemographic Features 43.89 (14.43) 60.95 (25.16)\n4m-HCRF 74.27 (8.99) 69.87 (7.21) 50.47 (12.63) 59.55 (12.38)\nTENER 69.86 (18.68) 60.71 (24.94) N/A N/A\nDTML 77.10 (9.89) 70.12 (14.26) 43.89 (11.60) 64.67 (12.88)\nAlt DTML 78.79 (3.95) 71.44 (9.82) 47.49 (14.64) 65.16 (12.56)\nMDCSA1,4,7 4APS 81.42 (6.95) 78.65 (7.59) 42.87 (17.34) 67.09 (7.42)\nMDCSA1,4,7 RSSI 81.69 (6.85) 77.12 (8.46) 49.95 (17.35) 69.71 (11.55)\nMDCSA1,4,7 4APS RSSI 82.80 (7.82) 79.37 (8.98) 43.57 (23.87) 65.46 (15.78)\nMDCSA1,4,7 83.32 (6.65) 80.24 (6.85) 55.43 (10.48) 78.24 (6.67)\nDemographic Features 32.87 (13.81) 53.68 (13.86)\n4m-PDRF 71.00 (9.67) 65.89 (11.96) N/A N/A\nTENER 65.30 (23.25) 58.57 (27.19) N/A N/A\nDTML 70.35 (14.17) 64.00 (17.88) N/A N/A\nAlt DTML 74.43 (9.59) 67.55 (14.50) N/A N/A\nMDCSA1,4,7 4APS 81.02 (8.48) 76.85 (10.94) 49.97 (7.80) 69.10 (7.64)\nMDCSA1,4,7 RSSI 77.47 (12.54) 73.99 (13.00) 41.79 (16.82) 67.37 (16.86)\nMDCSA1,4,7 4APS RSSI 83.01 (6.42) 79.77 (7.05) 41.18 (12.43) 63.16 (11.06)\nMDCSA1,4,7 83.30 (6.73) 76.77 (13.19) 48.61 (12.03) 76.39 (12.23)\nDemographic Features 36.69 (18.15) 50.53 (15.60)\nIn naturalistic settings, in-home mobility can be measured through the use of indoor localization models. We have shown, using\nroom transition duration results, that our PD cohort takes longer on average to perform a room transition when they withhold\nmedications. With accurate in-home gait speed features, a classifier model can then differentiate accurately if a person with PD is in\nan ON or OFF medication state. Such changes show the promise of these localization outputs to detect the dopamine-related gait\nfluctuations in PD that impact patients\u2019 quality of life and are important in clinical decision-making. We have also demonstrated\nthat our indoor localization system provides precise in-home gait speed features in PD with a minimal average offset to the ground\n8Table 2: Hallway prediction on limited training data. ), the best performer is bold, while the second best is italicized. !Data Models Kitch-Livin Kitch-Dinin Dinin-Livin\nGround Truth 18.71 (18.52) 14.65 (6.03) 10.64 (11.99)\nALL-HCRF 16.18 (12.08) 14.58 (10.22) 10.19 (9.46)\nTENER 15.58 (8.75) 16.30 (12.94) 12.01 (13.01)\nAlt DTML 15.27 (7.51) 13.40 (6.43) 10.84 (10.81)\nMDCSA 17.70 (16.17) 14.94 (9.71) 10.76 (9.59)\nLOO-HCRF 17.52 (16.97) 11.93 (10.08) 9.23 (13.69)\nTENER 14.62 (16.37) 9.58 (9.16) 7.21 (10.61)\nAlt DTML 16.30 (17.78) 14.01 (8.08) 10.37 (12.44)\nMDCSA 17.70 (17.42) 14.34 (9.48) 11.07 (13.60)\nLOO-PDRF 14.49 (15.28) 11.67 (11.68) 8.65 (13.06)\nTENER 13.42 (14.88) 10.87 (10.37) 6.95 (10.28)\nAlt DTML 16.98 (15.15) 15.26 (8.85) 9.99 (13.03)\nMDCSA 16.42 (14.04) 14.48 (9.81) 10.77 (14.18)\n4m-HCRF 14.22 (18.03) 11.38 (15.46) 13.43 (18.87)\nTENER 10.75 (15.67) 8.59 (14.39) N/A\nAlt DTML 16.89 (18.07) 14.68 (13.57) 9.31 (15.70)\nMDCSA 18.15 (19.12) 15.32 (14.93) 11.89 (17.55)\n4m-PDRF 11.52 (16.07) 8.73 (12.90) N/A\nTENER 8.75 (14.89) N/A N/A\nAlt DTML 14.75 (13.79) 13.47 (17.66) N/A\nMDCSA 17.96 (19.17) 14.74 (10.83) 10.16 (14.03)\ntruth. The network also outperforms other models in the production of in-home gait speed features, which is used to differentiate the\nmedication state of a person with PD. 9Table 4: PD participant room transition duration with ON and OFF medications comparison using Wilcoxon signed rank tests. OFF transitions Mean transition duration ON transitions Mean transition duration W z\nKitchen-Living OFF 17.2 sec Kitchen-Living ON 14.0 sec 75.0 2.824\nDining-Kitchen OFF 12.9 sec Dining-Kitchen ON 9.2 sec 76.0 2.903\nDining-Living OFF 10.4 sec Dining-Living ON 9.0 sec 64.0 1.961\n10",
        "Conclusion": "Finally, to evaluate\nthe ON/OFF medication state, participants with PD were asked to withhold their dopaminergic medications so that they were in\nthe practically-defined OFF medications state for a temporary period of several hours during the study. Finally, all\ntime-series measurements by the modalities are normalized. 4.4 Final Layer and Loss Calculation\nWe apply two different layers to produce two different outputs during training. 7 to produce the final predictions\nas:\n\u02c6yt=CRF (\u03d5(ht)) (7)\nq\u2032(ht) =Wpht+bp (8)\nwhere Wp\u2208Rd\u00d7mandbp\u2208Rmare the weight and bias to learn, mis the number of room locations, and h= [h1, ..., h T]\u2208RT\u00d7d\nis the refined embedding produced by Eq. 7. The\nfinal loss function can be formulated as a combination of both likelihood and binary cross-entropy loss functions described as:\nL(\u02c6e, y,\u02c6f, f) =LLL(\u02c6e, y) +TX\nt=1LBCE(\u02c6ft, ft) (10)\nLLL(\u02c6e, y) =TX\ni=0P(\u03d5(hi))qT\ni(yi|yi\u22121)\u2212TX\ni=0P(\u03d5(hi))[qT\ni(yi|yi\u22121)] (11)\n5LBCE(\u02c6f, f) =\u22121\nTTX\nt=0ftlog(\u02c6ft) + (1 \u2212ft) log(1 \u2212\u02c6ft) (12)\nwhere LLL(. Furthermore, TENER has shown its inability to provide any\nmedication state prediction from the 4m-HC data validations. 6 Conclusion\nWe have presented the MDCSA model, a new deep learning approach for indoor localization utilizing RSSI and wrist-worn\naccelerometer data.",
        "label": 3
    },
    {
        "Abstract": "Addressing Popularity Bias with Popularity-Conscious Alignment and\nContrastive Learning\nAbstract\nCollaborative Filtering (CF) often encounters substantial difficulties with popularity bias because of the skewed\ndistribution of items in real-world datasets. This is\nformalized as follows:\nL\u2032\npop=X\ni\u2208Ipoplogexp(h\u2032\nihi/\u03c4)P\nj\u2208Ipopexp(h\u2032\nihj/\u03c4) +\u03b2P\nj\u2208Iunpopexp(h\u2032\nihj/\u03c4), (4)\nsimilarly, the contrastive loss for unpopular items is defined as:\nL\u2032\nunpop =X\ni\u2208Iunpoplogexp(h\u2032\nihi/\u03c4)P\nj\u2208Iunpopexp(h\u2032\nihj/\u03c4) +\u03b2P\nj\u2208Ipopexp(h\u2032\nihj/\u03c4), (5)\nwhere the parameter \u03b2ranges from 0 to 1, controlling the negative sample weighting in the contrastive loss. L=LREC+\u03bb1LSA+\u03bb2LCL+\u03bb3||\u0398||2, (7)\nwhere \u0398is the set of model parameters in LREC as we do not introduce additional parameters, \u03bb1and\u03bb2are hyperparameters that\ncontrol the strengths of the popularity-aware supervised alignment loss and the re-weighting contrastive learning loss respectively,\nand\u03bb3is the L2regularization coefficient.",
        "Methodology": "This tendency creates a notable difference in accuracy between items\nthat are popular and those that are not. This discrepancy impedes the accurate comprehension of user preferences\nand intensifies the Matthew effect within recommendation systems. To counter popularity bias, current methods\nconcentrate on highlighting less popular items or on differentiating the correlation between item representations\nand their popularity. Despite their effectiveness, current approaches continue to grapple with two significant\nissues: firstly, the extraction of shared supervisory signals from popular items to enhance the representations of\nless popular items, and secondly, the reduction of representation separation caused by popularity bias. In this\nstudy, we present an empirical examination of popularity bias and introduce a method called Popularity-Aware\nAlignment and Contrast (PAAC) to tackle these two problems. Specifically, we utilize the common supervisory\nsignals found in popular item representations and introduce an innovative popularity-aware supervised alignment\nmodule to improve the learning of representations for unpopular items. Furthermore, we propose adjusting the\nweights in the contrastive learning loss to decrease the separation of representations by focusing on popularity. CF-based techniques primarily\nlearn user preferences and item attributes by matching the representations of users with the items they engage with. Despite their\nachievements, CF-based methods frequently encounter the issue of popularity bias, which leads to considerable disparities in\naccuracy between items that are popular and those that are not. Popularity bias occurs because there are limited supervisory signals\nfor items that are not popular, which results in overfitting during the training phase and decreased effectiveness on the test set. This\nhinders the precise comprehension of user preferences, thereby diminishing the variety of recommendations. The first challenge is the\ninadequate representation of unpopular items during training, which results in overfitting and limited generalization ability. 2 Methodology\nTo overcome the current difficulties in reducing popularity bias, we introduce the Popularity-Aware Alignment and Contrast (PAAC)\nmethod. We utilize the common supervisory signals present in popular item representations to direct the learning of unpopular\nrepresentations, and we present a popularity-aware supervised alignment module. Moreover, we incorporate a re-weighting system\nin the contrastive learning module to deal with representation separation by considering popularity. However, because unpopular items have\nlimited interactions, they are usually modeled based on a small group of users. Specifically, popular items gain from a wealth of supervisory signals during the alignment process, which helps in effectively\nlearning their representations. On the other hand, unpopular items, which have a limited number of users providing supervision, are\nmore susceptible to overfitting. This is because there is insufficient representation learning for unpopular items, emphasizing the\neffect of supervisory signal distribution on the quality of representation. In this section, we utilize common supervisory signals in popular item representations and suggest a\npopularity-aware supervised alignment method to improve the representations of unpopular items. We initially filter items with similar characteristics based on the user\u2019s interests. We count the frequency of each item appearing in the training dataset as its popularity. Subsequently, we group items based on\ntheir relative popularity. We divide items into two groups: the popular item group and the unpopular item group. To tackle the issue of insufficient representation learning for unpopular items, we utilize the concept that items interacted with by the\nsame user share some similar characteristics. Specifically, we use similar supervisory signals in popular item representations to\nimprove the representations of unpopular items. We align the representations of items to provide more supervisory information to\nunpopular items and improve their representation, as follows:\nLSA=X\nu\u2208U1\n|Iu|X\ni\u2208Iupop,j\u2208Iuunpop||f(i)\u2212f(j)||2, (1)\nwhere f(\u00b7)is a recommendation encoder and hi=f(i). By efficiently using the inherent information in the data, we provide more\nsupervisory signals for unpopular items without introducing additional side information. This module enhances the representation of\nunpopular items, mitigating the overfitting issue. Although methods based on contrastive learning aim to enhance overall uniformity by distancing negative samples, their current\nsampling methods might unintentionally worsen this separation. Conversely, when negative samples follow a uniform distribution, focusing on popular items\nseparates them from most unpopular ones, thus worsening the representation gap. Existing studies use the same weights for positive\nand negative samples in the contrastive loss function, without considering differences in item popularity. However, in real-world\nrecommendation datasets, the impact of items varies due to dataset characteristics and interaction distributions. Neglecting this\naspect could lead to suboptimal results and exacerbate representation separation. We propose to identify different influences by re-weighting different popularity items. To this end, we introduce re-weighting\ndifferent positive and negative samples to mitigate representation separation from a popularity-centric perspective. We incorporate\nthis approach into contrastive learning to better optimize the consistency of representations. Specifically, we aim to reduce the risk\nof pushing items with varying popularity further apart. For example, when using a popular item as a positive sample, our goal is\nto avoid pushing unpopular items too far away. Thus, we introduce two hyperparameters to control the weights when items are\nconsidered positive and negative samples. To ensure balanced and equitable representations of items within our model, we first propose a dynamic strategy to categorize items\ninto popular and unpopular groups for each mini-batch. Instead of relying on a fixed global threshold, which often leads to the\noverrepresentation of popular items across various batches, we implement a hyperparameter x. This hyperparameter readjusts the\nclassification of items within the current batch. By adjusting the hyperparameter x, we maintain a balance between different item\npopularity levels. This enhances the model\u2019s ability to generalize across diverse item sets by accurately reflecting the popularity\ndistribution in the current training context. Specifically, we denote the set of items within each batch as IB. And then we divide IB\ninto a popular group Ipopand an unpopular group Iunpop based on their respective popularity levels, classifying the top x%of items\nasIpop:\nIB=Ipop\u222aIunpop ,\u2200i\u2208Ipop\u2227j\u2208Iunpop , p(i)> p(j), (2)\nwhere Ipop\u2208IBandIunpop\u2208IBare disjoint, with Ipopconsisting of the top x%of items in the batch. In this work, we dynamically\ndivided items into popular and unpopular groups within each mini-batch based on their popularity, assigning the top 50% as popular\nitems and the bottom 50% as unpopular items. This radio not only ensures equal representation of both groups in our contrastive\nlearning but also allows items to be classified adaptively based on the batch\u2019s current composition. After that, we use InfoNCE to optimize the uniformity of item representations. Unlike traditional CL-based methods, we calculate\nthe loss for different item groups. Specifically, we introduce the hyperparameter \u03b1to control the positive sample weights between\npopular and unpopular items, adapting to varying item distributions in different datasets:\n2LCL\nitem=\u03b1\u00d7LCL\npop+ (1\u2212\u03b1)\u00d7LCL\nunpop , (3)\nwhere LCL\npoprepresents the contrastive loss when popular items are considered as positive samples, and LCL\nunpop represents the\ncontrastive loss when unpopular items are considered as positive samples. The value of \u03b1ranges from 0 to 1, where \u03b1= 0means\nexclusive emphasis on the loss of unpopular items LCL\nunpop , and \u03b1= 1 means exclusive emphasis on the loss of popular items\nLCL\npop. By adjusting \u03b1, we can effectively balance the impact of positive samples from both popular and unpopular items, allowing\nadaptability to varying item distributions in different datasets. Following this, we fine-tune the weighting of negative samples in the contrastive learning framework using the hyperparameter \u03b2. This parameter controls how samples from different popularity groups contribute as negative samples. Specifically, we prioritize\nre-weighting items with popularity opposite to the positive samples, mitigating the risk of excessively pushing negative samples\naway and reducing representation separation. Simultaneously, this approach ensures the optimization of intra-group consistency. For\ninstance, when dealing with popular items as positive samples, we separately calculate the impact of popular and unpopular items\nas negative samples. The hyperparameter \u03b2is then used to control the degree to which unpopular items are pushed away. When \u03b2= 0, it means\nthat only intra-group uniformity optimization is performed. Conversely, when \u03b2= 1, it means equal treatment of both popular and\nunpopular items in terms of their impact on positive samples. The setting of \u03b2allows for a flexible adjustment between prioritizing\nintra-group uniformity and considering the impact of different popularity levels in the training. We prefer to push away items\nwithin the same group to optimize uniformity. This setup helps prevent over-optimizing the uniformity of different groups, thereby\nmitigating representation separation. (6)\nIn this way, we not only achieved consistency in representation but also reduced the risk of further separating items with similar\ncharacteristics into different representation spaces, thereby alleviating the issue of representation separation caused by popularity\nbias. After completing the model training process, we use the dot product to predict unknown\npreferences for recommendations. 3 Experiments\nIn this section, we assess the efficacy of PAAC through comprehensive experiments, aiming to address the following research\nquestions:\n\u2022 How does PAAC compare to existing debiasing methods? 3\u2022 How does PAAC alleviate the popularity bias? We retained users and items with a\nminimum of 10 interactions. We\ncompare PAAC with several debiased baselines, including re-weighting-based models, decorrelation-based models, and contrastive\nlearning-based models. We utilize three widely used metrics, namely Recall@K, HR@K, and NDCG@K, to evaluate the performance of Top-K recommen-\ndation. In\ncontrast, NDCG@K evaluates the positions of target items in the ranking list, with a focus on their positions in the list. We use\nthe full ranking strategy, considering all non-interacted items as candidate items to avoid selection bias during the test stage. \u2022Our proposed model PAAC consistently outperforms all baselines and significantly mitigates the popularity bias. Compared to the strongest baselines, PAAC delivers better\nperformance. \u2022The performance improvements of PAAC are smaller on sparser datasets. Therefore, the benefits of using\nsupervisory signals for unpopular items may be reduced in very sparse environments, leading to smaller performance\nimprovements. \u2022Regarding the baselines for mitigating popularity bias, the improvement of some is relatively limited compared to the\nbackbone model (LightGCN) and even performs worse in some cases. This may be because some are specifically designed\nfor traditional data-splitting scenarios, where the test set still follows a long-tail distribution, leading to poor generalization. Some mitigate popularity bias by excluding item popularity information. Others use invariant learning to remove popularity\ninformation at the representation level, generally performing better than the formers. Some outperform the other baselines, emphasizing the necessary to\nimprove item representation consistency for mitigating popularity bias. This suggests that different\ndebiasing methods may need distinct optimization strategies for models. Additionally, we observe varying effects of PAAC\nacross different datasets. This difference could be due to the sparser nature of the Gowalla dataset. Conversely, our model\ncan directly provide supervisory signals for unpopular items and conduct intra-group optimization, consistently maintaining\noptimal performance across all metrics on the three datasets. 3.3 Ablation Study\nTo better understand the effectiveness of each component in PAAC, we conduct ablation studies on three datasets. Similarly, PAAC-w/o U denotes the removal of the re-weighting contrastive loss for unpopular items. The superscripts * indicate p \u22640.05 for the paired t-test of PAAC vs. the best baseline (the\nrelative improvements are denoted as Imp.). +9.78 % +10.81% +8.70% +3.18% +5.85% +5.47% +9.36% +6.78% 5.90%\nSimGCL in that we split the contrastive loss on the item side, LCL\nitem, into two distinct losses: LCL\npopandLCL\nunpop . This approach\nallows us to separately address the consistency of popular and unpopular item representations, thereby providing a more detailed\nanalysis of the impact of each component on the overall performance. From Table 2, we observe that PAAC-w/o A outperforms SimGCL in most cases. All the proposed modules significantly\ncontribute to alleviating popularity bias. Specifically,\nPAAC-w/o P removes the re-weighting contrastive loss of popular items, PAAC-w/o U eliminates the re-weighting contrastive loss\nof unpopular items, and PAAC-w/o A omits the popularity-aware supervised alignment loss. Specifically, 20% of the most popular items are labeled\n\u2019Popular\u2019, and the rest are labeled \u2019Unpopular\u2019. We use \u2206to denote the accuracy gap between the two groups. This improvement highlights the\nimportance of better recommendations for unpopular items and emphasizes their crucial role in enhancing overall model\nperformance. 5\u2022Our proposed PAAC significantly enhances the recommendation performance for unpopular items. This improvement is due to the popularity-aware alignment method, which uses supervisory signals from popular items to\nimprove the representations of unpopular items. This indicates that our method treats items from different groups fairly, effectively alleviating the impact of popularity\nbias. This success can be attributed to our re-weighted contrast module, which addresses representation separation from a\npopularity-centric perspective, resulting in more consistent recommendation results across different groups. Firstly, we investigate the influence of \u03bb1and\u03bb2, which\nrespectively control the impact of the popularity-aware supervised alignment and re-weighting contrast loss. Additionally, in the\nre-weighting contrastive loss, we introduce two hyperparameters, \u03b1and\u03b2, to control the re-weighting of different popularity items\nas positive and negative samples. (11), \u03bb1controls the extent of providing additional supervisory signals for unpopular items, while \u03bb2controls\nthe extent of optimizing representation consistency. Horizontally, with the increase in \u03bb2, the performance initially increases and\nthen decreases. However, overly strong contrastive loss may lead the model to neglect recommendation\naccuracy. Vertically, as \u03bb1increases, the performance also initially increases and then decreases. This suggests that suitable\nalignment can provide beneficial supervisory signals for unpopular items, while too strong an alignment may introduce more noise\nfrom popular items to unpopular ones, thereby impacting recommendation performance. 3.5.2 Effect of re-weighting coefficient \u03b1and\u03b2\nTo mitigate representation separation due to imbalanced positive and negative sampling, we introduce two hyperparameters into the\ncontrastive loss. Specifically, \u03b1controls the weight difference between positive samples from popular and unpopular items, while \u03b2\ncontrols the influence of different popularity items as negative samples. In our experiments, while keeping other hyperparameters constant, we search \u03b1and\u03b2within the range {0, 0.2, 0.4, 0.6, 0.8, 1}. As\n\u03b1and\u03b2increase, performance initially improves and then declines. The\nYelp2018 dataset, with a higher average interaction frequency per item, benefits more from a higher weight \u03b1for popular items as\npositive samples. Conversely, the Gowalla dataset, being relatively sparse, prefers a smaller \u03b1. This indicates the importance of\nconsidering dataset characteristics when adjusting the contributions of popular and unpopular items to the model. Thus, \u03b1and\u03b2achieve optimal\nperformance without meticulous adjustments, focusing on weight coefficients to maintain model efficacy. 3.5.3 Effect of grouping ratio x\nTo investigate the impact of different grouping ratios on recommendation performance, we developed a flexible classification\nmethod for items within each mini-batch based on their popularity. Instead of adopting a fixed global threshold, which tends to\noverrepresent popular items in some mini-batches, our approach dynamically divides items in each mini-batch into popular and\nunpopular categories. Specifically, the top x%of items are classified as popular and the remaining (100 - x)% as unpopular, with x\nvarying. This strategy prevents the overrepresentation typical in fixed distribution models, which could skew the learning process\nand degrade performance. Numerous techniques have been suggested to examine and decrease performance variations between popular and unpopular\nitems. \u2022Re-weighting-based methods aim to increase the training weight or scores for unpopular items, redirecting focus away\nfrom popular items during training or prediction. For instance, IPS adds compensation to unpopular items and adjusts\nthe prediction of the user-item preference matrix, resulting in higher preference scores and improving rankings for\nunpopular items. \u03b1-AdjNorm enhances the focus on unpopular items by controlling the normalization strength during the\nneighborhood aggregation process in GCN-based models. \u2022Decorrelation-based methods aim to effectively remove the correlations between item representations (or prediction scores)\nand popularity. For instance, MACR uses counterfactual reasoning to eliminate the direct impact of popularity on item\noutcomes. In contrast, InvCF operates on the principle that item representations remain invariant to changes in popularity\nsemantics, filtering out unstable or outdated popularity characteristics to learn unbiased representations. \u2022Contrastive-learning-based methods aim to achieve overall uniformity in item representations using InfoNCE, preserving\nmore inherent characteristics of items to mitigate popularity bias. This approach has been demonstrated as a state-of-the-art\nmethod for alleviating popularity bias. It employs data augmentation techniques such as graph augmentation or feature\naugmentation to generate different views, maximizing positive pair consistency and minimizing negative pair consistency\nto promote more uniform representations. Specifically, Adap- \u03c4adjusts user/item embeddings to specific values, while\nSimGCL integrates InfoNCE loss to enhance representation uniformity and alleviate popularity bias. The quality of these representations critically\ndetermines a recommender system\u2019s effectiveness by precisely capturing the interplay between user interests and item features. This principle is crucial when accurately reflecting user preferences through\ncorresponding item characteristics. Conversely, the uniformity principle ensures a balanced distribution of all embeddings across the\nrepresentation space. This approach prevents the over-concentration of embeddings in specific areas, enhancing recommendation\ndiversity and improving generalization to unseen data. In this work, we focus on aligning the representations of popular and unpopular items interacted with by the same user and re-\nweighting uniformity to mitigate representation separation. Unlike previous works that align positive user-item pairs or contrastive pairs,\nPAAC directly aligns popular and unpopular items, leveraging the rich information of popular items to enhance the representations\nof unpopular items and reduce overfitting. Additionally, we introduce targeted re-weighting from a popularity-centric perspective to\nachieve a more balanced representation. We postulated that items\nengaged with by the same user exhibit common traits, and we utilized this insight to coordinate the representations of both popular\nand unpopular items via a popularity-conscious supervised alignment method. This strategy furnished additional supervisory data for\nless popular items. It is important to note that our concept of aligning and categorizing items according to user-specific preferences\nintroduces a fresh perspective on alignment. Moreover, we tackled the problem of representation separation seen in current CL-based\n7models by incorporating two hyperparameters to regulate the influence of items with varying popularity levels when considered\nas positive and negative samples. This method refined the uniformity of representations and successfully reduced separation. We\nvalidated our method, PAAC, on three publicly available datasets, demonstrating its effectiveness and underlying rationale. In the future, we will explore deeper alignment and contrast adjustments tailored to specific tasks to further mitigate popularity\nbias. We aim to investigate the synergies between alignment and contrast and extend our approach to address other biases in\nrecommendation systems. Acknowledgments\nThis work was supported in part by grants from the National Key Research and Development Program of China, the National Natural\nScience Foundation of China, the Fundamental Research Funds for the Central Universities, and Quan Cheng Laboratory.",
        "Results and Findings": "Personalized recommendations frequently\nemploy collaborative filtering (CF) to assist users in discovering items that may interest them. Intuitively, items interacted with by the same user have\nsome similar characteristics. For any user, we define the set of items they interact\nwith. The popularity of\neach item in the popular group is higher than that of any item in the unpopular group. This indicates that popular items receive more\nsupervisory information than unpopular items, resulting in poorer recommendation performance for unpopular items. When negative samples follow the popularity distribution, which\nis dominated by popular items, prioritizing unpopular items as positive samples widens the gap between popular and unpopular\nitems in the representation space. Recall@K and HR@K assess the number of target items retrieved in the recommendation results, emphasizing coverage. We\nrepeated each experiment five times with different random seeds and reported the average scores. 3.2 Overall Performance\nAs shown in Table 1, we compare our model with several baselines across three datasets. The best performance for each metric\nis highlighted in bold, while the second best is underlined. Our model consistently outperforms all compared methods across all\nmetrics in every dataset. Specif-\nically, PAAC enhances LightGCN, achieving improvements of 282.65%, 180.79%, and 82.89% in NDCG@20 on the\nYelp2018, Gowalla, and Amazon-Book datasets, respectively. The most significant improvements are observed on Yelp2018, where our model achieves an 8.70% increase\nin Recall@20, a 10.81% increase in HR@20, and a 30.2% increase in NDCG@20. This improvement can be attributed\nto our use of popularity-aware supervised alignment to enhance the representation of less popular items and re-weighted\ncontrastive learning to address representation separation from a popularity-centric perspective. \u2022Different metrics across various datasets show varying improvements in model performance. The best performance is indicated in bold, while the\nsecond-best performance is underlined. !ModelYelp2018 Gowalla Amazon-book\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\nMF 0.0050 0.0109 0.0093 0.0343 0.0422 0.0280 0.0370 0.0388 0.0270\nLightGCN 0.0048 0.0111 0.0098 0.0380 0.0468 0.0302 0.0421 0.0439 0.0304\nIPS 0.0104 0.0183 0.0158 0.0562 0.0670 0.0444 0.0488 0.0510 0.0365\nMACR 0.0402 0.0312 0.0265 0.0908 0.1086 0.0600 0.0515 0.0609 0.0487\n\u03b1-Adjnorm 0.0053 0.0088 0.0080 0.0328 0.0409 0.0267 0.0422 0.0450 0.0264\nInvCF 0.0444 0.0344 0.0291 0.1001 0.1202 0.0662 0.0562 0.0665 0.0515\nAdap- \u03c4 0.0450 0.0497 0.0341 0.1182 0.1248 0.0794 0.0641 0.0678 0.0511\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\nImp. This validates that re-weighting the importance of\npopular and unpopular items can effectively improve the model\u2019s performance in alleviating popularity bias. Moreover, compared with PAAC-w/o U, PAAC-w/o P results in much\nworse performance. This confirms the importance of re-weighting popular items in contrastive learning for mitigating popularity\nbias. Table 2: Ablation study of PAAC, highlighting the best-performing model on each dataset and metrics in bold. !ModelYelp2018 Gowalla Amazon-book\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\nPAAC-w/o P 0.0443 0.0536 0.0340 0.1098 0.1191 0.0750 0.0616 0.0639 0.0458\nPAAC-w/o U 0.0462 0.0545 0.0358 0.1120 0.1179 0.0752 0.0594 0.0617 0.0464\nPAAC-w/o A 0.0466 0.0547 0.0360 0.1195 0.1260 0.0815 0.0687 0.0711 0.0536\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\n3.4 Debias Ability\nTo further verify the effectiveness of PAAC in alleviating popularity bias, we conduct a comprehensive analysis focusing on the\nrecommendation performance across different popularity item groups. We compare the performance of PAAC with LightGCN, IPS, MACR, and SimGCL\nusing the NDCG@20 metric across different popularity groups. Specially, on the\nYelp2018 dataset, PAAC shows reduced accuracy in recommending popular items, with a notable decrease of 20.14%\ncompared to SimGCL. However, despite this decrease, the overall recommendation accuracy surpasses that of SimGCL\nby 11.94%, primarily due to a 6.81% improvement in recommending unpopular items. Specifically, we observe\nan improvement of 8.94% and 7.30% in NDCG@20 relative to SimGCL on the Gowalla and Yelp2018 datasets, respectively. Specifically, PAAC achieved the smallest\ngap, reducing the NDCG@20 accuracy gap by 34.18% and 87.50% on the Gowalla and Yelp2018 datasets, respectively. This indicates that appropriate re-weighting contrastive loss effectively enhances the consistency of representation\ndistributions, mitigating popularity bias. This may be attributed to the characteristics of the datasets. Notably, \u03b1and\u03b2are not highly sensitive within the range [0, 1], performing well across a broad spectrum. Performance exceeds the\nbaseline regardless of \u03b2values when other parameters are optimal. Additionally, \u03b1values from [0.4, 1.0] on the Yelp2018 dataset\nand [0.2, 0.8] on the Gowalla dataset surpass the baseline, indicating less need for precise tuning. To quantify the effects of these varying ratios, we examined various division ratios for popular items,\nincluding 20%, 40%, 60%, and 80%, as shown in Table 3. The preliminary results indicate that both extremely low and high ratios\nnegatively affect model performance, thereby underscoring the superiority of our dynamic data partitioning approach. Moreover,\nwithin the 40%-60% range, our model\u2019s performance remained consistently robust, further validating the effectiveness of PAAC. !RatioYelp2018 Gowalla\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\n20% 0.0467 0.0555 0.0361 0.1232 0.1319 0.0845\n40% 0.0505 0.0581 0.0378 0.1239 0.1325 0.0848\n50% 0.0494 0.0574 0.0375 0.1232 0.1321 0.0848\n60% 0.0492 0.0569 0.0370 0.1225 0.1314 0.0843\n80% 0.0467 0.0545 0.0350 0.1176 0.1270 0.0818\n4 Related Work\n4.1 Popularity Bias in Recommendation\nPopularity bias is a prevalent problem in recommender systems, where unpopular items in the training dataset are seldom recom-\nmended. These techniques can be broadly divided into three categories. It\ncreates personalized embeddings that capture user preferences and item characteristics. The alignment principle\nensures that embeddings of similar or related items (or users) are closely clustered together, improving the system\u2019s ability to\nrecommend items that align with a user\u2019s interests. 8",
        "Conclusion": "We confirm the efficacy and logic of PAAC in reducing popularity bias through thorough experiments on three\nreal-world datasets. The final re-weighting contrastive objective is the weighted sum of the user objective and the item objective:\nLCL=1\n2\u00d7(LCL\nitem+LCL\nuser). Finally, PAAC consistently outperforms the three variants, demonstrating the effectiveness of combining supervised alignment\nand re-weighting contrastive learning. Based on the above analysis, we conclude that leveraging supervisory signals from popular\nitem representations can better optimize representations for unpopular items, and re-weighting contrastive learning allows the model\nto focus on more informative or critical samples, thereby improving overall performance. We\ndraw the following conclusions:\n\u2022Improving the performance of unpopular items is crucial for enhancing overall model performance. \u2022PAAC has successfully narrowed the accuracy gap between different item groups. Finally, we explore the impact of the grouping ratio xon the model\u2019s performance. 5 Conclusion\nIn this study, we have examined popularity bias and put forward PAAC as a method to lessen its impact.",
        "label": 3
    },
    {
        "Abstract": "Safe Predictors for Input-Output Specification\nEnforcement\nAbstract\nThis paper presents an approach for designing neural networks, along with other\nmachine learning models, which adhere to a collection of input-output specifica-\ntions. One ap-\nproach to achieving provably correct neural networks is through abstraction-refinement optimization. Given:\n\u2022c different input constraint proximity functions, \u03c3i:X\u2192[0,1], where \u03c3iis continuous and\n\u2200x\u2208Ai,\u03c3i(x) = 0 ,\n\u2022kdifferent constrained predictors, Gb:X\u2192Bb, one for each b\u2208O, such that the domain\nof each Gbis non-empty,\nWe define:\n\u2022a set of weighting functions, wb(x) =Q\ni:bi=1(1\u2212\u03c3i(x))Q\ni:bi=0\u03c3i(x)P\nb\u2208OQ\ni:bi=1(1\u2212\u03c3i(x))Q\ni:bi=0\u03c3i(x), where\nP\nb\u2208Owb(x) = 1 , and\n\u2022 a safe predictor, F(x) =P\nb\u2208Owb(x)Gb(x). 22.2 Learning\nIf we have families of differentiable functions Gb(x;\u03b8b), continuously parameterized by \u03b8b, and\nfamilies of \u03c3i(x;\u03c7i), differentiable and continuously parameterized by \u03c7i, then F(x; \u0398, X), where\n\u0398 ={\u03b8b:b\u2208O}andX={\u03c7i:i= 1, ..., c}, is also continuously parameterized and differentiable. The unconstrained network\nconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.",
        "Methodology": "Our method involves the construction of a constrained predictor for each set\nof compatible constraints, and combining these predictors in a safe manner using a\nconvex combination of their predictions. These models may be required\nto satisfy specific input-output specifications to ensure the algorithms comply with physical laws,\ncan be executed safely, and are consistent with prior domain knowledge. Furthermore, these models\nshould demonstrate adversarial robustness, meaning their outputs should not change abruptly within\nsmall input regions \u2013 a property that neural networks often fail to satisfy. Therefore, additional methods are needed to adjust networks if it is found\nthat they are not meeting the desired properties. There has been an increase in techniques for designing networks with certified adversarial robustness,\nbut enforcing more general safety properties in neural networks is still largely unexplored. This approach has been applied to the ACAS-Xu dataset, but the network was not guaranteed to meet\nthe specifications until after training. Our work seeks to design networks with enforced input-output\nconstraints even before training has been completed. This paper presents an approach for designing a safe predictor (a neural network or any other\nmachine learning model) that will always meet a set of constraints on the input-output relationship. This assumes that the constrained output regions can be formulated to be convex. Our correct-\nby-construction safe predictor is guaranteed to satisfy the constraints, even before training, and at\nevery training step. .2 Method\nConsidering two normed vector spaces, an input space X and an output space Y , and a collection\nof c different pairs of input-output constraints, (Ai, Bi), where Ai\u2286XandBiis a convex subset\nofYfor each constraint i, the goal is to design a safe predictor, F:X\u2192Y, that guarantees\nx\u2208Ai\u21d2F(x)\u2208Bi. Letbbe a bit-string of length c. Define Obas the set of points zsuch that, for all i,bi= 1implies\nz\u2208Ai, and bi= 0implies z /\u2208Ai.Obthus represents the overlap regions for each combination of\ninput constraints. We also define Oas the set of bit strings, b, such that Ob\nis non-empty, and define k=|O|. The sets {Ob:b\u2208O}create a partition of Xaccording to the\ncombination of input constraints that apply. Only the constrained predictors that map to\nBiwill be given non-zero weight, and because of the convexity of Bi, the weighted average of the\npredictions will remain in Bi. In the worst case, as the number of constraints grows linearly,\nthe number of constrained predictors needed to describe our safe predictor grows exponentially. In\npractice, however, we expect many of the constraint overlap sets, Ob, to be empty. Consequently, any\npredictors corresponding to an empty set can be ignored. This significantly reduces the number of\nconstrained predictors needed for many applications. These functions are used to compute the weights of the constrained predictors. This ensures that\nwhen an input is far from a constraint region, that constraint has little influence on the prediction for\nthat input. Here, \u03a3iis a set of parameters \u03c31\u2208(0,\u221e)and\u03c32\u2208(1,\u221e), which can be specified based on\nengineering judgment, or learned using optimization over training data. In our experiments in\nthis paper, we use proximity functions of this form and learn independent parameters for each\ninput-constrained region. We can thus apply standard optimization techniques (e.g., gradient descent) to find parameters of F\nthat minimize a loss function on some dataset, while also preserving the desired safety properties. Note that the safety guarantee holds regardless of the parameters. To create each Gb(x;\u03b8b)we\nconsider choosing:\n\u2022 a latent space Rm,\n\u2022 a map hb:Rm\u2192Bb,\n\u2022 a standard neural network architecture gb:X\u2192Rm,\nand then defining Gb(x;\u03b8b) =hb(gb(x;\u03b8b)). The framework proposed here does not require an entirely separate network for each b. In addition, our definition of the safe predictor is\ngeneral and is not limited to neural networks. The system was originally designed to\nchoose optimal advisories while minimizing disruptive alerts by solving a partially observable Markov\ndecision process. The solution took the form of a large look-up table, mapping each possible input\ncombination to scores for all possible advisories. The advisory with the highest score would then be\nissued. By using a deep neural network (DNN) to compress the policy tables, it has been necessary to\nverify that the DNNs meet certain safety specifications. This property\nspeci01ed that for any given input state within the \u02d8201csafeable region, \u02d8201d an advisory would never\nbe issued that could put the aircraft into a state where a safe advisory would no longer exist. DNNs were then generated to approximate the learned policy, and Reluplex was\nused to verify whether the DNNs satisfied the safeability property. Our approach for designing a safe predictor ensures any collision avoidance system will meet the\nsafeability property by construction. For both\nnetworks, we present the percentage accuracy (ACC) and violations (percentage of inputs for which\nthe network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets,\nbased on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500). As shown in the table, our safe predictor adheres to the required safeability property. Furthermore,\nthe accuracy of our predictor remains the same as the unconstrained network, demonstrating we are\nnot losing accuracy to achieve safety guarantees. The novelty of our approach is its simplicity and guaranteed enforcement of specifications\nthrough combinations of convex output constraints during all stages of training. Future work includes\nadapting and using techniques from optimization and control barrier functions, as well as incorporating\nnotions of adversarial robustness into our design, such as extending the work to bound the Lipschitz\nconstant of our networks. Fixiand assume that x\u2208Ai. The safe predictor shares this structure with the unconstrained network but has its own fully connected\nlayer for the constrained predictors, G0andG1. Training uses a sampled subset of points from\nthe input space. Figure 3 shows an example of applying our safe predictor to a notional regression\nproblem with a 2-D input and 1-D output, using two overlapping constraints. The constrained predictors, G00,G10,G01, and G11, share the hidden layers but also have an\nadditional hidden layer of size 20 with ReLU, followed by a fully connected layer. Training uses a\nsampled subset of points from the input space. If no future advisories exist, the advisory is \"unsafeable\"\nand the corresponding input region is the \"unsafeable region\". The constraints we enforce are that x\u2208Aunsafeable ,i\u21d2Fi(x)<max jFj(x),\u2200i, where Aunsafeable ,iis\nthe unsafeable region for the ithadvisory, and Fj(x)is the output score for the jthadvisory. Because\nthe output regions of the safeable constraints are not convex, we make a conservative approximation,\nenforcing Fi(x) = min jFj(x), for all x\u2208Aunsafeable ,i. 4C.2 Proximity Functions\nWe start by generating the unsafeable region bounds from the open source code. We then compute a\n\"distance function\" between input space points (vO - vI, h, \u03c4), and the unsafeable region for each\nadvisory. These are not true distances but are 0 if and only if the data point is within the unsafeable\nset. C.3 Structure of Predictors\nThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden\nlayers with a dimension of 45, and ReLU activation functions. For our constrained predictors, we use the same structure but have shared\nfirst four layers for all predictors. This provides a common learned representation of the input space,\nwhile allowing each predictor to adapt to its own constraints. After the shared layers, each constrained\npredictor has an additional two hidden layers and their final outputs are projected onto our convex\napproximation of the safe region of the output space, using Gb(x) = min jGj(x). In our experiments,\nwe set \u03f5= 0.0001 . With this construction, we needed 30 separate predictors to enforce the VerticalCAS safeability\nconstraints. The number of nodes for the unconstrained and safe implementations were 270 and 2880,\nrespectively. Our safe predictor is orders of magnitude smaller than the original look-up tables. C.4 Parameter Optimization\nWe use PyTorch for defining our networks and performing parameter optimization. We optimize both\nthe unconstrained and safe predictors using the asymmetric loss function to select advisories while\nalso accurately predicting scores. The data is split using an 80/20 train/test split with a random seed\nof 0. The safe predictor\nshares this structure with constrained predictors, G0andG1, but each predictor has its own fully\nconnected layer. The training uses a sampled subset of points from the input space and the learned\npredictors are shown for the continuous input space. The constrained\npredictors G00,G10,G01andG11share the hidden layers and have an additional hidden layer of size\n20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of points\nfrom the input space and the learned predictors are shown for the continuous input space. The constraints we enforce in our safe predictor are: x\u2208Aunsafeable ,i\u21d2Fi(x)<max jFj(x),\n\u2200i. To make the output regions convex, we approximate by enforcing Fi(x) = min jFj(x), for all\nx\u2208Aunsafeable ,i.\nC.2 Proximity Functions\nWe start by generating the bounds on the unsafeable regions. Then, a distance function is computed\nbetween points in the input space ( vO\u2212vI, h,\u03c4), and the unsafeable region for each advisory. While\nthese are not true distances, their values are 0 if and only if the data point is inside the unsafeable set. For constrained predictors, we use a similar architecture. However, the first four hidden layers are shared between all of the predictors. This learns a single,\nshared input space representation, and also allows each predictor to adapt to its constraints. Each\nconstrained predictor has two additional hidden layers and their outputs are projected onto our convex\napproximation of the safe output region. We accomplish this by setting the score for any unsafeable\nadvisory itoGi(x) = min jGj(x)\u2212\u03f5. This increases\nthe size of the network from 270 to 2880 nodes for the unconstrained and safe implementations\nrespectively. However, our safe predictor remains smaller than the original look-up tables by several\norders of magnitude. C.4 Parameter Optimization\nWe define our networks and perform parameter optimization using PyTorch. We optimize the\nparameters of both the unconstrained network and our safe predictor using the asymmetric loss\nfunction, guiding the network to select optimal advisories while accurately predicting scores from\nthe look-up tables. Each dataset is split using an 80/20 train/test split, with a random seed of 0.",
        "Results and Findings": "Recent studies have shown the capacity to verify formally input-output specifications and adversarial\nrobustness properties of neural networks. Results on synthetic datasets can be found in Appendix B. For all i, ifx\u2208Ai, then F(x)\u2208Bi. This work found thousands of\ncounterexamples where the DNNs did not meet the criteria. These constraints are defined such that if an aircraft state is in the \"unsafeable region\",\nAunsafeable ,i, for the ithadvisory, the score for that advisory must not be the highest, i.e., x\u2208\nAunsafeable ,i\u21d2Fi(x)<max jFj(x), where Fj(x)is the output score for the jthadvisory. 3Table 1: Results of the best configurations of \u03b2-TCV AE on DCI, FactorV AE, SAP, MIG, and IRS\nmetrics. Then, \u03c3i(x) = 0 , and for all b\u2208Owhere bi= 0,wb(x) = 0 . Thus,\nF(x) =X\nb\u2208O,bi=1wb(x)Gb(x)\nIfbi= 1, then Gb(x)\u2208Bi, and therefore F(x)is inBidue to the convexity of Bi. In our experiments, we used \u03f5= 0.0001 .",
        "Conclusion": "It follows that \u03c3i(x) = 0 , so for all b\u2208Owhere bi= 0,\nwb(x) = 0 . Thus,\nF(x) =X\nb\u2208O,bi=1wb(x)Gb(x). Letx\u2208Ai. 6",
        "label": 4
    },
    {
        "Abstract": "Generalization in ReLU Networks via Restricted\nIsometry and Norm Concentration\nAbstract\nRegression tasks, while aiming to model relationships across the entire input space,\nare often constrained by limited training data. While this establishes the asymptotic convergence of the empirical norm to the function norm for a\nsingle function f, we have to consider two issues to formulate our concept of norm concentration:\nFirst, we need non-asymptotic results, that is bounds on the distance |\u2225f\u2225m\u2212 \u2225f\u2225\u00b5|for a fixed\nnumber of samples m. Second, the bounds on the distance need to be uniformly valid for all functions\nfin a given set. We overcome this\nby not specifying the outcome of an optimization method and instead stating uniform bounds on\nthe norm concentration. To this end, we define\nthe Sub-Gaussian metric for any parameter tuples \u00afp,\u00afq\u2208\u00afPas\nd\u03c82(\u03d5\u00afp, \u03d5\u00afq) := inf(\nC\u03c82\u22650 :E\"\nexp \n|\u03d5\u00afp(x)\u2212\u03d5\u00afq(x)|2\nC2\n\u03c82!#\n\u22642)\n.",
        "Methodology": "Nevertheless, if the hypothesis func-\ntions can be represented effectively by the data, there is potential for identifying a\nmodel that generalizes well. To determine the\nsample complexity necessary to achieve NeuRIPs, we bound the covering numbers\nof the networks using the Sub-Gaussian metric and apply chaining techniques. As-\nsuming the NeuRIPs event, we then provide bounds on the expected risk, applicable\nto networks within any sublevel set of the empirical risk. In recent\nyears, supervised machine learning has seen the development of tools for automated model discovery\nfrom training data. However, these methods often lack a robust theoretical framework to estimate\nmodel limitations. Understanding the risk surface during neural network training is crucial for establishing a strong\ntheoretical foundation for neural network-based machine learning, particularly for understanding\ngeneralization. In large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,\nglobal minima exist in each connected component of the risk\u2019s sublevel set and are path-connected. In this work, we contribute to a generalization theory for shallow ReLU networks, by giving uniform\ngeneralization error bounds within the empirical risk\u2019s sublevel set. We use methods from the analysis\nof convex linear regression, where generalization bounds for empirical risk minimizers are derived\nfrom recent advancements in stochastic processes\u2019 chaining theory. Empirical risk minimization\nfor non-convex hypothesis functions cannot generally be solved efficiently. Existing works have applied methods from compressed sensing to\nbound generalization errors for arbitrary hypothesis functions. We begin in Section II by outlining our assumptions about the\nparameters of shallow ReLU networks and the data distribution to be interpolated. We provide upper bounds on the generalization error that are uniformly applicable\nacross the sublevel sets of the empirical risk in Section IV . We develop the key proof techniques for deriving the sample complexity of achieving\nNeuRIPs in Section V , by using the chaining theory of stochastic processes. 2 Notation and Assumptions\nIn this section, we will define the key notations and assumptions for the neural networks examined\nin this study. Shallow neural networks are constructed as weighted sums of neurons. When using the ReLU activation function, we can\napply a symmetry procedure to represent these as sums:\n\u00af\u03d5\u00afp(x) =nX\ni=0\u03d5pi(x),\nwhere \u00afpis the tuple (p1, . We denote the set of shallow networks indexed by a parameter set \u00afPby\n\u03a6\u00afP:={\u03d5\u00afp: \u00afp\u2208\u00afP}. We now equip the input space Rdof the networks with a probability distribution. This distribution\nreflects the sampling process and makes each neural network a random variable. Additionally, a\nrandom label ytakes its values in the output space R, for which we assume the following. The random sample x\u2208Rdand label y\u2208Rfollow a joint distribution \u00b5such that\nthe marginal distribution \u00b5xof sample x is standard Gaussian with density\n1\n(2\u03c0)d/2exp\u0012\n\u2212\u2225x\u22252\n2\u0013\n. As available data, we assume independent copies {(xj, yj)}m\nj=1of the random pair (x, y), each\ndistributed by \u00b5. 3 Concentration of the Empirical Norm\nSupervised learning algorithms interpolate labels yfor samples x, both distributed jointly by \u00b5on\nX \u00d7 Y . This task is often solved under limited data accessibility. The training data, respecting\nAssumption 2, consists of mindependent copies of the random pair (x, y). During training, the\ninterpolation quality of a hypothesis function f:X \u2192 Y can only be assessed at the given random\nsamples {xj}m\nj=1. After training, the quality of a resulting model is often measured by\nits generalization to new data not used during training. With Rd\u00d7Ras the input and output space,\nwe quantify a function f\u2019s generalization error with its expected risk:\nE\u00b5[f] :=E\u00b5|y\u2212f(x)|2. If the label ydepends deterministically on the associated sample x, we can treat yas an element of\nL2(Rd, \u00b5x), and the expected risk of any function fis the function\u2019s distance to y. By sketching any\nhypothesis function fwith the sample operator S, we perform a Monte-Carlo approximation of the\nexpected risk, which is termed the empirical risk:\n\u2225f\u22252\nm:=1\nmmX\nj=1(f(xj)\u2212yj)2=\r\r\r\r1\u221am(y1, . Under mild assumptions, || \u00b7 || mfails to be a norm. In order to obtain a well generalizing model, the goal is to identify a function fwith a low expected\nrisk. However, with limited data, we are restricted to optimizing the empirical risk. Our strategy for\nderiving generalization guarantees is based on the stochastic relation between both risks. For shallow ReLU networks of the form (1), we define\nthe restricted isometry property of the sampling operator Sas follows. Lets\u2208(0,1)be a constant and \u00afPbe a parameter set. While in the regime of low deviations and high probabilities the sample size\nmmust scale quadratically with u/s, in the regime of less precise statements one observes a linear\nscaling. Using the sufficient condition\nfor NeuRIPs from Theorem 1, we can provide generalization bounds for \u03d5\u00afp\u2208\u00afQy,\u03f5for any \u03f5 >0. Let\u00afPbe a parameter set that satisfies Assumption 1 and let u\u22652andt\u2265\u03f5 >0be\nconstants. We notice that \u00afQy,\u03f5is a set of shallow neural networks with 2nneurons. We normalize such\nnetworks with a function norm greater than tand parameterize them by\n\u00afRt:={\u03d5\u00afp\u2212\u03d5\u00afp\u2217: \u00afp\u2208\u00afP,\u2225\u03d5\u00afp\u2212\u03d5\u00afp\u2217\u2225\u00b5> t}. We assume that NeuRIPs( \u00afRt) holds for s= (t\u2212\u03f5)2/t2. We also note that \u00afRtsatisfies Assumption 1 with a rescaled constant cw/tand normalization-invariant\ncb, if\u00afPsatisfies it for cwandcb. However, in the chosen stochastic setting, we cannot assume that the termination of an\noptimization and the norm concentration at that network are independent events. 4We further select confidence parameters v1, v2> C 0, and define for some \u03c9\u22650the parameter\n\u03b7:= 2(1 \u2212s)\u2225\u03d5\u00afp\u2217\u2212y\u2225\u00b5+C3v1v2c\u00afp\u22171\n(1\u2212s)1/4+\u03c9\u221a\n1\u2212s. This reflects a lower limit of the generalization bound, which is the\nsum of the theoretically achievable minimum of the expected risk and the additional tolerance \u03c9. 5 Size Control of Stochastic Processes on Shallow Networks\nIn this section, we introduce the key techniques for deriving concentration statements for the em-\npirical norm, uniformly valid for sets of shallow ReLU networks. We begin by rewriting the event\nNeuRIPs( \u00afP) by treating \u00b5as a stochastic process, indexed by the parameter set \u00afP. To determine the size\nof a process, it is essential to determine the correlation between its variables. A small Sub-Gaussian metric between random variables indicates that their values are likely to be\nclose. To capture the Sub-Gaussian structure of a process, we introduce \u03f5-nets in the Sub-Gaussian\nmetric. Let\u00afPbe a parameter set satisfying Assumption 1. This\nmethod offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric. A sequence T= (Tk)k\u2208N0in a set Tis admissible if T0= 1andTk\u22642(2k). This bound is expected to\nbe of independent interest. Let\u00afPsatisfy Assumption 1. Let\u03a6be a set of real functions, indexed by a parameter set \u00afPand define\nN(\u03a6) :=Z\u221e\n0q\nlnN(\u03a6, d\u03c82, \u03f5)d\u03f5 and \u2206(\u03a6) := sup\n\u03d5\u2208\u03a6\u2225\u03d5\u2225\u03c82. Applying Lemma 3, and further applying the bounds on the covering numbers and the Talagrand-\nfunctional for shallow ReLU networks, the NeuRIPs( \u00afP) event holds in case of s >3. We use similar techniques and we define the following sets. \u2225f\u2225p= sup\n1\u2264q\u2264p\u2225f\u2225q\n\u039bk0,u= inf\n(Tk)sup\nf\u2208F\u221eX\nk02k\u2225f\u2212Tk(f)\u2225u2k\n6and we need the following lemma:\nLemma 9. Let P be a parameter set satisfying Assumption 1. To this end we have to bound the Talagrand functional, where we can use Dudley\u2019s inequality\n(Lemma 6). Let\u00afP\u2286(Rd\u00d7R\u00d7 \u00b11)nsatisfy Assumption 1. Future studies may focus on\nperforming uniform empirical norm concentration on the critical points of the empirical risk, which\ncould lead to even tighter bounds for the sample complexity. We also plan to apply our methods to input distributions more general than the Gaussian distribution. If generic Gaussian distributions can be handled, one could then derive bounds for the Sub-Gaussian\ncovering number for deep ReLU networks by induction across layers. Because these bounds scale with the Lipschitz constant of the function, they can be used to find \u03f5-nets\nfor neurons that have identical activation patterns. Generalization is\ncritical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeper\nunderstanding of the relationships between generalization, architectural design, and available data.",
        "Results and Findings": "Recent studies on neural networks suggest intriguing properties of the risk surface. This paper is organized as follows. These results\nensure a small generalization error, when any optimization algorithm finds a network with a small\nempirical risk. Given a weight vector w\u2208Rd, a bias b\u2208R, and a sign \u03ba\u2208 {\u00b1 1}, a ReLU neuron is a function\n\u03d5(w, b, \u03ba ) :Rd\u2192Rdefined as\n\u03d5(w, b, \u03ba )(x) =\u03ba\u03d5(wTx+b). Typically they are represented\nby a graph with nneurons in a single hidden layer. . . . . , (wn, bn, \u03ban)} \u2208\u00afP, we have\n\u2225wi\u2225 \u2264cwand|bi| \u2264cb. . . . . , y m)T\u2212S[f]\r\r\r\r2\n2. If {xj}m\nj=1\nare independently distributed by \u00b5x, the law of large numbers implies that for any f\u2208L2(Rd, \u00b5x)\nthe convergence\nlim\nm\u2192\u221e\u2225f\u2225m=\u2225f\u2225\u00b5. There exist universal constants C1,C2\u2208Rsuch that the following holds: For\nany sample operator S, constructed from random samples {xj}, respecting Assumption 2, let\n\u00afP\u2282(Rd\u00d7R\u00d7 {\u00b1 1})nbe any parameter set satisfying Assumption 1 and ||\u03d5\u00afp||\u00b5>1for all\n\u00afp\u2208\u00afP. Then, for any u > 2ands\u2208(0,1), NeuRIPs( \u00afP) is satisfied with probability at least\n1\u221217 exp( \u2212u/4)provided that\nm\u2265n3c2\nw\n(1\u2212s)2max\u0012\nC1(8cb+d+ ln(2))\nu, C2n2c2\nw\n(u/s)2\u0013\n. Therefore, instead of analyzing only the solution \u03d5\u2217\n\u00afpof the opti-\nmization problem, we introduce a tolerance \u03f5 >0for the empirical risk and provide bounds on the\ngeneralization error, which hold uniformly on the sublevel set\n\u00afQy,\u03f5:=\b\n\u00afp\u2208\u00afP:\u2225\u03d5\u00afp\u2212y\u22252\nm\u2264\u03f5\t\n. Furthermore, let the number mof samples satisfy\nm\u22658n3c2\nw(8cb+d+ ln(2)) max\u0012\nC1u\n(t\u2212\u03f5)2, C2n2c2\nwu\n(t\u2212\u03f5)2\u0013\n,\nwhere C1andC2are universal constants. Let {(xj, yj)}m\nj=1be a dataset respecting Assumption 2\nand let there exist a \u00afp\u2217\u2208\u00afPsuch that yj=\u03d5\u00afp\u2217(xj)holds for all j\u2208[m]. Then, with probability at\nleast1\u221217 exp( \u2212u/4), we have for all \u00afq\u2208\u00afQy,\u03f5that\n\u2225\u03d5\u00afq\u2212\u03d5\u00afp\u2217\u22252\n\u00b5\u2264t. There exist constants C0,C1,C2,C3,C4, and C5such that the following holds: Let \u00afP\nsatisfy Assumption 1 for some constants cw,cb, and let \u00afp\u2217\u2208\u00afPbe such that for some c\u00afp\u2217\u22650we\nhave\nE\u00b5\u0014\nexp\u0012(y\u2212\u03d5\u00afp\u2217(x))2\nc2\n\u00afp\u2217\u0013\u0015\n\u22642. We assume, for any s\u2208(0,1)and confidence parameter u >0, that the number of samples mis\nlarge enough such that\nm\u22658\n(1\u2212s)2max\u0012\nC1\u0012n3c2\nw(8cb+d+ ln(2))\nu\u0013\n, C2n2c2\nw\u0010u\ns\u0011\u0013\n. If we set \u03f5=\u2225\u03d5\u00afp\u2217\u2212y\u22252\nm+\u03c92as the tolerance for the empirical risk, then the probability that all\n\u00afq\u2208\u00afQy,\u03f5satisfy\n\u2225\u03d5\u00afq\u2212y\u2225\u00b5\u2264\u03b7\nis at least\n1\u221217 exp\u0010\n\u2212u\n4\u0011\n\u2212C5v2exp\u0012\n\u2212C4mv2\n2\n2\u0013\n. We have E[E(\u00afq,\u00afp\u2217)]>0. Then there exists a set \u02c6Pwith \u00afP\u2286\u02c6P\nsuch that\nN(\u03a6\u02c6P, d\u03c82, \u03f5)\u22642n\u00b7\u001216ncbcw\n\u03f5+ 1\u0013n\n\u00b7\u001232ncbcw\n\u03f5+ 1\u0013n\n\u00b7\u00121\n\u03f5sin\u00121\n16ncw\u0013\n+ 1\u0013d\n. We define it as follows. With the bounds on the Sub-Gaussian covering number from Lemma 1, we provide a bound on the\nTalagrand-functional for shallow ReLU networks in the following Lemma. Then we have\n\u03b32(\u03a6\u00afP, d\u03c82)\u2264r\n2\n\u03c0\u00128n3/2cw(8cb+d+ 1)\nln(2)p\n2 ln(2)\u0013\n. Then, for any u\u22652, we have with probability at least 1\u221217 exp( \u2212u/4)that\nsup\n\u03d5\u2208\u03a6|\u2225\u03d5\u2225m\u2212 \u2225\u03d5\u2225\u00b5| \u2264u\u221am\u0014\nN(\u03a6) +10\n3\u2206(\u03a6)\u0015\n. Since we assume ||\u03d5\u00afp||\u00b5>1for all \u00afp\u2208\u00afP, we have\nsup\n\u00afp\u2208\u00afP|\u2225\u03d5\u00afp\u2225m\u2212 \u2225\u03d5\u00afp\u2225\u00b5| \u2264sup\n\u00afp\u2208\u00afP|\u2225\u03d5\u00afp\u2225m\u2212 \u2225\u03d5\u00afp\u2225\u00b5|/\u2225\u03d5\u00afp\u2225\u00b5. With the previous results, we can now derive\nuniform generalization error bounds in the sublevel set of the empirical risk. For any set Fof functions and u\u22651, we have\n\u039b0,u(F)\u22642\u221ae(\u03b32(F, d\u03c82) + \u2206( F)). Then, for any u\u22651, we have with\nprobability at least 1\u221217 exp( \u2212u/4)that\nsup\n\u00afp\u2208P\u2225\u03d5\u00afp\u2225m\u2212 \u2225\u03d5\u00afp\u2225\u00b5\u2264u\u221am\u0010\n16n3/2cw(8cb+d+ 1) + 2 ncw\u0011\n. Then there exist universal constants\nC1,C2such that\nsup\n\u00afp\u2208P\u2225\u03d5\u00afp\u2225m\u2212 \u2225\u03d5\u00afp\u2225\u00b5\u2264r\n2\n\u03c0\u00128n3/2cw(8cb+d+ 1)\nln(2)p\n2 ln(2)\u0013\n. We defined the Neural Restricted Isometry Property\n(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs, which depends on\nrealistic parameter bounds and the network architecture. We applied our findings to derive upper\nbounds on the expected risk, which are valid uniformly across sublevel sets of the empirical risk. If a network optimization algorithm can identify a network with a small empirical risk, our results\nguarantee that this network will generalize well. Broader Impact\nSupervised machine learning now affects both personal and public lives significantly. 7",
        "Conclusion": "Our results show that all\nnetworks with sufficiently small empirical risk achieve uniform generalization. (NeuRIPs) as a uniform norm concentration event. The derived results are\nsummarized in Section VI, where we also explore potential future research directions. , p n). We say that the Neural Restricted\nIsometry Property (NeuRIPs( \u00afP)) is satisfied if, for all \u00afp\u2208\u00afPit holds that\n(1\u2212s)\u2225\u03d5\u00afp\u2225\u00b5\u2264 \u2225\u03d5\u00afp\u2225m\u2264(1 +s)\u2225\u03d5\u00afp\u2225\u00b5. Proof. In this case, for all \u00afq\u2208\u00afQy,\u03f5, we have that\n\u2225\u03d5\u00afq\u2212\u03d5\u00afp\u2217\u2225m\u2265tand thus \u00afq /\u2208\u00afQ\u03d5\u00afp\u2217,\u03f5, which implies that \u2225\u03d5\u00afq\u2212\u03d5\u00afp\u2217\u2225\u00b5\u2264t. Theorem 1 gives a lower bound on the sample complexity for\nNeuRIPs( \u00afRt), completing the proof. It suffices to show, that within the stated confidence level we have \u2225\u03d5\u00afq\u2212y\u2225\u00b5> \u03b7. This implies the\nclaim since \u2225\u03d5\u00afq\u2212y\u2225m\u2264\u03f5implies \u2225\u03d5\u00afq\u2212y\u2225\u00b5\u2264\u03b7. It now only remains\nto strengthen the condition on \u03b7 >3\u2225\u03d5\u00afp\u2217\u2212y\u2225\u00b5to achieve E(\u00afq,\u00afp\u2217)> \u03c92. Finally in Appendix E, Theorem 12 gives\na general bound to achieve\nE(\u00afq,\u00afp\u2217)> \u03c92\nuniformly for all \u00afqwith\u2225\u03d5\u00afq\u2212\u03d5\u00afp\u2217\u2225\u00b5> \u03b7. Theorem 3 then follows as a simplification. The event\nNeuRIPs( \u00afP) holds if and only if we have\nsup\n\u00afp\u2208\u00afP|\u2225\u03d5\u00afp\u2225m\u2212 \u2225\u03d5\u00afp\u2225\u00b5| \u2264ssup\n\u00afp\u2208\u00afP\u2225\u03d5\u00afp\u2225\u00b5. 5Lemma 1. The bounds on the sample complexity for achieving the NeuRIPs event, from Theorem 1, are proven\nby applying these Lemmata. Proof. To finish the proof, we apply the bounds on the covering numbers provided by Theorem\n6. 7 Conclusion\nIn this study, we investigated the empirical risk surface of shallow ReLU networks in terms of uniform\nconcentration events for the empirical norm. By deriving uniform concentration statements, we\nhave resolved the problem of independence between the termination of an optimization algorithm at\na certain network and the empirical risk concentration at that network. We have discussed the concepts and demonstrated the effectiveness of using uniform concentration\nevents for generalization guarantees of common supervised machine learning algorithms.",
        "label": 4
    },
    {
        "Abstract": "Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems\nwith Solutions Exhibiting Weak Minty Properties\nAbstract\nThis research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-\nstrate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has\nalready demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time. For a given operator F:Rd\u2192Rd, there is a point u\u2217\u2208Rdand a parameter \u03c1 >0such that:\n\u27e8F(u), u\u2212u\u2217\u27e9 \u2265 \u2212\u03c1\n2\u2225F(u)\u22252\u2200u\u2208Rd. 1.2 Related literature\nWe will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap\nfunction or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,\nor under the Polyak-\u0141ojasiewicz assumption. First and\nforemost is the extensively studied setting of \u03bd-weak monotonicity:\n\u27e8F(u)\u2212F(v), u\u2212v\u27e9 \u2265 \u2212 \u03bd\u2225u\u2212v\u22252. In particular, we can choose \u03b3= 1anda <1\n2L. The main advantage resides in the fact that the\nLipschitz constant of the operator Fdoes not need to be known. 5.1 Von Neumann\u2019s ratio game\nWe consider von Neumann\u2019s ratio game, which is given by:\nmin\nx\u2208\u2206mmax\ny\u2208\u2206nV(x, y) =\u27e8x, Ry\u27e9\n\u27e8x, Sy\u27e9, (5)\nwhere R\u2208Rm\u00d7nandS\u2208Rm\u00d7nwith\u27e8x, Sy\u27e9>0for all x\u2208\u2206m, y\u2208\u2206n, with \u2206 :={z\u2208Rd:zi>0,Pd\ni=1zi= 1}denoting\nthe unit simplex. 5.3 Lower bound example\nThe following min-max problem was introduced as a lower bound on the dependence between \u03c1andLfor EG+:\nmin\nx\u2208Rmax\ny\u2208R\u00b5xy+\u03b6\n2(x2\u2212y2).",
        "Methodology": "While practical methods have been developed\nthat generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization\ncomponent and nonconcave in the maximization component remains limited, with some research even suggesting intractability in\ncertain cases. Surprisingly, these problems did not appear to possess any of\nthe recognized favorable characteristics, such as monotonicity or Minty solutions. While these earlier studies focused on the monotone setting, the true significance of \u03b3becomesapparent only when dealing with weak Minty solutions. In this context, we find that \u03b3must be greater than 1 to ensure convergence,\na phenomenon that is not observed in monotone problems. Specifically, it\nis essential that the step size exceeds a value proportional to \u03c1. Simultaneously, as is typical, the step size is limited from above\nby the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than1\n4L, their\nconvergence claim is valid only if \u03c1 <1\n4L. This condition was later improved to \u03c1 <1\n2Lfor the choice \u03b3= 1and to \u03c1 <1\nLfor\neven smaller values of \u03b3. As in the monotone setting, OGDA requires a smaller step size than EG. 1.1 Contribution\nOur contributions are summarized as follows:\n1.We establish a new convergence rate of O(1/k), measured by the squared operator norm, for a modified version of OGDA,\nwhich we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to\nthe Minty variational inequality. 4.We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without\nrequiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in\nareas with low curvature, enabling convergence where a fixed step size strategy might fail. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the\nlength of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is\nalso proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step\nsize we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps\nper descent step, achieving the same O(1/k)rate as EG. Although previously studied under the term \"cohypomonotonicity,\" the concept of negative comono-\ntonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty\nsolutions, and only a limited number of studies have examined methods in this context. Whether such acceleration is possible in the more general setting of weak Minty\nsolutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty\nsolution). However, we did not observe this in our experiments, emphasizing the need to differentiate\nthis class from problems with weak Minty solutions. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively\ncomonotone. The idea dates back even further and has\nalso been studied in the mathematical programming community. (MVI)\nFor a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. Specifically, there are nonmonotone problems with Stampacchia solutions but\nwithout any Minty solutions. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring\nthe non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and\nspurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. Clearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1. Behavior with respect to the solution. While the above properties are standard assumptions in the literature, it is usually sufficient\nto require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of\nmonotonicity, it is enough to ask for the operator Fto be star-monotone, i.e.,\n\u27e8F(u), u\u2212u\u2217\u27e9 \u22650,\nor star-cocoercive,\n\u27e8F(u), u\u2212u\u2217\u27e9 \u2265\u03b3\u2225F(u)\u22252. In this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the\noperator Fto be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the\nabove star notions are sometimes required to hold for all solutions u\u2217, in the following we only require it to hold for a single solution. (3)\nThen, for all k\u22650,\nmin\ni=0,...,k\u22121\u2225F(ui)\u22252\u22641\nka\u03b3(a\u2212\u03c1)\u2225u0+aF(u0)\u2212u\u2217\u22252. The first observation is that we would like to choose aas large as possible, as this allows us to treat the largest class of problems\nwith\u03c1 < a . To be able to choose a large step size a, we must decrease \u03b3, as evident from (3). However, this degrades the algorithm\u2019s\nspeed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. Furthermore, we want to point out that the condition \u03c1 <1\nLis precisely the best possible bound for EG+. LetF:Rd\u2192Rdbe monotone and L-Lipschitz. However, we\nwant to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. This was later improved to a\u22641\n3L. We assume here that the estimator Fis unbiased, i.e., E[F(uk, \u03be)|uk\u22121] =F(uk), and has\nbounded variance E[\u2225F(uk, \u03be)\u2212F(uk)\u22252]\u2264\u03c32. In practice, large batch sizes of order O(\u03f5\u22121)are typically not desirable; instead, a small or decreasing step size is preferred. Unfortunately,\nthe current analysis does not allow for variable \u03b3. Additionally, it is well known that rough estimates will lead to\nsmall step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in\nchoosing large step sizes. For these reasons, we want\nto outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried\nout. Due to the fact that the literature on adaptive methods,\nespecially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially\ninteresting properties. We use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone\ndecreasing behavior. To the best of our\nknowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch\nis the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of Fat\nall. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative\nconvergence result is still open. Moreover, it is bounded away from zero by the simple observation that\nak\u2265min{a0, \u03c4/L}>0. The sequence therefore converges to a positive number, which we denote by a\u221e:= lim kak. LetF:Rd\u2192RdbeL-Lipschitz that satisfies Assumption 1, where u\u2217denotes any weak Minty solution, with\na\u221e>2\u03c1, and let (uk)k\u22650be the iterates generated by Algorithm 3 with \u03b3=1\n2and\u03c4\u2208(0,1). In such cases, these larger step sizes come with the additional advantage\nthat they allow us to solve a richer class of problems, as we are able to relax the condition \u03c1 <1\n4Lin the case of EG+ to \u03c1 < a \u221e/2,\nwhere a\u221e= lim kak\u2265\u03c4/L. This drawback could be mitigated by choosing \u03c4smaller. However, this will result in poor performance\ndue to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be\ncircumvented, and authors instead focused on the convergence of the iterates without any rate. 5 Numerical experiments\nIn the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see\nAlgorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification\nof EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with\nan initial guess made by second-order information, whose extra cost we ignore in the experiments. In addition, it was also observed that the limit cycle closer to\nthe solution repels possible trajectories of iterates, thus \"shielding\" the solution. In light of this\nobservation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz\nconstant. On top of this, it does so at a faster rate and without the need for additional computations in\nthe backtracking procedure. Although the general upper bound proved in Theorem 3.1 only states convergence in the case \u03c1 <1\nL, we\nobserve rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios. Very recently, it was demonstrated that the O(1/k)bounds on the squared operator norm for EG and OGDA for the\nlast iterate (and not just the best one) are valid even in the negatively comonotone setting. Our EG+ method with adaptive step size accomplishes this even without the added expense of a\nbacktracking linesearch.article graphicx\n7",
        "Results and Findings": "We establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within\nthis framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared\noperator norm, a result that aligns with the extragradient method (EG). A specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)\nexhibited favorable convergence behavior in experimental settings. Specifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0and a parameter 0< \u03b3\u22641\nas follows:\nuk= \u00afuk\u2212aF(\u00afuk),\n\u00afuk+1= \u00afuk\u2212\u03b3aF(uk),\u2200k\u22650,\ncan achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration. Nevertheless, through a different\nanalysis, we are able to match the most general condition on the weak Minty parameter \u03c1 <1\nLfor appropriate \u03b3anda. 2.Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible\nstep sizes for OGDA+ and obtain the most favorable result known for the standard method ( \u03b3= 1). We demonstrate a complexity bound of O(\u03f5\u22122)for a stochastic variant of the OGDA+ method. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,\ntermed \"weak Minty,\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions. It was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing\nthe resulting strongly monotone VI using any convergent method. An anchored version of EG was studied, and\nan improved convergence rate of O(1/k2)(in terms of the squared operator norm) was shown. Similarly, an accelerated version of\nthe reflected gradient method was investigated. The concept of \u03b1-interaction dominance for nonconvex-nonconcave min-max problems was investigated,\nand it was shown that the proximal-point method converges sublinearly if this condition is met in yand linearly if it is met in both\ncomponents. Departing from monotonicity. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this\nproperty. LetF:Rd\u2192RdbeL-Lipschitz continuous satisfying Assumption 1 with1\nL> \u03c1, and let (uk)k\u22650be the iterates\ngenerated by Algorithm 1 with step size asatisfying a > \u03c1 and\naL\u22641\u2212\u03b3\n1 +\u03b3. In particular, as long as \u03c1 <1\nL, we can find a \u03b3small enough such that the above bound holds. In practice, the strategy of decreasing \u03b3until convergence is achieved, but not further, yields reasonable results. If aL=2\u2212\u03b3\n2+\u03b3\u2212\u03f5for\u03f5 >0, then the iterates generated by OGDA+\nfulfill\nmin\ni=0,...,k\u22121\u2225F(ui)\u22252\u22642\nka2\u03b32\u03f5\u2225u0+aF(u0)\u2212u\u2217\u22252. To summarize, not\nonly do we show for the first time that the step size of a generalization of OGDA can go above1\n2L, but we also provide the least\nrestrictive bound for any value of \u03b3. We show that we can still guarantee convergence by using batch sizes Bof order\nO(\u03f5\u22121). LetF:Rd\u2192RdbeL-Lipschitz satisfying Assumption 1 with1\nL> \u03c1, and let (uk)k\u22650be the sequence of\niterates generated by stochastic OGDA+, with aand\u03b3satisfying \u03c1 < a <1\u2212\u03b3\n1+\u03b31\nL. Then, to visit an \u03f5-stationary point such that\nmini=0,...,k\u22121E[\u2225F(ui)\u22252]< \u03f5, we require\n1\nka\u03b3(a\u2212\u03c1)\u2225u0+a\u02dcg0\u2212u\u2217\u22252max\u001a\n1,4\u03c32\naL\u03f5\u001b\ncalls to the stochastic oracle \u02dcF, with large batch sizes of order O(\u03f5\u22121). A version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the\noptimal rates for both smooth and nonsmooth problems without modification. fork= 0,1, ...do\nFind the step size:\nak= min\u001a\nak\u22121,\u03c4\u2225\u00afuk\u2212\u00afuk\u22121\u2225\n\u2225F(\u00afuk)\u2212F(\u00afuk\u22121)\u2225\u001b\n(4)\nCompute next iterate:\nuk= \u00afuk\u2212akF(\u00afuk)\n\u00afuk+1= \u00afuk\u2212ak\u03b3F(uk). Then, there exists a k0\u2208Nsuch that\nmin\ni=k0,...,k\u2225F(uk)\u22252\u22641\nk\u2212k0L\n\u03c4(a\u221e/2\u2212\u03c1)\u2225\u00afuk0\u2212u\u2217\u22252. Interestingly, we still observe good convergence behavior, although\nan estimated \u03c1is more than ten times larger than the estimated Lipschitz constant. Later, it was noticed that, restricted to the box\n\u2225(x, y)\u2225\u221e<3, the above-mentioned solution is weak Minty with \u03c1\u22652\u00b70.477761 , which is much larger than1\n2L\u22480.08. In line\nwith these observations, we can see that none of the fixed step size methods with a step size bounded by1\nLconverge. (7)\nIn particular, it was stated that EG+ (with any \u03b3) and constant step size a=1\nLconverges for this problem if and only if (0,0)is a\nweak Minty solution with \u03c1 <1\u2212\u03b3\nL, where \u03c1andLcan be computed explicitly in the above example and are given by:\nL=p\n\u00b52+\u03b62and \u03c1=\u00b52\u2212\u03b62\n2\u00b5. By choosing \u00b5= 3and\u03b6=\u22121, we get exactly \u03c1=1\nL, therefore predicting divergence of EG+ for any \u03b3, which is exactly what is\nempirically observed. In general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the\nmajority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. The analysis of various methods in the presence of weak Minty solutions indicates\nthat convergence can be lost if the step size is excessively small and sometimes needs to be larger than1\nL, which one can typically\nonly hope for in adaptive methods.",
        "Conclusion": "We address the following question with an affirmative answer:\nCan OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions? 3. Weak Minty. The convergence of the OGDA method was proven, but without a\nspecific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. 33 OGDA for problems with weak Minty solutions\nThe generalized version of OGDA, which we denote with a \"+\" to emphasize the presence of the additional parameter \u03b3, is given by:\nAlgorithm 1 OGDA+\nRequire: Starting point u0=u\u22121\u2208Rd, step size a >0and parameter 0< \u03b3 < 1.\nfork= 0,1, ...do\nuk+1=uk\u2212a((1 + \u03b3)F(uk)\u2212F(uk\u22121))\nend for\nTheorem 3.1. (\u03bei)B\ni=1and compute estimator \u02dcgk=1\nBPB\ni=1F(uk, \u03bek\ni)\nuk+1=uk\u2212a((1 + \u03b3)\u02dcgk\u2212\u02dcgk\u22121)\nend for\n4Theorem 3.3. end for\nClearly, akis monotonically decreasing by construction. 66 Conclusion\nMany intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave\nframework. Finally, we note that the previous paradigm in pure minimization of \"smaller step size ensures convergence\" but \"larger step size\ngets there faster,\" where the latter is typically constrained by the reciprocal of the gradient\u2019s Lipschitz constant, does not appear\nto hold true for min-max problems anymore.",
        "label": 5
    },
    {
        "Abstract": "Examining the Convergence of Denoising Diffusion Probabilistic\nModels: A Quantitative Analysis\nAbstract\nDeep generative models, particularly diffusion models, are a significant family within deep learning. 2 Preliminaries\nThroughout this paper, lowercase letters are used to represent both probability measures and their densities with respect to the\nLebesgue measure, and variables are added in parentheses to enhance readability (e.g., q(xt|xt\u22121)to denote a time-dependent\nconditional distribution). Note that it is not assumed that \u00b5has a density with respect to\nthe Lebesgue measure. p \u03b8(x1|x2)p\u03b8(\u00b7|x1). p \u03b8(x1|x2)p\u03b8(\u00b7|x1). ** Assume the instance space Xhas finite diameter \u2206 = supx,x\u2032\u2208X||x\u2212x\u2032||<\u221e, and let \u03bb >0and\u03b4\u2208(0,1)be\nreal numbers. , \u03b1 Tis a fixed noise schedule such that 0< \u03b1t<1for all t. This definition implies that at each time step 1\u2264t\u2264T,\n5q(xt|x0) =N(xt;\u221a\u00af\u03b1tx0,(1\u2212\u00af\u03b1t)I),with\u00af\u03b1t=tY\ni=1\u03b1i.",
        "Methodology": "In contrast to earlier research, this analysis does not rely on presumptions regarding\nthe learned score function. (2023), and the proofs presented are fundamental. Two primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative\nmodels (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while\nsimultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Conversely, SGMs\nemploy score-matching methods to approximate the score function of the data-generating distribution, subsequently generating new\nsamples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function, adding varying\nnoise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the score\nfunction for all noise levels has been proposed. Moreover, a unified perspective of both methods using stochastic\ndifferential equations (SDEs) has been derived. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-based\nframework, necessitating assumptions about the effectiveness of the learned score function. In this research, a different strategy is employed, applying methods created for V AEs to DDPMs, which can be viewed as hierarchical\nV AEs with fixed encoders. This method enables the derivation of quantitative, Wasserstein-based upper bounds without making\nassumptions about the data distribution or the learned score function, and with simple proofs that do not need the SDE toolkit. Furthermore, the bounds presented here do not involve any complex discretization steps, as the forward and backward processes are\nconsidered discrete-time from the beginning, rather than being viewed as discretizations of continuous-time processes. However,\nthese studies frequently depend on restrictive assumptions regarding the data-generating distribution, produce non-quantitative upper\nbounds, or exhibit exponential dependencies on certain parameters. Some bounds are based on very restrictive assumptions about the data-generating distribution, such as log-Sobolev inequalities,\nwhich are unrealistic for real-world data distributions. Assuming L2 accurate scoreestimation, some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, but\ntheir Wasserstein-based bounds are not quantitative. 1.2 Our contributions\nIn this study, strong assumptions about the data-generating distribution are avoided, and a quantitative upper bound on the Wasserstein\ndistance is established without exponential dependencies on problem parameters, including the ambient space dimension. Moreover,\na common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According to\nsome, providing precise guarantees for the estimation of the score function is challenging, as it necessitates an understanding of the\nnon-convex training dynamics of neural network optimization, which is currently beyond reach. Therefore, upper bounds are derived\nwithout making assumptions about the learned score function. Instead, the bound presented here is dependent on a reconstruction\nloss calculated over a finite independent and identically distributed (i.i.d.) This method is inspired by previous work on\nV AEs. This approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution, avoids\nexponential dependencies on the dimension, and provides a quantitative upper bound based on the Wasserstein distance. Furthermore,\nthis method benefits from utilizing very straightforward and basic proofs. Given\nprobability measures p, q\u2208M+\n1(X)and a real number k >1, the Wasserstein distance of order kis defined as (Villani, 2009):\nWk(p, q) = inf\n\u03b3\u2208\u0393(p,q)\u0012Z\nX\u00d7X||x\u2212y||kd\u03b3(x, y)\u00131/k\n,\nwhere \u0393(p, q)denotes the set of couplings of pandq, meaning the set of joint distributions on X\u00d7Xwith respective marginals p\nandq. 2.1 Denoising Diffusion Models\nInstead of employing the SDE framework, diffusion models are presented using the DDPM formulation with discrete-time processes. Both processes are\nindexed by time 0\u2264t\u2264T, where the number of time steps Tis a predetermined choice. **The forward process. ** The forward process transforms a data point x0\u223c\u00b5into a noise distribution q(xT|x0)through a sequence\nof conditional distributions q(xt|xt\u22121)for1\u2264t\u2264T. **The backward process. The objective of the\nbackward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samples\nfrom the distribution \u00b5. Following previous work, it is assumed that the backward process is defined by Gaussian distributions\np\u03b8(xt\u22121|xt)for2\u2264t\u2264Tas\np\u03b8(xt\u22121|xt) =N(xt\u22121;g\u03b8\nt(xt), \u03c32\ntI),\nand\np\u03b8(x0|x1) =g\u03b8\n1(x1),\nwhere the variance parameters \u03c32\nt\u2208R\u22650are defined by a fixed schedule, the mean functions g\u03b8\nt:RD\u2192RDare learned using a\nneural network (with parameters \u03b8) for2\u2264t\u2264T, andg\u03b8\n1:RD\u2192Xis a separate function dependent on \u03c31. In practice, the same\nnetwork has been used for the functions g\u03b8\ntfor2\u2264t\u2264T, and a separate discrete decoder for g\u03b8\n1. 2Generating new samples from a trained diffusion model is accomplished by sampling xt\u22121\u223cp\u03b8(xt\u22121|xt)for1\u2264t\u2264T, starting\nfrom a noise vector xT\u223cp(xT)sampled from the prior p(xT). The following assumption is made regarding the backward process. In other words, g\u03b8\ntisK\u03b8\nt-Lipschitz continuous. Given a finite set S={x1\n0, . , xn\n0}i.i.d.\u223c\u00b5, the regenerated distribution is defined as the following mixture:\n\u00b5\u03b8\nn=1\nnnX\ni=1\u03c0\u03b8(\u00b7|xi\n0). This definition is analogous to the empirical regenerated distribution defined for V AEs. Hence, both \u03c0\u03b8(\u00b7)and\u03c0\u03b8(\u00b7|x0)are defined using the backward process, with the difference that \u03c0\u03b8(\u00b7)starts with the prior\np(xT) =N(xT; 0, I), while \u03c0\u03b8(\u00b7|x0)starts with the noise distribution q(xT|x0). Hence, given a noise vector xTand a sample x0, the loss l\u03b8(xT, x0)represents the average Euclidean distance between x0and any\nsample obtained by passing xTthrough the backward process. Since the triangle inequality implies\nW1(\u00b5, \u03c0\u03b8(\u00b7))\u2264W1(\u00b5, \u00b5\u03b8\nn) +W1(\u00b5\u03b8\nn, \u03c0\u03b8(\u00b7)),\nthe distance W1(\u00b5, \u03c0\u03b8(\u00b7))can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The\nupper bound on W1(\u00b5, \u00b5\u03b8\nn)is obtained using a straightforward adaptation of a proof. First, W1(\u00b5, \u00b5\u03b8\nn)is upper-bounded using the\nexpectation of the loss function l\u03b8, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependent\non the empirical risk and the prior-matching term. Using the definitions and assumptions of the previous section, the following inequality holds with probability at least\n1\u2212\u03b4over the random draw of S={x1\n0, . * Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. * The first term of the\nright-hand side is the average reconstruction loss computed over the sample S={x1\n0, . Note that for each 1\u2264i\u2264n, the\nexpectation of l\u03b8(xT|xi\n0)is only computed with respect to the noise distribution q(xT|xi\n0)defined by xi\n0itself. In Remark 3.2 below, we show that the assumption that K\u03b8\nt<1\nfor all tis a quite reasonable one. * The hyperparameter \u03bbcontrols the trade-off between the prior-matching (KL) term and the\ndiameter term \u22062. IfK\u03b8\nt<1for all 1\u2264t\u2264TandT\u2192 \u221e , then the convergence of the bound largely depends on the choice of \u03bb. In that case, \u03bb\u221dn1/2leads to faster convergence, while \u03bb\u221dnleads to slower convergence to a smaller quantity. * The last term of the equation does not depend on the\nsample size n. Hence, the upper bound given by Theorem 3.1 does not converge to 0 as n\u2192 \u221e . With probability at least 1\u2212\u03b4over the randomness of the sample\nS={x1\n0, . Now, let us focus our attention on the second term of the right-hand side of the equation, namely W1(\u00b5\u03b8\nn, \u03c0\u03b8(\u00b7)). This part is trickier\nthan for V AEs, for which the generative model\u2019s distribution is simply a pushforward measure. Here, we have a non-deterministic\nsampling process with Tsteps. Assumption 1 leads to the following lemma on the backward process. **Lemma 3.3. ** For any given x1, y1\u2208X, we have\nEp\u03b8(x0|x1)Ep\u03b8(y0|y1)[||x0\u2212y0||]\u2264K\u03b8\n1||x1\u2212y1||. ** For the first part, let x1, y1\u2208X. For the second part, let 2\u2264t\u2264Tandxt, yt\u2208X. Since p\u03b8(xt\u22121|xt) =N(xt\u22121;g\u03b8\nt(xt), \u03c32\ntI), the reparameterization trick implies\nthat sampling xt\u22121\u223cp\u03b8(xt\u22121|xt)is equivalent to setting\nxt\u22121=g\u03b8\nt(xt) +\u03c3t\u03f5t,with\u03f5t\u223cN(0, I). \u03c3tE\u03f5,\u03f5\u2032[||\u03f5\u2212\u03f5\u2032||],\nwhere \u03f5, \u03f5\u2032\u223cN(0, I). ** Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step. \u03c3tE\u03f5,\u03f5\u2032[||\u03f5\u2212\u03f5\u2032||],\nwhere \u03f5, \u03f5\u2032\u223cN(0, I). 3.3 Special case using the forward process of Ho et al. (2020)\nTheorem 3.1 establishes a general upper bound that holds for any forward process, as long as the backward process satisfies\nAssumption 1. In this section, we specialize the statement of the theorem to the particular case of the forward process defined in\nprevious work. The optimization objective to train the backward process ensures that for each time step t, the distribution p\u03b8(xt\u22121|xt)remains close\nto the ground-truth distribution q(xt\u22121|xt, x0)given by\nq(xt\u22121|xt, x0) =N(xt\u22121; \u02dc\u00b5q\nt(xt, x0),\u02dc\u03c32\ntI),\nwhere\n\u02dc\u00b5q\nt(xt, x0) =\u221a\u03b1t(1\u2212\u00af\u03b1t\u22121)\n1\u2212\u00af\u03b1txt+\u221a\u00af\u03b1t\u22121(1\u2212\u03b1t)\n1\u2212\u00af\u03b1tx0. For a given x0\u223c\u00b5, let us take a look at the Lipschitz norm of x7\u2192\u02dc\u00b5q\nt(x, x0). Remark 3.2 shows that the Lipschitz norm of the mean function \u02dc\u00b5q\nt(\u00b7, x0)does not depend on x0. Since g\u03b8\nt(\u00b7, x0)is optimized to match \u02dc\u00b5q\nt(\u00b7, x0)for each x0in the training set, and all the functions \u02dc\u00b5q\nt(\u00b7, x0)have the same Lipschitz\nnorm K\u2032\nt, we believe it is reasonable to assume g\u03b8\ntis Lipschitz continuous as well. ** With the definitions of this section, the prior matching term KL(q(xT|x0)||p(xT))has the following\nclosed form:\nKL(q(xT|x0)||p(xT)) =1\n2\u0002\n\u2212Dlog(1\u2212\u00af\u03b1T)\u2212D\u00af\u03b1T+ \u00af\u03b1T||x0||2\u0003\n. **Upper-bounds on the average distance between Gaussian vectors. ** With the definitions of this section, the inequality of Theorem 3.1 implies that with probability\nat least 1\u2212\u03b4over the randomness of {x1\n0, .",
        "Results and Findings": "This study\nprovides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion model\nand the target distribution. Furthermore, the findings are applicable to any data-generating distributions within\nrestricted instance spaces, even those lacking a density relative to the Lebesgue measure, and the upper limit is not\nexponentially dependent on the ambient space dimension. The primary finding expands upon recent research by\nMbacke et al. These models have demonstrated remarkable empirical results in generating images and audio,\nas well as in various other applications. 1.1 Related Works\nThere has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. Furthermore, some studies establish upper bounds on the Kullback-Leibler\n(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by the\ndiffusion model; however, unless strong assumptions are made about the support of the data-generating distribution, KL and TV\nreach their maximum values. Quantitative Wasserstein distance upper bounds under the manifold hypothesis\nhave been derived, but these bounds exhibit exponential dependencies on some of the problem parameters. sample. ** It is assumed that for each 1\u2264t\u2264T, there exists a constant K\u03b8\nt>0such that for every x1, x2\u2208X,\n||g\u03b8\nt(x1)\u2212g\u03b8\nt(x2)|| \u2264K\u03b8\nt||x1\u2212x2||. . . . . E p\u03b8(x1|x2)Ep\u03b8(\u02c6x0|x1)[f(\u02c6x0)]. . . . . In other words, for any function f:X\u2192R, the expectation of fwith respect to \u03c0\u03b8(\u00b7)is\nE\u03c0\u03b8(\u02c6x0)[f(\u02c6x0)] =Ep(xT)Ep\u03b8(xT\u22121|xT). . . E p\u03b8(x1|x2)Ep\u03b8(\u02c6x0|x1)[f(\u02c6x0)]. . . E p\u03b8(x1|x2)Ep\u03b8(\u02c6x0|x1)[||x0\u2212\u02c6x0||]. 33 Main Result\n3.1 Theorem Statement\nWe are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generating\ndistribution \u00b5and the learned distribution \u03c0\u03b8(\u00b7). . . , xn\n0}i.i.d.\u223c\u00b5:\nW1(\u00b5, \u03c0\u03b8(\u00b7))\u22641\nnnX\ni=1Eq(xT|xi\n0)[l\u03b8(xT, xi\n0)] +1\n\u03bbnnX\ni=1KL(q(xT|xi\n0)||p(xT)) +1\n\u03bbnlogn\n\u03b4+\u03bb\u22062\n8n\n+ TY\nt=1K\u03b8\nt! Eq(xT|xi\n0)Ep(yT)[||xT\u2212yT||]\n+TX\nt=2 t\u22121Y\ni=1K\u03b8\ni! . . . . This is because the product of K\u03b8\nt\u2019s then converges to 0. 3.2 Proof of the main theorem\nThe following result is an adaptation of a previous result. . . , xn\n0}i.i.d.\u223c\u00b5, the following holds:\nW1(\u00b5, \u00b5\u03b8\nn)\u22641\nnnX\ni=1Eq(xT|xi\n0)[l\u03b8(xT, xi\n0)] +1\n\u03bbnnX\ni=1KL(q(xT|xi\n0)||p(xT)) +1\n\u03bbnlogn\n\u03b4+\u03bb\u22062\n8n. The proof of this result is a straightforward adaptation of a previous proof. Moreover, if 2\u2264t\u2264T, then for any given xt, yt\u2208X, we have\n4Ep\u03b8(xt\u22121|xt)Ep\u03b8(yt\u22121|yt)[||xt\u22121\u2212yt\u22121||]\u2264K\u03b8\nt||xt\u2212yt||+\u03c3tE\u03f5,\u03f5\u2032[||\u03f5\u2212\u03f5\u2032||],\nwhere \u03f5, \u03f5\u2032\u223cN(0, I), meaning E\u03f5,\u03f5\u2032is a shorthand for E\u03f5,\u03f5\u2032\u223cN(0,I). Since according to the equation p\u03b8(x0|x1) =\u03b4g\u03b8\n1(x1)(x0)andp\u03b8(y0|y1) =\u03b4g\u03b8\n1(y1)(y0),\nthen\nEp\u03b8(x0|x1)Ep\u03b8(y0|y1)[||x0\u2212y0||] =||g\u03b8\n1(x1)\u2212g\u03b8\n1(y1)|| \u2264K\u03b8\n1||x1\u2212y1||. Using the above equation, the triangle inequality, and Assumption 1, we obtain\nEp\u03b8(xt\u22121|xt)Ep\u03b8(yt\u22121|yt)[||xt\u22121\u2212yt\u22121||]\n=E\u03f5t,\u03f5\u2032\nt\u223cN(0,I)[||g\u03b8\nt(xt) +\u03c3t\u03f5t\u2212g\u03b8\nt(yt)\u2212\u03c3t\u03f5\u2032\nt||]\n\u2264E\u03f5t,\u03f5\u2032\nt\u223cN(0,I)[||g\u03b8\nt(xt)\u2212g\u03b8\nt(yt)||] +\u03c3tE\u03f5t,\u03f5\u2032\nt\u223cN(0,I)[||\u03f5t\u2212\u03f5\u2032\nt||]\n\u2264K\u03b8\nt||xt\u2212yt||+\u03c3tE\u03f5,\u03f5\u2032[||\u03f5\u2212\u03f5\u2032||],\nwhere \u03f5, \u03f5\u2032\u223cN(0, I). Next, we can use the inequalities of Lemma 3.3 to prove the following result. . . E p\u03b8(x0|x1)Ep\u03b8(y0|y1)[||x0\u2212y0||]\n\u2264 TY\nt=1K\u03b8\nt! ||xT\u2212yT||+TX\nt=2 t\u22121Y\ni=1K\u03b8\ni! Using the two previous lemmas, we obtain the following upper bound on W1(\u00b5\u03b8\nn, \u03c0\u03b8(\u00b7)). Eq(xT|xi\n0)Ep(yT)[||xT\u2212yT||] +TX\nt=2 t\u22121Y\ni=1K\u03b8\ni! ** Using the definition of W1, the trivial coupling, the definitions of \u00b5\u03b8\nnand\u03c0\u03b8(\u00b7), and Lemma 3.4, we get the desired result. LetX\u2286RD. . . Using the above equation, we have\n\u02dc\u00b5q\nt(xt, x0)\u2212\u02dc\u00b5q\nt(yt, x0) =\u221a\u03b1t(1\u2212\u00af\u03b1t\u22121)\n1\u2212\u00af\u03b1t(xt\u2212yt). Moreover, since q(xT|x0) =N(xT;\u221a\u00af\u03b1Tx0,(1\u2212\u00af\u03b1T)I)and the prior p(yT) =N(yT; 0, I),\nEq(xT|x0)Ep(yT)[||xT\u2212yT||]\u2264p\n\u00af\u03b1T||x0||2+ (2\u2212\u00af\u03b1T)D.\n**Special case of the main theorem. . .",
        "Conclusion": "This work successfully circumvents all three of these limitations. Finally, the loss function l\u03b8:X\u00d7X\u2192Ris defined as\nl\u03b8(xT, x0) =Ep\u03b8(xT\u22121|xT)Ep\u03b8(xT\u22122|xT\u22121). , xn\n0}. , xn\n0}. **Proof. ** Let T\u22651. The following inequality holds:\nEp\u03b8(xT\u22121|xT)Ep\u03b8(yT\u22121|yT)Ep\u03b8(xT\u22122|xT\u22121)Ep\u03b8(yT\u22122|yT\u22121). ** The following inequality holds:\nW1(\u00b5\u03b8\nn, \u03c0\u03b8(\u00b7))\u22641\nnnX\ni=1 TY\nt=1K\u03b8\nt! **Proof. Hence, x7\u2192\u02dc\u00b5q\nt(x, x0)isK\u2032\nt-Lipschitz continuous with\nK\u2032\nt=\u221a\u03b1t(1\u2212\u00af\u03b1t\u22121)\n1\u2212\u00af\u03b1t. Now, if \u03b1t<1for all 1\u2264t\u2264T, then we have 1\u2212\u00af\u03b1t>1\u2212\u00af\u03b1t\u22121, which implies K\u2032\nt<1for all 1\u2264t\u2264T. , x\n6",
        "label": 5
    },
    {
        "Abstract": "Dual Cross-Attention Learning for Fine-Grained Visual Categorization and\nObject Re-Identification\nHaowei Zhu*, Wenjing Ke*, Dong Li, Ji Liu, Lu Tian, Yi Shan\nAdvanced Micro Devices, Inc., Beijing, China\n{haowei.zhu, wenjing.ke, d.li, lu.tian, yi.shan }@amd.com\nAbstract\nRecently, self-attention mechanisms have shown impres-\nsive performance in various NLP and CV tasks, which can\nhelp capture sequential characteristics and derive global\ninformation. First, we propose\nglobal-local cross-attention (GLCA) to enhance the inter-\nactions between global images and local high-response re-\ngions, which can help reinforce the spatial-wise discrim-\ninative clues for recognition. For general image classification, self-\nattention has been proved to work well for recognizing 2D\nimages by viewing image patches as words and flattening\nthem as sequences [12, 45]. Ac-\ncording to Eq. * means results without side information for fair comparison. Ablation Study\nContributions from Algorithmic Components. Ablation Study on GLCA. Ablation Study on PWCA. Abd-\nnet: Attentive but diverse person re-identification. In AAAI , 2020. In AAAI , 2020. In AAAI ,\n2020.",
        "Methodology": "In this work, we explore how to extend self-\nattention modules to better learn subtle feature embed-\ndings for recognizing fine-grained objects, e.g., different\nbird species or person identities. To this end, we propose\na dual cross-attention learning (DCAL) algorithm to co-\nordinate with self-attention learning. Second, we propose pair-\nwise cross-attention (PWCA) to establish the interactions\nbetween image pairs. PWCA can regularize the attention\nlearning of an image by treating another image as dis-\ntractor and will be removed during inference. We conduct extensive evaluations on fine-\ngrained visual categorization and object re-identification. In this work, we investigate how to extend self-attention\nmodules to better learn subtle feature embeddings for rec-\nognizing fine-grained objects, e.g., different bird species or\nperson identities. Fine-grained recognition is more chal-\nlenging than general image classification owing to the sub-\ntle visual variations among different sub-classes. To address the subtle\nappearance variations, local characteristics are often cap-\ntured by learning spatial attention [15, 34, 40, 60] or explic-\nitly localizing semantic objects / parts [11, 56, 58, 61]. We\nadopt a different way to incorporate local information based\non vision Transformer. To this end, we propose global-local\ncross-attention (GLCA) to enhance the interactions between\nglobal images and local high-response regions. Specifically,\nwe compute the cross-attention between a selected subset\nof query vectors and the entire set of key-value vectors. The intuition is that one can identify the\nsubtle variations by comparing image pairs. Exiting CNN-\nbased methods design dedicated network architectures to\nenable pair-wise feature interaction [16, 69]. Motivated by this, we also employ a pair-wise learning\nscheme to establish the interactions between image pairs. Different from optimizing the feature distance, we propose\npair-wise cross-attention (PWCA) to regularize the atten-\ntion learning of an image by treating another image as dis-\ntractor. Specifically, we compute the cross-attention be-\ntween query of an image and combined key-value from both\nimages. By introducing confusion in key and value vectors,\nthe attention scores are diffused to another image so that\n4692\nthe difficulty of the attention learning of the current image\nincreases. Such regularization allows the network to dis-\ncover more discriminative regions and alleviate overfitting\nto sample-specific features. It is noted that PWCA is only\nused for training and thus does not introduce extra compu-\ntation cost during inference. The proposed two types of cross-attention are easy-to-\nimplement and compatible with self-attention learning. We\nconduct extensive evaluations on both fine-grained visual\ncategorization (FGVC) and object re-identification (Re-ID). Our main contributions can be summarized as follows. (1) We propose global-local cross-attention to enhance the\ninteractions between global images and local high-response\nregions for reinforcing the spatial-wise discriminative clues. (2) We propose pair-wise cross-attention to establish the in-\nteractions between image pairs by regularizing the attention\nlearning. Each encoder / decoder in-\ncludes a multi-head self-attention (MSA) layer and a feed-\nforward network (FFN) layer. A decoder also has an ex-\ntra MSA layer to handle the output of encoder. Besides,\nlayer normalization (LN) and residual connection are used\nin each MSA or FFN layer. For general image classification, iGPT [5]\nfirst uses auto-regressive and BERT [10] objectives for self-\nsupervised pre-training and then fine-tunes for classifica-\ntion tasks. ViT [12] reshapes an image into a sequence\nof flattened fixed-size patches for training Transformer en-\ncoders only. Fine-grained recognition is more challenging than\ngeneral image classification owing to the sutble visual vari-\nations among different sub-classes. In this work, we extend\nself-attention to better recognize fine-grained objects with\ntwo types of cross-attention modules. Prior\nCNN-based methods address this task by mining effective\ninformation from multi-level features [13, 34, 58], adopt-\ning multi-granularity training strategies [13], locating dis-\ncriminative objects or parts [11, 61] and exploring feature\ninteraction in pair-wise learning [16, 69]. Recently, a few\nTransformer-based methods address FGVC by feature fu-\nsion on multi-level Transformer layers [52] and part selec-\ntion [17]. However, they are based on self-attention\nonly while we design two cross-attention modules for learn-\ning. Mainstream Re-ID methods are\nbased on the CNN structure and metric learning [30, 32]. Transformer with self-\nattention structure has recently been applied to Re-ID by in-\ntroducing part tokens [67], shuffling patch embeddings [17],\nand learning disentangled features [24]. Our work differs\nfrom the most related methods [17, 67] in the following\naspects. First, we adopt a different way to encode the lo-\ncal information by GLCA, while [17] does not explicitly\nmine part regions and [67] computes the attention between\na part token and its associated subset of patch embeddings\nby online clustering. Third, [17] requires side information (e.g., camera IDs and\nviewpoint labels) while our method only takes images as\ninput. In gen-\n4693\n(a) Global-Local Cross-Attention (GLCA) (b) Pair-Wise Cross-Attention (PWCA)\nFigure 1. Overview of the proposed two types of cross-attention mechanisms. We stack Lself-attention, Mglobal-local cross-attention,\nTpair-wise cross-attention modules in our network. The output is computed as a weighted sum of value vec-\ntors, where the weight assigned to each value is computed\nby a scaled inner product of the query with the correspond-\ning key. The products are then\nscaled and normalized by a softmax function to obtain N\nattention weights. By packing Nquery vector into a matrix\nQ= [q1;q2;\u00b7\u00b7\u00b7;qN], the output matrix of self-attention\n(SA) can be represented as:\nfSA(Q, K, V ) = softmax (QKT\n\u221a\nd)V=SV (1)\nwhere1\u221a\ndis a scaling factor. Query, key and value matrices\nare computed from the same input embedding X\u2208RN\u00d7D\nwith different linear transformations: Q=XWQ,K=\nXWK,V=XWV, respectively. To encode positional infor-\nmation, fixed / learnable position embeddings are added to\npatch embeddings and then fed to the network. To pre-\ndict the class, an extra class embedding \u02c6CLS\u2208R1\u00d7dis\nprepended to the input embedding Xthroughout the net-\nwork, and finally projected with a linear classifer layer for\nprediction. Thus, the input embeddings as well as query,\nkey and value matrices become (N+ 1)\u00d7dand the self-attention function (Eq. 1) allows to spread information be-\ntween patch and class embeddings. Layer normalization (LN) is put prior\nto each MSA and FFN layer and residual connections are\nused for both layers. Global-Local Cross-Attention\nSelf-attention treats each query equally to compute\nglobal attention scores according to Eq. In other words,\neach local position of image is interacted with all the po-\nsitions in the same manner. For recognizing fine-grained\nobjects, we expect to mine discriminative local information\nto facilitate the learning of subtle features. To this end, we\npropose global-local cross-attention to emphasize the inter-\naction between global images and local high-response re-\ngions. First, we follow attention rollout [1] to calculate the\naccumulated attention scores for i-th block:\n\u02c6Si=\u00afSi\u2297\u00afSi\u22121\u00b7\u00b7\u00b7 \u2297 \u00afS1 (2)\nwhere \u00afS= 0.5S+ 0.5Emeans the re-normalized attention\nweights using an identity matrix Eto consider residual con-\nnections, \u2297means the matrix multiplication operation. In\nthis way, we track down the information propagated from\nthe input layer to a higher layer. Then, we use the aggre-\ngated attention map to mine the high-response regions. 2, the first row of \u02c6Si= [\u02c6si,j](N+1)\u00d7(N+1)\nmeans the accumulated weights of class embedding \u02c6CLS. We select top Rquery vectors from Qithat correspond\nto the top Rhighest responses in the accumulated weights\nof\u02c6CLS to construct a new query matrix Ql, representing\nthe most attentive local embeddings. fGLCA(Ql, Kg, Vg) =softmax (QlKgT\n\u221a\nd)Vg(3)\nIn self-attention (Eq. 1), all the query vectors will be\ninteracted with the key-value vectors. 3), only a subset of query vectors will be interacted with\nthe key-value vectors. However, through es-\ntablishing the interaction between local query and global\nkey-value vectors, we can relate the high-response regions\nwith not only themselves but also with other context out-\nside of them. Figure 1 (a) illustrates the proposed global-\nlocal cross-attention and we use M= 1 GLCA block in\nour method. Moreover, smaller visual differences between\nclasses exist in FGVC and Re-ID compared to large-scale\nclassification tasks. To alleviate the problem, we propose pair-wise cross at-\ntention to establish the interactions between image pairs. PWCA can be viewed as a novel regularization method to\nregularize the attention learning. Specifically, we randomly\nsample two images ( I1,I2) from the same training set to\nconstruct the pair. The query, key and value vectors are sep-\narately computed for both images of a pair. For training I1,\nwe concatenate the key and value matrices of both images,\nand then compute the attention between the query of the\ntarget image and the combined key-value pairs as follows:\nfPWCA(Q1, Kc, Vc) =softmax (Q1KT\nc\u221a\nd)Vc (4)\nwhere Kc= [K1;K2]\u2208R(2N+2)\u00d7dandVc= [V1;V2]\u2208\nR(2N+2)\u00d7d. For a specific query from I1, we compute N+1\nself-attention scores within itself and N+ 1cross-attention\nscores with I2according to Eq. All the 2N+ 2 atten-\ntion scores are normalized by the softmax function together\nand thereby contaminated attention scores for the target im-\nageI1are learned. Optimizing this noisy attention out-\nput increases the difficulty of network training and reducesthe overfitting to sample-specific features. Figure 1 (b) il-\nlustrates the proposed pair-wise cross-attention and we use\nT= 12 PWCA blocks in our method. Note that PWCA\nis only used for training and will be removed for inference\nwithout consuming extra computation cost. For\nFGVC, we use three standard benchmarks for evaluations:\nCUB-200-2011 [47], Stanford Cars [27], FGVC-Aircraft\n[35]. Baselines. In detail, ViT backbones are pre-trained on\nImageNet-21k [9] and DeiT backbones are pre-trained on\nImageNet-1k [9]. Implementation Details. We coordinate the proposed\ntwo types of cross-attention with self-attention in the form\nof multi-task learning. We build L= 12 SA blocks, M= 1\nGLCA blocks and T= 12 PWCA blocks as the overall ar-\nchitecture for training. The PWCA branch shares weights\nwith the SA branch while GLCA does not share weights\nwith SA. We follow [59] to adopt dynamic loss weights\nfor collaborative optimization, avoiding exhausting manual\nhyper-parameter search. The PWCA branch has the same\nGT target as the SA branch since we treat another image as\ndistractor. For FGVC, we resize the original image into 550 \u00d7550\nand randomly crop to 448 \u00d7448 for training. We select input embeddings with top\nR= 10% highest attention responses as local queries. We\napply stochastic depth [21] and use Adam optimizer with\nweight decay of 0.05 for training. The learning rate is ini-\ntialized as lrscaled =5e\u22124\n512\u00d7batchsize and decayed with\na cosine policy. We train the network for 100 epochs with\nbatch size of 16 using the standard cross-entropy loss. We select\ninput embeddings with top R= 30% highest attention re-\nsponses as local queries. We use SGD optimizer with a mo-\nmentum of 0.9 and a weight decay of 1e-4. The batch size\nis set to 64 with 4 images per ID. The learning rate is initial-\nized as 0.008 and decayed with a cosine policy. We train the\n4695\nMethod BackboneAccuracy (%)\nCUB CAR AIR\nRA-CNN [15] VGG19 85.3 92.5 88.4\nMA-CNN [60] VGG19 86.5 92.8 89.9\nMAMC [40] ResNet101 86.5 93.0 -\nPC [14] DenseNet161 86.9 92.9 89.2\nFDL [29] DenseNet161 89.1 94.0 -\nNTS-Net [56] ResNet50 87.5 93.9 91.4\nCross-X [34] ResNet50 87.7 94.6 -\nS3N [11] ResNet50 88.5 94.7 92.8\nMGE-CNN [58] ResNet50 88.5 93.9 -\nDCL [8] ResNet50 87.8 94.5 93.0\nTASN [61] Resnet50 87.9 93.8 -\nPMG [13] ResNet50 89.6 95.1 93.4\nCIN [16] ResNet50 88.1 94.5 92.8\nAPI-Net [69] DenseNet161 90.0 95.3 93.9\nLIO [65] ResNet50 88.0 94.5 92.7\nSPS [22] ResNet50 88.7 94.9 92.7\nCAL [38] ResNet101 90.6 95.5 94.2\nTransFG [17] ViT-Base 91.7 94.8 -\nRAMS-Trans [20] ViT-Base 91.3 - -\nFFVT [52] ViT-Base 91.6 - -\nBaseline DeiT-Tiny 82.1 87.2 84.7\nBaseline + DCAL DeiT-Tiny 84.6 89.4 87.4\nBaseline DeiT-Small 85.8 90.7 88.1\nBaseline + DCAL DeiT-Small 87.6 92.3 90.0\nBaseline DeiT-Base 88.0 92.9 90.3\nBaseline + DCAL DeiT-Base 88.8 93.8 92.6\nBaseline ViT-Base 90.8 92.5 90.0\nBaseline + DCAL ViT-Base 91.4 93.4 91.5\nBaseline R50-ViT-Base 91.3 94.0 92.4\nBaseline + DCAL R50-ViT-Base 92.0 95.3 93.3\nTable 1. network for 120 epochs using the cross-entropy and triplet\nlosses. All of our experiments are conducted on PyTorch with\nNvidia Tesla V100 GPUs. Our method costs 3.8 hours with\nDeiT-Tiny backbone for training using 4 GPUs on CUB,\nand 9.5 hours with ViT-Base for training using 1 GPU on\nMSMT17. During inference, we remove all the PWCA\nmodules and only use the SA and GLCA modules. We add\nclass probabilities output by classifiers of SA and GLCA\nfor prediction for FGVC, and concat two final class tokens\nof SA and GLCA for prediction for Re-ID. A single image\nwith the same input size as training is used for test. Our method achieves competitive performance\ncompared to the prior CNN-based and Transformer-basedmethods. Table 1 also shows our method can\nconsistently improve different vision Transformer baselines\non all the three benchmarks, e.g., surpassing the pure Trans-\nformer (DeiT-Tiny) by 2.2% and the hybrid structure of\nCNN and Transformer (R50-ViT-Base) by 1.3% on Stan-\nford Cars. Our\nmethod performs on par with the recent Transformer vari-\nants on FGVC: TransFG [17], RAMS-Trans [20], FFVT\n[52]. These existing methods also select tokens based on\naggregated attention responses. Differently, they continue\nto model the selected tokens by self-attention while we per-\nform cross-attention between local query and global key-\nvalue vectors. Compared to self-attention in selected to-\nkens, we can relate the high-response regions with not only\nthemselves but also with other context outside of them. Besides, TransFG [17] uses overlapping patches and will\nlargely increase training time and computation overhead,\nwhile we adopt the standard non-overlapping patch split\nmethod. Ex-\nplicit localization methods (e.g, RACNN [15], MA-CNN\n[60], NTS-Net [56], MGE-CNN [58]) utilize attention /\nlocalization sub-network with ranking losses to mine ob-\nject regions. Implicit localization methods (e.g., S3N [11],\nTASN [61]) use class activation map and Gaussian sampling\nto amplify object regions in the original image. Our GLCA\nadopts a different scheme to incorporate the local informa-\ntion with higher performance, e.g., +3.5% over MGE-CNN\non CUB. (2) Pair-wise learning is also applied for FGVC by\ninteracting features (CIN [16], API-Net [69]) or introducing\nconfusion (PC [14], SPS [22]) between image pairs during\ntraining. Our motivation of PWCA is similar to [14,22] but\nwe implement a different regularization method to alleviate\noverfitting. Our method surpasses these related pair-wise\nlearning methods, e.g., +3.9% over CIN and +5.1% over\nPC on CUB. The input size is 256 \u00d7128\nfor pedestrian datasets and 256 \u00d7256 for vehicle datasets. Our\nmethod performs on par with the recent Transformer vari-\nants on Re-ID: DRL-Net [24], AAformer [67], TransReID\n[18]. DRL-Net [24] imposes decorrelation constraints on\nTransformer decoder to disentangle ID relevant and irrel-\nevant features, while we only employ Transformer encoder\nand extend self-attention to cross-attention. Both of existing\nmethods (TransReID [18], AAformer [67]) and our meth-\nods incorporate local information for recognition but adopt\ndifferent manners. TransReID [18] designs a jigsaw patch\nmodule to shuffle the patch embeddings for learning robust\nfeatures. AAformer [67] computes the attention between\na part token and its associated subset of patch embeddings\nby online clustering. Differently, we proposes global-local\ncross-attention to enhance the interactions between global\nimages and local regions. Typical part-based ReID\nmethods include SPReID [26] and PCB [43]. SPReID [26]\nutilizes a parsing model to generate human part masks tocompute reliable part representations, which consumes ex-\ntra computation overhead in segmentation part. Our method does not aim to mine precise ob-\nject parts but establish the interactions between global im-\nages and high-response local regions. Re-\ncent Re-ID methods also introduce pair-wise spatial trans-\nformer to match the holistic and partial image pairs [33]\nor design pair-wise loss to learn fine-grained features for\nrecognition [55]. We use DeiT-Tiny for FGVC and ViT-Base\nfor Re-ID. With either GLCA or PWCA alone, our method\ncan obtain higher performance than the baselines. We note that PWCA will be removed for inference\nso that it does not introduce extra parameters or FLOPs. We\n4697\nMethodCUB-200-2011 VeRi-776 MSMT17\nParams FLOPs Acc Params FLOPs mAP R1 Params FLOPs mAP R1\nBaseline 5.5M 8.6G 82.1 81.6M 41.1G 78.1 96.0 81.6M 20.5G 61.6 81.4\n+ GLCA 6.0M 8.8G 83.1 88.4M 42.4G 79.5 96.5 88.4M 21.3G 63.7 83.0\n+ PWCA 5.5M 8.6G 83.1 81.6M 41.1G 79.2 96.5 81.6M 20.5G 62.8 82.3\nOurs 6.0M 8.8G 84.6 88.4M 42.4G 80.2 96.9 88.4M 21.3G 64.0 83.1\nTable 3. Effect of the proposed two types of cross-attention learning on CUB-200-2011, VeRi-776 and MSMT17. We use DeiT-Tiny for\nCUB, ViT-Base for VeRi-776 and MSMT17 as baselines in this ablation experiment. MethodCUB MSMT17\nAcc mAP\nBaseline 82.1 61.6\n+ PWCA 83.1 62.8\n+ Adding noise in I1 77.3 56.0\n+ Adding noise in label of I1 81.6 60.8\n+I2from noise 82.1 62.1\n+I2from COCO 82.5 62.2\n+I2from intra-class only 81.7 62.2\n+I2from inter-class only 83.0 62.7\n+I2from intra- & inter-class (1:1) 83.0 62.5\nTable 4. uses one GLCA module in our method, which only requires\na small increase of parameters or FLOPs compared to the\nbaseline. It constructs two Transformer branches to\nhandle image tokens of different sizes and uses the class\ntoken from one branch to interact with patch tokens from\nanother branch. We implement this idea using the same\nselected local queries and the same DeiT-Tiny backbone. The cross-token strategy obtains 82.1% accuracy on CUB,\nwhich is worse than our GLCA by 1%. This local self-attention base-\nline obtains 82.6% accuracy on CUB using the DeiT-Tiny\nbackbone, which is also worse than our GLCA (83.1%). We compare PWCA withdifferent regularization strategies in Table 4 by taking I1as\nthe target image. As the extra image I2used in PWCA can be viewed as dis-\ntractor, we also test replacing the key and value embeddings\nofI2with Gaussian noise. Such method performs better\nthan adding image / label noise, but still worse than our\nmethod. Moreover, sampling I2from a different dataset\n(i.e., COCO), sampling intra-class / inter-class pair only,\nor sampling intra-class & inter-class pairs with equal prob-\nability performs worse than PWCA. We assume that the\nrandomly sampled image pairs from the same dataset (i.e.,\nnatural distribution of the dataset) can regularize our cross-\nattention learning well. Moreover, using one GLCA block\nonly introduces small extra Parameters and FLOPs for in-\nference. It implies that adding I2throughout all the encoders\ncan sufficiently regularize the network as our self-attention\nbaseline has L= 12 blocks in total. Note that PWCA is\nonly used for training and will be removed for inference\nwithout consuming extra computation cost. We observe that self-attention tend to highlight the\nmost discriminative regions in the image. Thanks to GLCA,\nour method can reduce misleading attention and encourage\nthe network to discover more discriminative clues for recog-\nnition. Effect on the amount of cross-attention blocks. DeiT-\nTiny is used for CUB and ViT-base ise used for MSMT17. For\nall the backbones and all the datasets, we build the same M= 1\nGLCA block and same T= 12 PWCA blocks in our method. (a) SA vs. GLCA\n(b) SA vs. PWCA\nFigure 3. It is accordance with our expectation\nthat the attention weights will dominate on the target im-\nage as we compute the cross-attention between the query of\ntarget image and the combined key-value vectors (Eq. (a) SA vs. GLCA (b) SA vs. PWCA\nFigure 4. Limitations\nCompared to the self-attention learning baseline, our\nmethod may take longer time for network convergence as\nwe perform joint training of self-attention and the proposed\ntwo types of cross-attention. However, it is noted that fine-grained\nrecognition datasets are much smaller than the large-scale\nimage classification benchmark and thereby our training\ntime in practice is still acceptable. PWCA\ncan establish the interactions between image pairs and can\nbe viewed as a regularization strategy to alleviate over-\nfitting. Our cross-attention design is easy-to-implement\nand compatible to different vision Transformer baselines. We expect that our method can inspire new insights\nfor the self-attention learning regime in Transformer. 3, 7\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. 2\n[5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. In ICML , 2020. In ECCV , pages\n330\u2013346. 5\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 4\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. arXiv preprint\narXiv:1810.04805 , 2018. 1, 2\n[11] Yao Ding, Yanzhao Zhou, Yi Zhu, Qixiang Ye, and Jianbin\nJiao. Selective sparse sampling for fine-grained image recog-\nnition. 1, 2, 5\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. Fine-grained\nvisual classification via progressive multi-granularity train-\ning of jigsaw patches. 2, 5\n[14] Abhimanyu Dubey, Otkrist Gupta, Pei Guo, Ramesh Raskar,\nRyan Farrell, and Nikhil Naik. 1, 2, 5\n[17] Ju He, Jie-Neng Chen, Shuai Liu, Adam Kortylewski,\nCheng Yang, Yutong Bai, Changhu Wang, and Alan Yuille. Deep networks with stochastic depth. Stochas-\ntic partial swap: Enhanced model generalization and inter-\npretability for fine-grained recognition. Learning disentangled representation implicitly via trans-\nformer for occluded person re-identification. Human semantic\nparsing for person re-identification. 6\n[27] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. In\nICCV workshop , 2013. 4\n[28] Hanjun Li, Gaojie Wu, and Wei-Shi Zheng. Combined depth\nspace based architecture search for person re-identification. 6\n[29] Chuanbin Liu, Hongtao Xie, Zheng-Jun Zha, Lingfeng Ma,\nLingyun Yu, and Yongdong Zhang. End-to-end comparative attention networks\nfor person re-identification. In ACM MM , pages\n907\u2013915, 2020. Bag of tricks and a strong baseline for deep person\nre-identification. Stnreid:\nDeep convolutional networks with pairwise spatial trans-\nformer networks for partial person re-identification. TMM ,\n22(11):2905\u20132913, 2020. 6\n[34] Wei Luo, Xitong Yang, Xianjie Mo, Yuheng Lu, Larry S\nDavis, Jun Li, Jian Yang, and Ser-Nam Lim. 6\n[37] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. arXiv preprint\narXiv:1906.05909 , 2019. 5, 6\n[39] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Beyond part models: Person retrieval with refined\npart pooling (and a strong convolutional baseline). Beyond part models: Person retrieval with refined\npart pooling (and a strong convolutional baseline). In ECCV ,\npages 480\u2013496, 2018. 2\n[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv \u00b4e J\u00b4egou. 1, 2[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. arXiv preprint\narXiv:1706.03762 , 2017. 1, 2\n[47] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge Belongie. Learning discriminative features with multiple granu-\nlarities for person re-identification. In ACM MM , pages 274\u2013\n282, 2018. Deep coattention-based com-\nparator for relative representation learning in person re-\nidentification. Beyond triplet loss: person\nre-identification with fine-grained difference-aware pairwise\nloss. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. Learning a mixture of granularity-specific experts for fine-\ngrained categorization. Learn-\ning multi-attention convolutional neural network for fine-\ngrained image recognition. 1, 2, 5\n[62] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. 4\n[63] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. 5\n[66] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\nWang. Identity-guided human semantic parsing for person\nre-identification. In ECCV , pages 346\u2013363. Learning attentive\npairwise interaction for fine-grained classification.",
        "Results and Findings": "Experiments demonstrate that DCAL performs on par with\nstate-of-the-art methods and consistently improves multiple\nself-attention baselines, e.g., surpassing DeiT-Tiny and ViT-\nBase by 2.8% and 2.4% mAP on MSMT17, respectively. Experiments demonstrate that DCAL performs on par with\nstate-of-the-art methods and consistently improves multiple\nself-attention baselines. Particularly, for FGVC, DCAL im-\nproves DeiT-Tiny by 2.5% and reaches 92.0% top-1 accu-\nracy with the larger R50-ViT-Base backbone on CUB-200-\n2011. For Re-ID, DCAL improves DeiT-Tiny and ViT-Base\nby 2.8% and 2.4% mAP on MSMT17, respectively. (3) The proposed dual cross-attention learning can\ncomplement the self-attention learning and achieves consis-\ntent performance improvements over multiple vision Trans-\nformer baselines on various FGVC and Re-ID benchmarks. Transformer carrying forward this mecha-\nnism has dominated in various sequence-to-sequence NLP\ntasks [10, 46]. Recent work has applied Trans-\nformers to various vision tasks (e.g., image classification\n[5,12,19,37,45,51], object detection [2,41,44,68], seman-\ntic segmentation [23, 39, 50, 51, 63] and low-level tasks [4])\nand shown competitive performance compared to the state-\nof-the-art CNNs. 2.3. Revisit Self-Attention\n[46] originally proposes the self-attention mechanism to\naddress NLP tasks by calculating the correlation between\neach word and all the other words in the sentence. See Section 3 for details. Specifically, a query q\u2208R1\u00d7dis first matched\nagainst Nkey vectors ( K= [k1;k2;\u00b7\u00b7\u00b7;kN], where each\nki\u2208R1\u00d7d) using inner product. To jointly attend to information from different repre-\nsentation subspaces at different positions, multi-head self-\nattention (MSA) is defined by considering multiple atten-\ntion heads. 3.2. We conduct extensive experiments on two\nfine-grained recognition tasks: fine-grained visual catego-\nrization (FGVC) and object re-identification (Re-ID). We use multiple architectures of DeiT-\nT/16, DeiT-S/16, DeiT-B/16, ViT-B/16, R50-ViT-B/16 with\nL= 12 SA blocks for evaluation. Performance comparisons in terms of top-1 accuracy on\nthree standard FGVC benchmarks: CUB-200-2011, Stanford Cars\nand FGVC-Aircraft. 4.2. Results on Fine-Grained Visual Categorization\nWe evaluate our method on three standard FGVC bench-\nmarks and compare with the state-of-the-art approaches in\nTable 1. Particularly, with the R50-ViT-Base backbone,\nDCAL reaches 92.0%, 95.3% and 93.3% top-1 accuracy on\nCUB-200-2011, Stanford Cars and FGVC-Aircraft bench-\nmarks, respectively. The results validate the compatibility of our\nmethod to different Transformer architectures. (1) Existing\nregion-based methods can be divided to two categories. Results on Object Re-ID\nWe evaluate our method on four standard Re-ID bench-\nmarks in Table 2 and achieve competitive performance com-\npared to the state-of-the-art methods on both Person Re-ID\nand Vehicle Re-ID tasks. Particularly, with the ViT-Base\nbackbone, DCAL reaches 80.2%, 64.0%, 87.5%, 80.1%\nmAP on VeRi-776, MSMT17, Market1501, DukeMTMC,\nrespectively. Similar to FGVC, our method can consis-\ntently improve different vision Transformer baselines, e.g.,\nsurpassing the light-weight Transformer (DeiT-Tiny) by\n4696\nMethodVeRi-776 MSMT17 Market1501 DukeMTMC\nmAP (%) R1 (%) mAP (%) R1 (%) mAP (%) R1 (%) mAP (%) R1 (%)\nSPReID [26] - - - - 83.4 93.7 73.3 86.0\nPCB [43] - - - - 81.6 93.8 69.2 83.3\nMGN [49] - - 52.1 76.9 86.9 95.7 78.4 88.7\nSAN [25] 72.5 93.3 55.7 79.2 88.0 96.1 75.7 87.9\nABDNet [6] - - 60.8 82.3 88.3 95.6 78.6 89.0\nHOReID [48] - - - - 84.9 94.2 75.6 86.9\nISP [66] - - - - 88.6 95.3 80.0 89.6\nSTNReID [33] - - - - 84.9 93.8 - -\nCDNet [28] - - 54.7 78.9 86.0 95.1 76.8 88.6\nFIDI [55] 77.6 95.7 - - 86.8 94.5 77.5 88.1\nSPAN [7] 68.9 94.0 - - - - - -\nPVEN [36] 79.5 95.6 - - - - - -\nCAL (ResNet50) [38] 74.3 95.4 56.2 79.5 87.0 94.5 76.4 87.2\nDRL-Net [24] - - 55.3 78.4 86.9 94.7 76.6 88.1\nAAformer [67] - - 63.2 83.6 87.7 95.4 80.0 90.1\nTransReID* (ViT-Base) [18] 79.2 96.9 63.6 82.5 - - - -\nDeiT-Tiny 71.3 94.3 42.1 63.9 77.9 90.3 69.5 82.9\nDeiT-Tiny + DCAL (Ours) 74.1 94.7 44.9 68.2 79.8 91.8 71.7 84.9\nDeiT-Small 76.7 95.5 53.3 75.0 84.3 93.7 75.7 87.6\nDeiT-Small + DCAL (Ours) 78.1 95.9 55.1 77.3 85.3 94.0 77.4 87.9\nDeiT-Base 78.3 95.9 60.5 81.6 86.6 94.4 79.1 88.7\nDeiT-Base + DCAL (Ours) 80.0 96.5 62.3 83.1 87.2 94.5 80.2 89.6\nViT-Base 78.1 96.0 61.6 81.4 87.1 94.3 78.9 89.4\nViT-Base + DCAL (Ours) 80.2 96.9 64.0 83.1 87.5 94.7 80.1 89.0\nTable 2. Performance comparisons on four Re-ID benchmarks: VeRi-776, MSMT17, Market1501, DukeMTMC. 2.8% and the larger Transformer (ViT-Base) by 2.4% on\nMSMT17. With\nboth cross-attention modules, we can further improve the\nresults. (3)\nWe conduct more ablation experiments to examine the ef-\nfect of GLCA. We obtain 82.6% accuracy on CUB by se-\nlecting local query randomly and obtain 82.8% by select-\ning local query based on the penultimate layer only. Our\nGLCA outperforms both baselines, validating that mining\nhigh-response local query with aggregated attention map is\neffective for our cross-attention learning. The results show that adding image noise\nor label noise without cross-attention causes degraded per-\nformance compared to the self-attention learning baseline. Figure 2 presents\nthe ablation experiments on the amount of our cross-\nattention blocks using DeiT-Tiny for CUB and ViT-Base\nfor MSMT17. For GLCA, the results show that M= 1\nperforms best. We analyze that the deeper Transformer\nencoder can produce more accurate accumulated attention\nscores as the attention flow is propagated from the input\nlayer to higher layer. For PWCA, the results show that T= 12 performs\nbest. 4.5. We\n4698\nFigure 2. observe that PWCA can diffuse the attention responses to\nexplore more complementary parts of objects compared to\nself-attention. We also test removing both GLCA and PWCA blocks\nfor maintaining the same computation cost with the self-\nattention baseline, and the performance slightly drops, e.g,\n84.3% vs. 84.6% (Ours) accuracy on CUB and 80.1% vs.\n80.2% (Ours) mAP on VeRi-776. 4699\nReferences\n[1] Samira Abnar and Willem Zuidema. arXiv preprint arXiv:2005.00928 ,\n2020. arXiv preprint arXiv:2103.14899 , 2021. 7\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. In\nCVPR , 2021. 1, 2\n[6] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\nChen, Yang Yang, Zhou Ren, and Zhangyang Wang. In ICCV ,\npages 8351\u20138361, 2019. Springer, 2020. 6\n[8] Yue Chen, Yalong Bai, Wei Zhang, and Tao Mei. arXiv preprint\narXiv:2010.11929 , 2020. 1, 2\n[13] Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang\nXie, Zhanyu Ma, Yi-Zhe Song, and Jun Guo. In ECCV , 2018. 5\n[15] Jianlong Fu, Heliang Zheng, and Tao Mei. 1, 5[16] Yu Gao, Xintong Han, Xun Wang, Weilin Huang, and\nMatthew Scott. arXiv preprint arXiv:2103.07976 , 2021. 2, 5\n[18] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. arXiv preprint arXiv:2102.04378 , 2021. 6\n[19] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. 1,\n2\n[20] Yunqing Hu, Xuan Jin, Yin Zhang, Haiwen Hong, Jingfeng\nZhang, Yuan He, and Hui Xue. 5\n[21] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Springer, 2016. 4\n[22] Shaoli Huang, Xinchao Wang, and Dacheng Tao. 5\n[23] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. 2\n[24] Mengxi Jia, Xinhua Cheng, Shijian Lu, and Jian Zhang. arXiv preprint\narXiv:2107.02380 , 2021. 2, 6\n[25] Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and\nZhibo Chen. In AAAI , volume 34, pages 11173\u2013\n11180, 2020. 3d object representations for fine-grained categorization. 5\n[30] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and\nShuicheng Yan. TIP, 26(7):3492\u20133506, 2017. 2\n[31] Xinchen Liu, Wu Liu, Jinkai Zheng, Chenggang Yan, and\nTao Mei. 2\n[32] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei\nJiang. In CVPR Workshop , 2019. 2\n[33] Hao Luo, Wei Jiang, Xing Fan, and Chi Zhang. Cross-x learn-\ning for fine-grained visual categorization. 1,\n2, 5\n[35] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. arXiv preprint arXiv:1306.5151 , 2013. In CVPR , pages 7103\u20137112,\n2020. 1, 2\n[38] Yongming Rao, Guangyi Chen, Jiwen Lu, and Jie Zhou. 2\n[40] Ming Sun, Yuchen Yuan, Feng Zhou, and Errui Ding. In ECCV , 2018. 1, 5\n[41] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\nChanghu Wang, et al. 2\n[42] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. In ECCV ,\n2018. 2\n[43] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. 6\n[44] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. arXiv preprint arXiv:2011.10881 , 2020. arXiv preprint arXiv:2012.12877 , 2020. The caltech-ucsd birds-200-2011\ndataset. In CVPR ,\npages 6449\u20136458, 2020. 6\n[49] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\nZhou. 2, 6\n[50] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. 2\n[51] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. In ECCV ,\n2020. 1, 2\n[52] Jun Wang, Xiaohan Yu, and Yongsheng Gao. arXiv preprint arXiv:2107.02341 , 2021. 2, 5\n[53] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. 4\n[54] Lin Wu, Yang Wang, Junbin Gao, Meng Wang, Zheng-\nJun Zha, and Dacheng Tao. IEEE T NEUR NET LEAR , 32(2):722\u2013735,\n2020. 4\n[55] Cheng Yan, Guansong Pang, Xiao Bai, Changhong Liu, Ning\nXin, Lin Gu, and Jun Zhou. 6\n[56] Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao,\nand Liwei Wang. In ECCV , 2018. 1, 5\n[57] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. arXiv preprint arXiv:2101.11986 , 2021. 2\n[58] Lianbo Zhang, Shaoli Huang, Wei Liu, and Dacheng Tao. 1, 2, 5\n[59] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\nand Wenyu Liu. IJCV , pages 1\u201319,\n2021. 4\n[60] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. 1, 5\n[61] Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo. In\nCVPR , 2019. In ICCV , pages 1116\u20131124, 2015. 2\n[64] Zhedong Zheng, Tao Ruan, Yunchao Wei, Yi Yang, and Tao\nMei. 4\n[65] Mohan Zhou, Yalong Bai, Wei Zhang, Tiejun Zhao, and Tao\nMei. Springer, 2020. 6\n[67] Kuan Zhu, Haiyun Guo, Shiliang Zhang, Yaowei Wang,\nGaopan Huang, Honglin Qiao, Jing Liu, Jinqiao Wang, and\nMing Tang. arXiv preprint arXiv:2104.00921 , 2021. 2,\n6\n[68] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. 1, 2\n[69] Peiqin Zhuang, Yali Wang, and Yu Qiao. 1, 2, 5\n4702",
        "Conclusion": "We observe\nthat DCAL can reduce misleading attentions and diffuse the\nattention response to discover more complementary parts\nfor recognition. 1. 2. 2.2. 3. The final output is the weighted sum\nofNvalue vectors ( V= [v1;v2;\u00b7\u00b7\u00b7;vN], where each\nvi\u2208R1\u00d7d). 1. Finally, we compute\n4694\nthe cross attention between the selected local query and the\nglobal set of key-value pairs as below. We observe that GLCA can help\nreinforce the spatial-wise discriminative clues to promote\nrecognition of fine-grained classes. 3.3. 4. 4. 4.3. 4.4. 4). 5. Conclusion\nIn this work, we introduce two types of cross-attention\nmechanisms to better learn subtle feature embeddings for\nrecognizing fine-grained objects. Extensive experiments on seven benchmarks have demon-\nstrated the effectiveness of our method on FGVC and Re-ID\ntasks. End-to-\nend object detection with transformers. In ECCV , 2020. InCVPR , 2019. In CVPR , 2009. In ICCV , 2019. In ECCV , 2020. In CVPR , 2017. In ICCV , 2019. In ICCV , 2019. In CVPR , 2018. In ICCV , 2019. In CVPR , 2021. In CVPR , 2021. 2011. In CVPR , 2021. In CVPR , 2018. TMM , 2021. In ICCV , 2019. In ICCV , 2017. In CVPR , 2021. TMM , 2020. In CVPR , 2020. In ICLR , 2021.",
        "label": 1
    },
    {
        "Abstract": "SimAN: Exploring Self-Supervised Representation Learning of Scene Text\nvia Similarity-Aware Normalization\nCanjie Luo1, Lianwen Jin1,2,*, Jingdong Chen3\n1South China University of Technology,2Peng Cheng Laboratory,3Ant Group\n{canjie.luo, lianwen.jin }@gmail.com, jingdongchen.cjd@antgroup.com\nAbstract\nRecently self-supervised representation learning has\ndrawn considerable attention from the scene text recogni-\ntion community. Making the best use of unlabeled data be-\ncomes a crucial topic. For\ninstance, the pretext task is to predict relative rotation [31]\nand position [56]. the content patch IcasIaugto make its style\ndifferent from the style patch Is. Since it can make the best use of abun-\ndant unlabeled data, it has important practical significance. In our\npractice, we find that our approach is sufficient for cross-\nlanguage editing, as shown in Figure 7. 2017A030312006). Anal. CoRR ,\nabs/1601.07140, 2016.",
        "Methodology": "Different from previous studies using con-\ntrastive learning, we tackle the issue from an alternative\nperspective, i.e., by formulating the representation learning\nscheme in a generative manner. Moti-\nvated by this common sense, we augment one image patch\nand use its neighboring patch as guidance to recover itself. Specifically, we propose a Sim ilarity-A ware N ormalization\n(SimAN) module to identify the different patterns and align\nthe corresponding styles from the guiding patch. In this\nway, the network gains representation capability for distin-\nguishing complex patterns such as messy strokes and clut-\ntered backgrounds. How-\never, the supervised learning methods heavily rely on\nlabor-intensive and expensive annotations. It\nhas been shown that the self-supervised representations can\nbenefit subsequent supervised tasks [6\u201310, 18]. f(\u00b7)ContentStyleAugmentationSimilarity-Aware Norm.f(\u00b7)ContentStyle-AlignedContentDecoderRepresentationSpace(a) Contrastive representation learning\n(b) Generative representation learning (ours) RecoveredImageSeqCLRAugmentation\nf(\u00b7)f(\u00b7)f(\u00b7)\nRepresentationSpaceProjectionHeadFigure 1. We estimate the similarity\nof the content representations between the augmented patch and\nits neighboring patch, and align the corresponding styles to recon-\nstruct the augmented patch. Only high-quality representations are\ndistinguishable so that a precise reconstruction can be achieved. Directly adopt-\ning current non-sequential contrastive learning schemes for\nsequence-like characters [44] usually leads to performance\ndeterioration [1]. This suggests the gap between the non-\nsequential and sequential schemes. Although it brought a huge leap forward, the\nrepresentation learning of scene text remains a challenging\nopen research problem, where the nature of scene text has\nnot been fully explored. Specifically, we randomly crop two neighboring im-\nage patches from one text line. One patch is augmented and\nthe other one guides the recovery of the augmented one. Thus, we propose a\nSimilarity-A ware N ormalization (SimAN) module to align\ncorresponding styles from the neighboring patch by esti-\nmating the similarity of the content representations between\nthese two patches. This means that the representations are\nrequired to be sufficiently distinguishable so that different\npatterns can be identified and the corresponding styles can\nbe correctly aligned. Only in this way, the network can\nproduce a precise recovered image patch. Therefore, the\nproposed SimAN enables high-quality self-supervised rep-\nresentation learning in a generative way. To summarize, our contributions are as follows:\n\u2022 We propose a generative (opposite of contrastive [34])\nrepresentation learning scheme by utilizing the unique\nproperties of scene text, which might inspire rethink-\ning the learning of better representations for sequential\ndata like text images. \u2022 We propose a SimAN module, which estimates the\nsimilarity of the representations between the aug-mented image patch and its neighboring patch to align\ncorresponding styles. Only if the representations are\nsufficiently distinguishable, different patterns can be\nidentified and be aligned with correct styles. Moreover, the self-supervised net-\nwork shows impressive capabilities to synthesize data,\nedit text images and interpolate fonts, suggesting the\nbroad practical applications of the proposed approach. However, data collection\nand annotation cost a lot of resources. Although synthetic data are available, previous\nstudies [26,33,37,61] suggested that there is a gap between\nreal and synthetic data. Furthermore, the similarity measuring task using only posi-\ntive pairs and discarding negative samples [9, 16] is also an\n1040\nAlignKQVInstance Norm\nInstance NormLocal Statistic\nRandomCrop\nAugment\nEncoder\nDecoder\nDiscriminatorTrue/FalseAdversaryLossL2Loss\nDataFlowSupervisionFlowTrainableModuleOperationFeatureMapsLossSoftMaxIsIcIaugIcIrec\nSimilarity-AwareNormalizationFigure 2. We decouple content and style as two different inputs and\nguide the network to recover the augmented image. The proposed SimAN module learns to align corresponding styles for different patterns\naccording to the distinguishable representations. [3] introduced ex-\nisting self-supervised techniques [18, 31] to use unlabeled\ndata but resulted in approximately the same performance. [1] proposed a contrastive representation\nlearning scheme, termed SeqCLR, to satisfy the sequence-\nto-sequence structure of scene text recognition. Although it is possible to\nmodel the image pixel by pixel [50], this theoretically re-\nquires much more high-performance clusters [6]. We propose a novel representation learning scheme by\nstudying the unique properties of scene text and using an\nimage reconstruction pretext task. Methodology\nIn this section, we first introduce the design of the pretext\ntask and the construction of the training samples. Then, we\ndetail the proposed SimAN module. We enable the scene text\nrepresentation learning by recovering an augmented image\npatch using its neighboring patch as guidance. This ensures sufficient differences in\ncontent between the two patches. Even if the neighboring\npatches might contain a same characters, their positions are\ndifferent. Then, we augment (blurring, random noise, color\nchanges, etc.) Therefore, we avoid all of the spatial\ntransformation augmentation methods that are widely used\nfor self-supervised representation learning. This is also a\nsignificant difference with previous study SeqCLR [1]. The IN(\u00b7)is compute as\nIN(x) =x\u2212\u00b5(x)p\n\u03c3(x)2+\u03f5, (2)\nwhere \u00b5(\u00b7)and\u03c3(\u00b7)respectively compute the mean and\nstandard deviation, performing independently for each\nchannel and each sample. For the local style representations, we extract eight-\nneighborhood mean and standard deviation at position (i, j)\non the c-th channel of the feature maps as\n\u00b5c,i,j=1\n9X\np,q\u2208Ni,jxc,p,q, (3)\n\u03c3c,i,j=1\n3rX\np,q\u2208Ni,j(xc,p,q\u2212\u00b5c,i,j)2, (4)\nwhere Ni,jis the position set comprising of the eight-\nneighborhood around the position (i, j)and itself. Then the statistics \u00b5and\u03c3is adaptively rearranged ac-\ncording to the similarity between the patterns of the two in-\nputs by (here K,Q,\u00b5and\u03c3are reshaped to RCF\u00d7HFWF)\n\u00b5\u2032=\u00b5Softmax\u0012KTQ\u221adk\u0013\n, \u03c3\u2032=\u03c3Softmax\u0012KTQ\u221adk\u0013\n,(5)\nwhere dkis the dimension of the input K. The \u00b5\u2032and\u03c3\u2032are\nreshaped to RCF\u00d7HF\u00d7WF. If the\nencoder produces meaningless content or style representa-\ntions, the decoder cannot correctly recover the source im-\nage. We min-\nimize the distance between the recovered image Irecand\ntarget image Icas\nL2=\u2225Irec\u2212Ic\u22252\n2. (8)\nSimultaneously, we adopt a widely used adversarial ob-\njective to minimize the distribution shift between the gener-\nated and real data:\nmin\nDLadv=E\u0002\u0000\nD(Is)\u22121\u00012] +E[\u0000\nD(Irec)\u00012\u0003\n, (9)\nmin\nEncoder, DecoderLadv=E\u0002\n(D(Irec)\u22121\u00012\u0003\n, (10)\nwhere Ddenotes a discriminator. The encoder/decoder and discriminator are alternately opti-\nmized to achieve adversarial training. Experiments\nIn this section, we conduct extensive experiments to val-\nidate the effectiveness of the proposed approach. First, we\ncompare the quality of the learned representations with that\nof the previous study SeqCLR [1]. Then, we study the per-\nformance of our approach by using a semi-supervised set-\nting, where we pre-train the encoder using unlabeled data\nand fine-tune it using partially labeled data. For instance, we attempt to synthesize diverse\ndata to train a robust recognizer. 1042\nRNNsBackbonePre-processing\n\u201cJewelers\u201dCTC/Attention(Rectification)(ResNet-29)(2*BiLSTM)InputImageNormalizedImageFigure 3. We construct a dataset for self-supervised representa-\ntion learning. To obtain more realistic and diverse scene\ntext images, we collect samples from public real train-\ning datasets, including IIIT5K [39], IC13 [28], IC15 [27],\nCOCO-Text [51], RCTW [46], ArT [11], ReCTS [60],\nMTWI [19], LSVT [47] and MLT [40]. We discard low-\nresolution images with a height of less than 32 pixels or\nwidth of less than 64 pixels (the width should be greater\nthan two times the height for constructing training sam-\nples). Because in practice, low-quality images confuse\nthe image recovery task and lead to inefficient training. As a result, we discard their labels and obtain an unla-\nbeled dataset composed of approximately 300k real sam-\nples, termed Real-300K1. Besides, we also use the popular\nsynthetic dataset SynthText [17] for fair comparisons with\nthe previous study SeqCLR [1]. We symmetrically de-\nsign a lightweight decoder. Optimization In the self-supervised representation\nlearning stage, we set the batch size to 256 and train the\nnetwork for 400K iterations. It takes less than 3 days for\nconvergence on two NVIDIA P100 GPUs (16GB memory\nper GPU). The learning rate is set to\n10\u22124and linearly decreased to 10\u22125. The images are re-\nsized to a height of 32 pixels, maintaining the aspect ratio. Specifically, we\nperform self-supervised pre-training of the ResNet-29 back-\nbone using SynthText [17]. Then we fix the parameters\nof the backbone and feed the frozen representations to a\nCTC/Attention probe. The probes are trained on the same\nlabeled SynthText dataset. It is believed that the higher the\nrepresentation quality, the better the probe can obtain cues\nfor classification. and word-level accuracy up to one edit distance (E.D. Note that our genera-\ntive scheme is significantly different from the contrastive\nscheme SeqCLR [1], which uses sufficient sequential mod-\neling (RNN projection head and sequential mapping) in\nthe self-supervised pre-training phase. Note that\nthe sequential modeling (2*RNN) in the encoder reduces\nthe quality of representations. This is because our approach\nmodels local patterns for recovery, but the sequential mod-\neling introduces contexts to disturb this learning scheme. This means our approach might lack the capacity\nof sequence modeling after self-supervised representation\nlearning. However, it is possible to equip a lightweight\nRNN in the probe, which remarkably improves the repre-\nsentation quality. Overall, we obtain promising representa-\ntions in a generative manner. In fact, we usually encounter one situation that we\nhave vast amounts of unlabeled real-world data. It is worth\nmaking the best use of the real-world data. Therefore, we\nconduct an experiment under this new setting to further ver-\nify the effectiveness of our approach. We perform self-\nsupervised learning of the backbone using the Real-300K\ndataset. As the real-world dataset provides\nmore realistic and diverse images, it benefits the robustness\nof the backbone. Probe evaluation. We report the word-level accuracy (Acc., %) and accuracy up to\none edit distance (E.D. Method EncoderDecode Block Probe IIIT5K IC03 IC13\n(Train) (Test) Acc. We discard the spatial trans-\nformation augmentations because our ap-\nproach recovers images based on consis-\ntent visual cues. Probe evaluation. We report the word accuracy (Acc., %)\nand word-level accuracy up to one edit distance (E.D. The\nreal training data provides more robust representations. Probe Training Data IIIT5K IC03 IC13\nType Encoder Probe Acc. Discussion Here we reveal two significant differences\nbetween the contrastive learning scheme SeqCLR and our\ngenerative learning scheme SimAN. As our SimAN recov-\ners an image according to the consistent visual cues, we\ndo not introduce spatial transformation augmentations into\nour pipeline. This means that our approach is more suitable\nfor scene text images, rather than handwritten text images\n(focusing on stroke deformations) in black and white. This is because our approach models\nlocal patterns for recovery, while the SeqCLR requires con-\ntextual information within the sequence for discrimination. There exist different properties of the two schemes. Semi-Supervision Evaluation\nWe further study the performance under a semi-\nsupervision manner. As SynthText provides six million training samples, it isable to sample smaller subsets with three orders of scales\n(10K, 100K, and 1M from the original 6M data). After\nperforming self-supervised pre-training of the backbone on\nSynthText, we use the pre-trained parameters to initialize\nthe recognizer backbone. As shown in Table 4, our approach using the semi-\nsupervised setting outperforms the supervised baseline. With the increase of the scale of labeled\ndata, our approach can still contribute to recognition ac-\ncuracy. Note that our approach can still slightly improve recogni-\ntion performance using the whole SynthText for fine-tuning,\nwhereas the SeqCLR shows inconsistent performance. This\nindicates the generalization ability of our approach. For the generalization to several\ndifferent tasks, we adopt a widely used VGG encoder and a\ncorresponding decoder [22, 25] in our model. Then the generative network can synthesize an im-\nage in a similar style retaining the required content. Semi-supervised performance evaluation. We sample three orders of scales (10K, 100K, and 1M) of data from SynthText (6M). Our approach can learn high-quality representations from unlabeled data and improve the supervised baseline, especially when used with\nlow-resource labeled data. Method SupervisionIIIT5K IC03 IC13\nLabeled Training Data Labeled Training Data Labeled Training Data\n10K 100K 1M 6M 10K 100K 1M 6M 10K 100K 1M 6M\nSeqCLR [1]Sup. Thus, it is pos-\nsible to introduce more background noise by adding variant\nsketches extracted by the Canny edge detection operator on\nImageNet samples [32]. EncoderDecoderSimAN\nContentInput\uff08Iaug\uff09ImageNet SampleCannyStyleReference\uff08Is\uff09\nFigure 4. We show two distributions of (a) 200 real labeled\nsamples and (b) 200 real samples and our 2000 synthetic samples. The large empty space of original distribution might suggest the\nlack of diversity of labeled data. After adding our synthetic sam-\nples, the distribution is more even and dense. However, our synthetic data fills\nthe empty style space, indicating the significantly enriched\nstyles. Therefore, we simply add blurring to the\nsamples. The recognition performance is further boosted,\nsuggesting that our synthesis pipeline is scalable. We\ngenerate 10K images using the corpus of SynthText [17]\nand the style of IC13 [28]. Then we evaluate the style dis-\ntribution similarity using the FID score [20] and the read-\nability using a mainstream recognizer3[44]. That means the editing is limited to ap-\n2https://github.com/youdao-ai/SRNet\n3https://github.com/meijieru/crnn.pytorch\n1045\nproximately the same length words. On the contrary, our approach can adap-\ntively align correct styles to arbitrary-length text, indicat-\ning the flexibility of our self-supervised approach. We report FID\nscore and word-level recognition accuracy (%). Although the su-\npervised EditText can imitate more font category and background\ntexture, our self-supervised approach achieves better readability. We present an interesting\napplication of our approach on font interpolation for auto-\nmatically and efficiently generating font candidates. For instance, we compute the style representations\n(local statistics) of two images and rearrange them accord-\ning to the same content representations. We interpolate the\ntwo style representations to decode images so that we can\nobtain the gradually changing colors, sheens, and shadows,\nas shown in Figure 8. Simultaneously, we interpolate the\ncontent representations to achieve font glyph changes. Cross language editing via our self-supervised approach. Font interpolation effects produced by our approach. One promising technological solution is to detect the\nedited/attacking image using a qualified discriminator. It takes advantage\nof the style consistency of neighboring patches among one\ntext image to reconstruct one augmented patch under the\nguidance of its neighboring patch. Specifically, we propose\na SimAN module to identify different patterns ( e.g., back-\nground noise and foreground characters) based on the repre-\nsentation similarity between the two patches. The represen-\ntations are required to be sufficiently distinguishable so that\ncorresponding styles can be correctly aligned to reconstruct\nthe augmented patch. Our study might arouse the\nrethinking of self-supervised learning of scene text. In the\nfuture, we will study the complementarity of contrastive and\ngenerative learning schemes to further improve the repre-\nsentation quality. Acknowledgment\nThis research was supported in part by NSFC (Grant No. 61936003) and GD-NSF (No. [3] Jeonghun Baek, Yusuke Matsui, and Kiyoharu Aizawa. What if we only use real datasets for scene text recognition? [4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and\ntranslate. [5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. In ICML , pages 1691\u20131703, 2020. [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML , pages 1597\u20131607, 2020. [8] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\nNorouzi, and Geoffrey Hinton. Big self-supervised models\nare strong semi-supervised learners. In CVPR , pages 15750\u201315758, 2021. [13] Boris Epshtein, Eyal Ofek, and Yonatan Wexler. Detecting\ntext in natural scenes with stroke width transform. Read like humans: Au-\ntonomous, bidirectional and iterative language modeling for\nscene text recognition. Connectionist temporal classification:\nlabelling unsegmented sequence data with recurrent neural\nnetworks. In ICML , pages 369\u2013376, 2006. [16] Jean-Bastien Grill, Florian Strub, Florent Altch \u00b4e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,Bernardo Pires, Zhaohan Guo, Mohammad Azar, et al. Synthetic data for text localisation in natural images. [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium. Robust scene text\ndetection with convolution neural network induced MSER\ntrees. In ECCV , pages 497\u2013511, 2014. [23] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-\ndrew Zisserman. Mach. [25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. [26] Lei Kang, Marc \u00b8al Rusinol, Alicia Forn \u00b4es, Pau Riba, and\nMauricio Villegas. Unsupervised writer adaptation for\nsynthetic-to-real handwritten word recognition. [31] Nikos Komodakis and Spyros Gidaris. Unsupervised repre-\nsentation learning by predicting image rotations. [32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Separating content from style using adver-\nsarial learning for recognizing text in the wild. J. Com-\nput. In BMVC ,\npages 1\u201311, 2012. [41] Luk \u00b4a\u02c7s Neumann and Ji \u02c7r\u00b4\u0131 Matas. In CVPR , pages 3538\u20133545, 2012. A robust arbitrary text detection\nsystem for natural scene images. Mach. , 39(11):2298\u20132304, 2017. Mach. [48] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. [50] Aaron Van Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. In\nICML , pages 1747\u20131756, 2016. [51] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas,\nand Serge Belongie. [52] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In ICML , pages 1096\u2013\n1103, 2008. [54] Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo, Xi-\naoxue Chen, Yaqiang Wu, Qianying Wang, and Mingxiang\nCai. ACM\nTrans. Detecting texts of arbitrary orientations in natural images. In\nCVPR , pages 1083\u20131090, 2012. In ECCV , pages 249\u2013266, 2018.",
        "Results and Findings": "Typically, the neighbor-\ning image patches among one text line tend to have simi-\nlar styles, including the strokes, textures, colors, etc. Experiments show that the proposed\nSimAN significantly improves the representation quality and\nachieves promising performance. Moreover, we surpris-\ningly find that our self-supervised generative network has\nimpressive potential for data synthesis, text image editing,\nand font interpolation, which suggests that the proposed\nSimAN has a wide range of practical applications. As a scene text image containing dense characters is\nsignificantly different from a natural image, SeqCLR [1]\n1039\ndivided one text line into several instances using certain\nstrategies and performed contrastive learning on these in-\nstances. The\nSeqCLR designed for sequence-to-sequence visual recogni-\ntion outperformed the representative non-sequential method\nSimCLR [7]. Simultaneously, it is ob-\nserved that color similarity typically occurs across one text\nline. As one text line usually exhibits consistent styles, includ-\ning the strokes, textures, colors, etc., the original styles of\nthe augmented patch can be found on the neighboring patch\naccording to similar content patterns. Moreover, we\nfind that our self-supervised network has competitive per-\nformance with state-of-the-art scene text synthesis meth-\nods [17, 23, 35, 59]. [61] and Kang et al. We sum-\nmarize popular methods into two main categories according\nto their objectives as follows. One reason for this may\nbe that the raw image signal is in a continuous and high-\ndimensional space, unlike the natural language sentences in\na discrete space ( e.g., words or phrases) [18]. This design\nconsiders the unique properties of scene text, i.e., the styles\n(e.g., stroke width, textures, and colors) within one text line\ntend to be consistent. As shown in Figure 2, given an unlabeled text image\nI\u2208R3\u00d7H\u00d7W(the width Wis required to be larger than\ntwo times of height H), we randomly crop two neighbor-\ning image patches Is, Ic\u2208R3\u00d7H\u00d7Has style and content\ninput, respectively. The excessively distorted strokes are also diverse from the\nsource font style. 1041\n3.2. Based on this finding, we perform instance normal-\nization (IN) [22,48] on the feature maps to remove the style\nand obtain content representations as key ( K, from Is) and\nquery ( Q, from Iaug) as\nK= IN\u0000\nEncoder( Is)\u0001\n, Q= IN\u0000\nEncoder( Iaug)\u0001\n, (1)\nwhere the KandQare normalized feature maps with spa-\ntial scale RCF\u00d7HF\u00d7WF. Algorithm 1 Representation Learning Scheme\nInput: Encoder ,Decoder , Discriminator D\nOutput: Encoder ,Decoder\n1:foriteration t = 0, 1, 2, ..., T do\n2: Sample a mini-batch {Ii}B\ni=1from unlabeled data\n3: foreachIido\n4: Randomly crop IsandIc, augment IcasIaug\n5: Forward Encoder ,SimAN andDecoder\n6: Compute loss for {Irec,i}B\ni=1\n7: Update Dusing min\nDLadv\n8: Update Encoder andDecoder using\nmin\nEncoder, DecoderLadv+\u03bbL2\n9: (The\u03bbis empirically set to 10.) Moreover, we compare our\nself-supervised model with mainstream supervised models\non the text image editing task. Dataset\nWe evaluate our approach on several public benchmarks\nthat are widely used in scene text recognition studies. 4.2. The quantized results, including word accuracy (Acc.) 1) [1], are reported in Table 1. Although the direct\ncomparisons between the two approaches are somewhat un-\nreasonable, we list SeqCLR\u2019s results under a similar exper-\nimental setting for reference. Here we analyze the results of our approach. Moreover, we find that this experimental setting (pre-\ntraining the backbone and fine-tuning the probe using the\nvery same synthetic dataset) might not meet the actual prac-\ntice. Although we cannot perform direct comparisons with SeqCLR,\nwe list its results for reference. The \u201cProj.\u201d, \u201cSeq. E.D. 1 Acc. E.D. 1 Acc. E.D. 1\nSeqCLR [1] ResNet + 2*RNN Proj. CTC 35.7 62.0 43.6 71.2 43.5 67.9\nOursResNet + 2*RNN FCN CTC 0.0 2.8 0.0 0.0 0.0 6.4\nResNet FCN CTC 1.5 7.9 2.3 5.2 2.2 12.9\nResNet FCN 1*RNN + CTC 57.4 75.1 64.8 78.9 63.0 81.2\nResNet FCN 2*RNN + CTC 60.8 75.6 64.9 78.9 64.0 81.0\nSeqCLR [1] ResNet + 2*RNN Proj. Att. 49.2 68.6 63.9 79.6 59.3 77.1\nOursResNet + 2*RNN FCN Att. 6.4 12.8 6.8 9.9 7.1 15.1\nResNet FCN Att. 22.2 39.7 22.3 38.6 24.1 43.6\nResNet FCN 1*RNN + Att. 65.0 78.3 73.6 85.9 71.8 84.3\nResNet FCN 2*RNN + Att. 66.5 78.8 71.7 83.6 68.7 81.6Table 2. \u2713 \u00d7\nPiecewise Affine \u2713 \u00d7\nTable 3. E.D. 1 Acc. E.D. 1 Acc. E.D. 68.9 82.8 75.0 87.2 72.9 86.0\nAtt.Synth. 73.7 85.6 81.2 90.4 77.9 87.8\nachieves better results might be the closer distribution to the\nbenchmarks, which are also real-world datasets. On\nthe contrary, the SeqCLR shows more promising results on\nhandwritten text than scene text. 2) We find that adding a se-\nquence model in the encoder yields degraded performance\nof our approach, whereas it provides noteworthy improve-\nments for SeqCLR. For\ninstance, under the 10K low-resource setting, our approach\nincreases the accuracy by more than 5%, which suggests\nthat the recognition robustness is highly correlated with rep-\nresentation quality. We compare the semi-supervised results with the\nprevious study termed SeqCLR [1] under the same setting. 4.5. The image height is set to 64 pixels. As shown in Figure 4, we replace the Iswith a style\nreference image and replace the Iaugwith a new content\ninput. Here the\n1044\nTable 4. - - - 82.9 \u21930.9 - - - 92.2\u21911.1 - - - 87.9 \u21930.2\nOursSup. 35.0 72.6 84.1 86.6 37.6 79.4 88.2 91.5 38.6 75.3 86.4 89.0\nSemi-Sup. We can synthesize similar\nstyle images containing new text strings. (a)(b)\nFigure 5. Best viewed in color.Table 5. For each column, the best result is highlighted in bold font,\nand the second-best result is shown with an underline . Method IIIT5K SVT IC15 SVT-P CT80 TText\nSynth90K [23] 51.6 39.2 35.7 37.2 30.9 30.5\nSynthText [17] 53.5 30.3 38.4 29.5 31.2 31.1\nVerisimilar Synthesis [59] 53.9 37.1 37.1 36.3 30.5 30.9\nUnrealText [35] 54.8 40.3 39.1 39.6 31.6 32.1\nOurs (high res., 64 \u00d7) 62.3 51.2 35.0 36.6 44.8 37.9\nOurs (blurred) 65.7 58.6 38.7 44.2 47.9 38.3\nFirst, we visualize the distributions of the limited real\nlabeled samples and our plentiful synthetic samples. As\nshown in Figure 5, the limited labeled real-world data can-\nnot cover diverse styles. Then, we conduct recognition experiments to show\nthe quantitative results. Following the settings of Unreal-\nText [35], we synthesize 1M samples to train the same rec-\nognizer and report the accuracy on several benchmarks. As\nshown in the second last row in Table 5, our samples out-\nperform previous synthesis methods [17, 23, 35, 59] on four\n(out of six) benchmarks without bells and whistles. We find\nthat our synthetic samples have a high resolution (height of\n64 pixels), which usually cannot meet the low-quality prac-\ntice of scene text. As our approach can synthesize new words within source\nstyles, we study the performance of our self-supervised ap-\nproach and a popular supervised method EditText2[57]. Table 6. As we\nparameterize the style and content as representations, we\ncan interpolate these representations to achieve transitional\neffects. It can be employed to tamper sensi-\ntive data, such as personal information, license plate num-\nbers, and financial statistics, to trick systems that rely on\nEnglishChineseKoreanArabicSourceImageGeneratedImageFigure 7. Extensive experiments show that our generative ap-\nproach achieves promising representation quality and out-\nperforms the previous contrastive method. 1046\nReferences\n[1] Aviad Aberdam, Ron Litman, Shahar Tsiper, Oron Anschel,\nRon Slossberg, Shai Mazor, R Manmatha, and Pietro Perona. In CVPR , pages 15302\u201315312, 2021. [2] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park,\nDongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwal-\nsuk Lee. In ICCV , pages\n4715\u20134723, 2019. In CVPR ,\npages 3113\u20133122, 2021. In ICLR , 2015. NeurIPS , 33:22243\u2013\n22255, 2020. [9] Xinlei Chen and Kaiming He. [10] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-\ncal study of training self-supervised vision transformers. In\nICCV , pages 9640\u20139649, 2021. [11] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng,\nCanjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang,\nJunyu Han, Errui Ding, et al. In ICDAR , pages 1571\u2013\n1576, 2019. [12] Chee-Kheng Ch\u2019ng, Chee Seng Chan, and Cheng-Lin Liu. J. Doc. , 23(1):31\u201352, 2020. In CVPR ,\npages 2963\u20132970, 2010. [14] Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong\nMao, and Yongdong Zhang. In CVPR , pages 7098\u20137107, 2021. [15] Alex Graves, Santiago Fern \u00b4andez, Faustino Gomez, and\nJ\u00a8urgen Schmidhuber. In NeurIPS , volume 1, pages 1\u20131, 2020. [17] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. [18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. In CVPR , pages 9729\u20139738, 2020. [19] Mengchao He, Yuliang Liu, Zhibo Yang, Sheng Zhang, Can-\njie Luo, Feiyu Gao, Qi Zheng, Yongpan Wang, Xin Zhang,\nand Lianwen Jin. In ICPR , pages 7\u201312, 2018. In NeurIPS , pages 6626\u20136637, 2017. [21] Weilin Huang, Yu Qiao, and Xiaoou Tang. [22] Xun Huang and Serge Belongie. In ICCV ,\npages 1501\u20131510, 2017. Reading text in the wild with convolutional\nneural networks. J. Comput. , 116(1):1\u201320, 2016. [24] Longlong Jing and Yingli Tian. IEEE\nTrans. , 43(11):4037\u20134058, 2021. In\nECCV , pages 694\u2013711, 2016. In WACV ,\npages 3502\u20133511, 2020. [27] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos\nNicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-\nmura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-\ndrasekhar, Shijian Lu, et al. In ICDAR , pages 1156\u20131160, 2015. [28] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,\nMasakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles\nMestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-\nmazan, and Lluis Pere De Las Heras. ICDAR 2013 robust\nreading competition. In ICDAR , pages 1484\u20131493, 2013. [29] Tero Karras, Samuli Laine, and Timo Aila. In\nCVPR , pages 4401\u20134410, 2019. [30] Diederik P Kingma and Jimmy Ba. In ICLR , 2015. In ICLR ,\n2018. NeurIPS , 25:1097\u20131105, 2012. 1047\n[33] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. In AAAI , volume 33, pages 8610\u20138617,\n2019. [34] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu\nWang, Jing Zhang, and Jie Tang. Data Eng. [35] Shangbang Long and Cong Yao. In CVPR ,\npages 5488\u20135497, 2020. [36] Simon M Lucas, Alex Panaretos, Luis Sosa, Anthony Tang,\nShirley Wong, and Robert Young. ICDAR 2003 robust read-\ning competitions. In ICDAR , pages 682\u2013687, 2003. [37] Canjie Luo, Qingxiang Lin, Yuliang Liu, Lianwen Jin, and\nChunhua Shen. , 129(4):960\u2013976, 2021. [38] Canjie Luo, Yuanzhi Zhu, Lianwen Jin, and Yongpan Wang. In CVPR , pages 13746\u201313755,\n2020. [39] Anand Mishra, Karteek Alahari, and CV Jawahar. [40] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowd-\nhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Uma-\npada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. In ICDAR , pages 1582\u20131587,\n2019. [42] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan\nTian, and Chew Lim Tan. Recognizing text with perspec-\ntive distortion in natural scenes. In ICCV , pages 569\u2013576,\n2013. [43] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng\nChan, and Chew Lim Tan. [45] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan\nLyu, Cong Yao, and Xiang Bai. , 41(9):2035\u20132048, 2018. In ICDAR , volume 1, pages 1429\u20131434, 2017. [47] Yipeng Sun, Jiaming Liu, Wei Liu, Junyu Han, Errui Ding,\nand Jingtuo Liu. In ICCV ,\npages 9086\u20139095, 2019. CoRR , abs/1607.08022, 2016. [49] Aaron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. InNeurIPS , pages 6309\u20136318, 2017. Pixel recurrent neural networks. [53] Kai Wang, Boris Babenko, and Serge Belongie. In ICCV , pages 1457\u20131464, 2011. In\nAAAI , volume 34, pages 12216\u201312224, 2020. [55] Yizhi Wang and Zhouhui Lian. DeepVecFont: Synthesizing\nhigh-quality vector fonts via dual-modality learning. [56] Chen Wei, Lingxi Xie, Xutong Ren, Yingda Xia, Chi Su,\nJiaying Liu, Qi Tian, and Alan L Yuille. [57] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jing-\ntuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild. InACM Int. [58] Cong Yao, Xiang Bai, Wenyu Liu, Yi Ma, and Zhuowen Tu. [59] Fangneng Zhan, Shijian Lu, and Chuhui Xue. [60] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li,\nKai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun\nYang, et al. ICDAR 2019 robust reading challenge on read-\ning chinese text on signboard. In ICDAR , pages 1577\u20131581,\n2019. [61] Yaping Zhang, Shuai Nie, Wenju Liu, Xing Xu, Dongxiang\nZhang, and Heng Tao Shen. In\nCVPR , pages 2740\u20132749, 2019.",
        "Conclusion": "1. \u2022 The proposed SimAN achieves promising representa-\ntion performance. 2. 2.2. Aberdam et al. 3. Finally, we present the\nobjectives of the task and the complete learning scheme. Finally, the pretext task\ntakes Iaugas content input and Isas the style guidance to\nrecover an image Irec. Finally, we perform a reverse process of IN(\u00b7)to align\nrearranged styles to each position for image recovery as\nQ\u2032\nc,i,j=Qc,i,j\u03c3\u2032\nc,i,j+\u00b5\u2032\nc,i,j, (6)\nIrec= Decoder( Q\u2032). 4. Finally, we\nshow the potential of our generative approach for other vi-\nsual tasks. 4.1. Therefore, we discard the sequential modeling in the en-\ncoder. 1, %). + Seq. + Seq. 1, %). 1\nCTCSynth. Synth. Synth. 4.4. Finally, we fine-tune the entire\nrecognizer using different subsets of SynthText. - - - 83.8 - - - 91.1 - - - 88.1\nSemi-Sup. Thus, a clean canvas containing a\nslanted/curved text can be finally rendered as abundant di-\nverse scene text images. 5. However, we point out the risks\nof text image editing. 6. Conclusion\nWe have presented a novel approach for self-supervised\nrepresentation learning of scene text from a brand new per-\nspective, i.e., in a generative manner. In\nNeurIPS , volume 33, pages 9912\u20139924, 2020. Int. Recogn. Int. Vis. Intell. IEEE Trans. Knowl. ,\n1(1):1\u20131, 2021. Int. Vis. An end-to-end\ntrainable neural network for image-based sequence recogni-\ntion and its application to scene text recognition. IEEE Trans. Intell. IEEE Trans. Intell. End-to-end\nscene text recognition. Graph , 1(1), 2021. Conf. 1048",
        "label": 1
    },
    {
        "Abstract": "Abstract\nWe propose a method to interactively control the anima-\ntion of fluid elements in still images to generate cinema-\ngraphs. However,\nforward warping (also known as splatting) has its challenges\n(i)it can map multiple source pixels to the same destination\nresulting in loss of information and aliasing, (ii)it may not\nmap any source pixel to a particular target location leading\nto blank region. method is automatic, unlike the other two methods. arXiv\npreprint arXiv:1906.07889 , 2019.",
        "Methodology": "Our approach takes in input image along with the user-provided motion hints (red arrows in (a) and the user-provided mask\n(white in (b)) indicating the regions of fluid elements to be animated and outputs the sequence of frames of the animated videos. Taking in-\nspiration from prior works, we represent the motion of such\nfluid elements in the image in the form of a constant 2D op-\ntical flow map. To this end, we allow the user to provide\nany number of arrow directions and their associated speeds\nalong with a mask of the regions the user wants to animate. The user-provided input arrow directions, their correspond-\ning speed values, and the mask are then converted into a\ndense flow map representing a constant optical flow map\n(FD). We observe that FD, obtained using simple exponen-\ntial operations can closely approximate the plausible mo-\ntion of elements in the image. We further refine computed\ndense optical flow map FDusing a generative-adversarial\nnetwork (GAN) to obtain a more realistic flow map. We\ndevise a novel UNet based architecture to autoregressively\ngenerate future frames using the refined optical flow map by\nforward-warping the input image features at different res-\nolutions. In addition, we show the qualitative animations of\nthe objects in directions that did not exist in the training\nset and provide a way to synthesize videos that otherwise\nwould not exist in the real world. However, the required animations\nor videos are less easily available for the users to leverage\nthan still images that exist in abundance in one\u2019s collection. Hence, it is desirable to empower the practitioners with con-\ntrollable tools to convert the still images to videos of the\nrequired kind. Similar to [11] we focus on the images that contain\nfluid elements like water, smoke, fire that have repeating\ntextures and continuous fluid motion. While these methods provide some control to the user,\nthey suffer from certain drawbacks. al [10]\nproposed a method in which the user can provide sparse tra-\njectories as input, defined by the direction of the motion at\ndifferent locations. A dense optical flow map is estimated\nin an unsupervised manner and is warped with the input im-\nage to obtain the future frames. To circumvent the problems associated with directly obtain-\ning a flow map from user inputs, we propose a two-step ap-\nproach to estimate the flow map from a sparse set of arrow\ndirections and their associated speeds. Firstly, we approxi-\nmate the dense optical flow using simple exponential oper-\nations on the movement directions and speeds input by the\nuser. Next, the thus estimated approximate flow map is fur-\nther refined using a GAN-based network [8] to obtain the\nfinal estimate of the flow map representing the constant 2D\nflow map of the desired movement. The estimated flow map\nalong with the input image is fed into a GAN-based image\ngenerator, similar to Holynski et al. The contributions of our paper are as follows. \u2022 We propose a two-stage approach to interactively con-\ntrol the animation of fluid elements from a still image. \u2022 We propose a novel approach to approximate the con-\nstant flow map governing the motion using simple ex-\nponential operations on the user-provided inputs in the\nform of speed and directions. [30] propose a two-stage approach to synthe-\nsize a video from a single image, wherein in the first stage\na sequence of frames is generated using a 3D-GAN and in\nthe second stage, the sequence of frames are further refined\nusing another GAN. [14] first predict a sequence\nof optical flow maps for future frames from the input image\nand then use them to obtain the future RGB frames. [15] propose a radically different approach\nby modeling the sequence of landscape frames in a video\nin the StyleGAN [12] latent space while enforcing the tem-\nporal consistency. Similar to [14], Holynski et al. [11] first\nestimates the optical flow for future frames, except that the\nwork assumes a constant 2D flow map across the video. These methods [4, 11, 14, 14, 15, 30] generate a video from\na single still image automatically and thus do not allow the\nuser interaction to control the animation. [5] propose a one-\nto-one mapping between image and video using a residual\nrepresentation, that allows the user to provide a single di-\nrection of motion for video generation. [1] and [2] propose\nmethods that govern the animation of different parts in the\nimage with a single poke at a particular location defined by\nthe start and end location of the motion. This approach allows for user interaction through\nsparse trajectories for every frame to be predicted. Given\nthe sparse trajectory for a particular frame, a single network\nis used to obtain a dense optical flow map and a halluci-\nnated image. The dense optical map is bilinearly warped\nwith the input image to obtain an estimate for the frame that\nis further added to the hallucinated image to obtain the final\npredicted frame. We differ from this method in two ways,\ni) instead of obtaining the dense optical flow directly from\nsparse trajectories, we first obtain its approximation by ap-\n3668\nplying simple exponential functions on the user inputs, and\nthen refining it using a network. and ii) instead of obtaining\nthe final image by simple bilinear interpolation on the input\nimage, we adapt the method from [11] and use a separate\nnetwork that takes in the input image and flow map corre-\nsponding to a particular frame, with symmetric splatting of\nintermediate features to obtain the RGB frame. Methodology\nGiven a single input image I\u2208RH\u00d7W\u00d73, mask of\nthe regions in the image the user wants to animate M\u2208\nRH\u00d7W\u00d71,Karrow directions A1..,Kand corresponding\nspeed values S1..,K, our goal is to generate a realistic an-\nimated video comprising of Nframes ( I1..,N). Our method\nconsists of first converting the arrow directions and speed\nvalues to sparse input flow maps, FS\u2208RH\u00d7W\u00d72(Section\n3.2), where flow at position i(at the location of arrows in\nFS) defines the position where a pixel at that location will\nmove to in all future frames. We propose to convert FSinto\na dense flow map FD(Section 3.3) using simple exponential\nfunctions. Using FDand input image Ias guidance, we use\na flow-refinement network to generate a dense refined flow\nmapFR\nD(Section 3.4). Following this principle, Endo et al. [6]\npredicts optical flow autoregressively using the previously\ngenerated frame. Although theoretically, this seems feasi-\nble, in the long-term it leads to large distortion due to error\npropagation. Since we also operate in the domain of fluid elements,\nwe adopt the principle proposed in [11] to use a constant\noptical flow field to model the motion of elements in the\ngenerated frames. Baseline: Sparse Input Flow FS\u2192Sparse Re-\nfined Flow FR\nS\nWe convert the arrow directions A1..,Kand correspond-\ning speed S1..,Kinto sparse optical flow map FS. Each\narrow An(where n\u2208[0, K]) at location iin the image,\nis given by the start (i)and end (j)location of the arrow\n(xi, yi)\u2192(xj, yj). [10], who generate time varying dense\nflow maps from input sparse trajectories, we transform the\nsparse flow map FSto a dense optical flow map FR\nS. How-\never, unlike their method that rely on generation of time-\nvarying flow maps, we generate a constant (Eulerian) dense\noptical flow FR\nS. For this we use a SPADE [19] based flow-\nrefinement network GFby using FS, input image and mask\nas cues, in the SPADE normalization layers. Sparse Input Flow FS\u2192Dense Flow FD\nFrom fig. 3, it can be observed that the refined dense op-\ntical flows FR\nSgenerated using FSare very different from\nground-truth Eulerian flows. We hypothesize that FSdoes\nnot provide adequate information to GFto produce realistic\nflows. Instead of directly using FSto generate dense flow\nusing GF, we propose to create an intermediate dense op-\nticalFDfrom FSusing simple functions of the distances\nbetween every pixel location and the arrow positions. We\ncalculate the exponential L2Euclidean distance (Dexp)be-\ntween each pixel location in input image and the starting\ncoordinate of all the K arrows. The inputs to our system are the input image, the user-provided mask indicating the region\nto be animated, and motion hints, FS. The motion hint is converted into a dense flow map FDusing simple exponential operations on FS,\nwhich is further refined using a SPADE network, GFto obtain FR\nD. During test time, instead of using both I0andIN, we obtain the tth\nframe, Itas the output of the UNet into which we feed the input image in place of both I0andINand the Euler integrated flow maps\ncorresponding to that frame, in both forward and backward directions that are used to perform symmetric splatting in deep feature space. Di,j\nexpand is given by,\nFD(i) =\uf8f1\n\uf8f2\n\uf8f3PK\nj=1Di,j\nexp\u2217FS(i)\nPN\nj=1Di,j\nexpifi\u2208M\n0 ifi /\u2208M(6)\n3.4. While the dense flow is a\ngood approximation, the refined dense flow is closer to the\nground-truth flow than the approximated flow. This is due to the\nfact the FDis generated purely based on the closeness of a\nparticular pixel location to hint points with simple exponen-\ntial operations. Hence, using our flow-refinement network GF, with the\ninput image, mask, and FDas cues, we generate a dense re-\nfined optical flow FR\nD. Using FR\nDas Eulerian flow fields, we\ngenerate video frames by the method discussed in Section\n3.5. Video Frame Generation\nFrom the refined dense optical flow field FR\nD, we esti-\nmate the flow fields F0\u2192tfrom input image to all the fu-\nture frames for t\u2208[0, N]using equation 2. To mitigate these artifacts, we use softmax-\nsplatting (proposed in [17]). The refined\nsparse flow obtained is quite poor. tribution of source pixels mapping to the same destination\nlocation, based on importance metric Z\u2208RH\u00d7W\u00d71. In our\nmethod, we predict Zas an additional channel in the output\nofGFduring fine-tuning (Section 3.5.3). Using a UNet based image gener-\natorGI, we extract the image features at different resolu-\ntions from the UNet encoder. We use softmax-splatting to\nwarp the features at different scales and generate an image\nusing the decoder of the UNet. Note that all the splatted fea-\ntures from the encoder part, except at the bottleneck layer\nof UNet, are connected to the decoder via skip connections. We hypothesize that GImight not be\nable to generate appropriate pixel values for these regions\nto fill in the gaps. To resolve this artifact we use the method\nof symmetric-splatting proposed in [11]. In this method,\nsimilar to producing flow fields in the forward direction\nF0\u2192t, we also generate flow fields in the backward direc-\ntionF0\u2192\u2212tby Euler-integration of \u2212MF. Thus, instead of\njust using softmax-splatting on deep features Dr\n0obtained\nfrom first frame I0withF0\u2192tto generate Dr\nt, we use a\ncombination of the deep features Dr\n0from I0andDr\nNob-\ntained from last frame IN, splatted with F0\u2192tandF0\u2192t\u2212N\nrespectively. 3.5.3 Training and Inference\nAs proposed by [17], for stable training, we first train the\n2 components of dense optical flow refinement and frame\ngenerator separately. While training GF, we use the stan-\ndard GAN loss and Discriminator Feature-matching loss\n[19]. In this stage, we compute losses based on generated\ndense refined optical flow (and not the generated Z). Dur-\ning the training of GI, we use the standard GAN loss, VGG\nloss [22], L1 loss, and Discriminator Feature-matching loss. Prior to end-to-end fine-tuning, we freeze the refined dense\noptical flow maps and only train GFto generate Z. In addi-\ntion, we only use the discriminator for the frame generator\npart. Contrary to training where we use both the first and\nthe last frames for symmetric-splatting used in GI, at test\ntime, since we only have a single input static image, use the\nsame image as both the first and the last frames in GI. 3671\nInput Image Ground-T ruth Flow1 Hint\nRefined Sparse Flow Refined Dense Flow5 Hints\n(a)\n(c)(b)\n(d)\nRefined Sparse Flow Refined Dense FlowFigure 4. The figure shows the comparison of the refined sparse flow and the refined dense flow for one hint and five hints. It is quite\nevident that across diverse set of hints, the refined dense flow resembles the ground-truth flow far more closely that refined sparse flow. This dataset already\ncontains the ground-truth videos, starting frame, and the\naverage optical flow for all videos. For our purpose of training and testing, in addi-\ntion to the input image, and average flow, we also require\na mask of regions the user wants to annotate along with ar-\nrows and corresponding speed values. We generate a sub-\nstitute of the user annotated mask and arrows with the mask\nand flow hint points generated heuristically from ground-\ntruth average optical flow. Mask Generation: For every average optical flow map\nFavgin the dataset, we calculate the mean-squared flow\nvalue for Favg. Then we mask out all the regions having\nper-pixel squared-flow less than mtimes the mean-squared\nflow value. Flow Hint Generation: Using the mask Mgenerated in\nthe previous step, we calculate masked average flow FM\navg\nfor each video as M\u2217Favg, where Favgis the ground-truth\naverage flow map for that video. We perform k-means clus-\ntering for 100 iterations on the individual FM\navg, to find the\ncluster centers based on the number of desired hint pointsfor our input sparse optical flow map FS.FSconsists of\nvalues equal to the ground-truth average flow maps at the\npixel location of cluster centers and zero elsewhere. In Section 3.2, we discuss the procedure to\nconvert user-provided arrows and speed values to FS, which\nis required in real-world interactive testing. Experimental Setup\nFor flow refinement network GF, we use SPADE [19]. We train the flow refinement part of our method\nseparately for 200 epochs with both generator and discrim-\ninator learning rates of 2\u00d710\u22123with TTUR method of up-\ndating learning rate proposed in [19]. We train on a triplet\nof (first frame, FS, ground-truth average flow), where FSis\nrandomly selected to have 1, 3, or 5 flow hints. For frame\ngeneration network GI, we use a modification of UNet\n(shown in fig. 2) which incorporates symmetric-splatting. We use the same multi-scale discriminator DIfrom [19]\nduring training. We train the frame generation part sep-\narately for 200 epochs on training tuples of (start frame,\nground-truth average flow, middle frame, last frame), where\nthe middle frame is selected randomly from time [1,59]. Both generator and discriminator learning rates are set to\n2\u00d710\u22123with the TTUR method of updating the learning\nrate. During fine-tuning we only use GF,GIandDI. Ad-\nditionally, we fix the value to GFthat is responsible for\nflow generation and only keep Ztrainable. Both generator\nand discriminator learning rates are reduced to 1\u00d710\u22123. We train for 40 epochs. Prior to training, we resize all the\naverage flows and frames to 288x512 (maintaining the16\n9\n3672\nInput Image\nDense Flow\nRefined Dense FlowGround-T ruth Flow\nGenerated Frames w/ Dense Flow\nGenerated Frames w/ Refined Dense FlowFigure 5. The figure shows the dense flow and the refined dense\nflow for a particular example and the corresponding generated\nframes of the video. [10], and the\nvanilla Eulerian method [11]. Our own baseline is comput-\ning the dense flow map using simple exponential functions\nfollowed by an image generator. provide an op-\ntimization procedure to compute the directions and speed\nduring test time. For Hao et al., we repurpose the method\nthey have provided in their paper by making the following\nmodifications. Instead of obtaining sparse trajectories, we\nuse the sparse hints that were obtained using the procedure\noutlined earlier in the section. Instead of having a single\nnetwork to convert the sparse user inputs, we first compute\ndense flow map directly from the sparse hints by training\nthe same procedure, i.e the GAN-based network. ), we use the same\nimage generator that is used for our method to generate theMethod FVD\u2193 PSNR \u2191\n1 HintEndo et al. The table shows the FVD and PSNR values for var-\nious methods that use different number of hints as well as the\nVanilla Eulerian method for the original speed. frames from the computed dense optical flow and the in-\nput image. In addition, we also compare our results\nwith the vanilla Eulerian method which is fully automatic\nand does not require any user inputs. Metrics\nIn order to evaluate our method against the various base-\nlines, we use the following metrics. Prior to obtaining the features,\nwe resize all videos to 224\u00d7224and use 60frames. To\nobtain the features from the videos, we use the pre-trained\nI3D [23] model that was trained on Kinematics dataset [13]. PSNR: While FVD assesses the perceptual quality of the\ngenerated videos, we assess the mean pixel accuracy using\nPSNR. Given that it is based on mean square error, PSNR\ntends to favor those methods that produce somewhat blurry\nresults. It is clear that our\nmethod that involves flow refinement network on dense flow\nis able to generate better flow maps than just sparse hints\nwith refinement network, in terms of PSNR. This trend is\nconsistent across a different number of hints. It is interest-\ning that for the vanilla Eulerian method, the PSNR is lower\nthan both methods. This shows the robustness of our flow generation\nmethod to arbitrary input directions for flow hints. modi-\nfied\u2019 and more closely resemble the ground-truth flows. We also observe that our method (Our\n(FD+ frame generator)) is better than \u2018Hao et al. modified\u2019,\nthus showing the need to approximate the dense flow map. It is also observed that the FVD scores for all methods get\nprogressively better with an increasing number of hints. This is directly reflected in the quality of the anima-Method PSNR \u2191\n1 HintHao et al. The table shows the average PSNR between the ground-\ntruth flows and the two refined flows. Animation in arbitrary directions: Our method is capa-\nble of generating flow maps from flow hints that correspond\nto any arbitrary directions that may not have existed in the\ntraining set. Figure 6 shows the refined dense flows gen-\nerated using the same input image with different arrow di-\nrections, and thus different dense flows. One of the limitations of our method is that it is restricted\nto the movements of fluid elements in an image. Another potential limitation of our method is its inability to\nmodel multiple flow streams that are adjacent to each other\nbut may belong to different objects. 3674\nReferences\n[1] Andreas Blattmann, Timo Milbich, Michael Dorkenwald,\nand Bjorn Ommer. 1, 2\n[2] Andreas Blattmann, Timo Milbich, Michael Dorkenwald,\nand Bjorn Ommer. Im-\nproved conditional vrnns for video prediction. 2\n[5] Michael Dorkenwald, Timo Milbich, Andreas Blattmann,\nRobin Rombach, Konstantinos G Derpanis, and Bjorn Om-\nmer. 1,\n2\n[6] Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. 2, 3, 4, 7\n[7] Jean-Yves Franceschi, Edouard Delasalles, Micka \u00a8el Chen,\nSylvain Lamprier, and Patrick Gallinari. 2\n[9] Tavi Halperin, Hanit Hakim, Orestis Vantzos, Gershon\nHochman, Netai Benaim, Lior Sassy, Michael Kupchik, Ofir\nBibi, and Ohad Fried. 1, 2, 3, 4, 5, 6, 7\n[12] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. 2[13] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. Flow-grounded spatial-temporal\nvideo prediction from still images. 2\n[15] Elizaveta Logacheva, Roman Suvorov, Oleg Khomenko, An-\nton Mashikhin, and Victor Lempitsky. 2\n[16] Matthias Minderer, Chen Sun, Ruben Villegas, Forrester\nCole, Kevin Murphy, and Honglak Lee. Unsupervised learn-\ning of object structure and dynamics from videos. Softmax splatting for video\nframe interpolation. 2\n[19] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. 3, 5, 6\n[20] Fitsum A Reda, Guilin Liu, Kevin J Shih, Robert Kirby, Jon\nBarker, David Tarjan, Andrew Tao, and Bryan Catanzaro. 5\n[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. 7\n[24] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. 7\n[25] Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru\nErhan, Quoc V Le, and Honglak Lee. High fidelity video\nprediction with large stochastic recurrent neural networks. Decomposing motion and con-\ntent for natural video sequence prediction. arXiv preprint\narXiv:1706.08033 , 2017. 2\n[27] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu,\nJan Kautz, and Bryan Catanzaro. 2\n[28] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,\nAndrew Tao, Jan Kautz, and Bryan Catanzaro. Learning to generate time-lapse videos using multi-stage dy-\nnamic generative adversarial networks. 2\n[31] Jiangning Zhang, Chao Xu, Liang Liu, Mengmeng Wang,\nXia Wu, Yong Liu, and Yunliang Jiang.",
        "Results and Findings": "We conduct extensive experiments on a publicly\navailable dataset and show that our method is superior to\nthe baselines in terms of qualitative and quantitative met-rics. Studies show that video-based\nads and explainers are far more likely to gain trust and en-\ngagement than those based on other modalities, leading to a\nsignificant boost in sales. Halperin et. \u2022 Through qualitative and quantitative experiments, we\nshow that our method beats all previous and other pro-\nposed baselines on a publicly available dataset of im-\nages of fluid motion. \u2022 We prove the generalizability of our method to any ar-\nbitrary set of user directions by showing the qualitativeanimations of fluid objects in directions that did not ex-\nist in the training set. [4] animate pictures by allowing the\nusers to decompose the images into several layers, each one\nof which being needed to be animated in a different fashion. Xiong et al. Dorkenwald et al. 3.1. in realistic videos. More\nspecifically, for a given pixel location i, the optical flow\nFt\u2192t+1between consecutive frames in a video at any time\ntis given by,\nFt\u2192t+1(i) =MF(i) (1)\nCorrespondingly, the optical flow between the first frame\nand frame at any time t, can be obtained by Euler-\nintegration of MF,ttimes as given by,\nF0\u2192t(i) =F0\u2192t\u22121(i) +MF(i+F0\u2192t\u22121(i)) (2)where F0\u21921=MF. 3.2. The exponential of the Eu-\nclidean distance between location (xi, yi)of input image\nand starting position of arrow Aj,(xj, yj)is given by,\nDi,j\nL2=\u2225(xi, yi)\u2212(xj, yj)\u22252 (4)\nDi,j\nexp=e\u2212(Di,j\nL2/\u03c3)2\n(5)\nwhere Di,j\nL2is the Euclidean distance between location\n(xi, yi)of input image and starting position of arrow Aj,\n(xj, yj)and\u03c3is a constant. The dense optical flow FD\nfor a particular pixel location iin in the image is defined\nas weighted average of flow magnitude at each non-zero lo-\ncation in input mask M, where the weights are taken from\n3669\n\ud835\udc4d\ud835\udc4d\nSoftMax Splatting\nSoftMax SplattingUNet Downsampling \nLayers (Encoder -part)\nUNet Downsampling \nLayers (Encoder -part)\nUNet Upsampling \nLayers (Decoder -part)Symmetric Splatting\ud835\udc3c\ud835\udc3c0\n\ud835\udc3c\ud835\udc3c\ud835\udc41\ud835\udc41\ud835\udc3c\ud835\udc3c0\ud835\udc3c\ud835\udc3c\ud835\udc61\ud835\udc61SPADEEuler \nIntegration\nDeep Features Deep Features\ud835\udc39\ud835\udc39\ud835\udc46\ud835\udc46 \ud835\udc39\ud835\udc39\ud835\udc37\ud835\udc37 \ud835\udc39\ud835\udc39\ud835\udc37\ud835\udc37\ud835\udc45\ud835\udc45\ud835\udc39\ud835\udc390\u2192\ud835\udc61\ud835\udc61\n\ud835\udc39\ud835\udc390\u2192\ud835\udc61\ud835\udc61\ud835\udc39\ud835\udc390\u2192\ud835\udc61\ud835\udc61\u2212\ud835\udc41\ud835\udc41\n\ud835\udc39\ud835\udc390\u2192\ud835\udc61\ud835\udc61\u2212\ud835\udc41\ud835\udc41\ud835\udc4d\ud835\udc4d\n\ud835\udc4d\ud835\udc4d \ud835\udc4d\ud835\udc4d\nFrame Generation Dense Optical Flow \nPrediction\nMaskFigure 2. From figure\n5 we see that the dense flow map generated is assigned the\nsame horizontal flow to the majority of the waterfall and\nbelow the lake, whereas, realistically the waterfall would\nhave been moving vertically downward. 3.5. Instead of the\nbackward optical flow field (as used in [6]), we use forward\nflow to warp input frame to generate future frames as [11]\nobserved that forward flow produces more reliable flow es-\ntimates and sharper values at object boundaries. For a feature map corresponding to input image Iat resolu-\ntionDr\n0, the softmax-splatting output Dr\ntat pixel location i\nusing F0\u2192tis given by,\nDr\nt(i) =P\nj\u2208XDr\n0(i)eZ(j)\nP\nj\u2208XeZ(j)(7)where Xconsists of all the pixels that map to the same tar-\nget location iafter splatting. Specifically, any given pixel location iin the\ncombined deep feature \u02c6Dr\ntis given by,\n\u02c6Dr\nt(i) =P\nj\u2208X\u03b1Dr\nt(i)eZ(j)+P\nj\u2208X\u02c6\u03b1Dr\nt\u2212N(i)eZ(j)\nP\nj\u2208\u02c6X\u03b1eZ(j)+P\nj\u2208\u02c6X\u02c6\u03b1eZ(j)\n(8)\nwhere \u03b1and\u02c6\u03b1equalst\nNand(1\u2212t\nN),Dr\nt(i)andDr\nt\u2212N(i)\nare feature map obtained by softmax-splatting Dr\n0,Dr\nNwith\nF0\u2192tandF0\u2192t\u2212Nrespectively, Xand\u02c6Xconsists of all the\npixels that map to the same target location iafter splatting\nforDr\ntandDr\nt\u2212Nrespectively. The number of frames\nacross all videos is 60, each frame having a resolution of\n720x1280. Following observation on the visual quality of\ngenerated mask at different mvalues, we set the value of m\nto 10. 4.2. [6], Hao et al. 561.33 23.59\nHao et al. 526.55 23.35\nHao et al. 519.18 23.21\nHao et al. modified + frame generator 344.55 25.11\nOur (FD+ frame generator) 335.4 25.24\nOur (FR\nD+ frame generator) 315.31 25.1\nVanilla Eulerian 419.74 25.2\nTable 1. 4.5. Results\nEvaluating the flow map generation: In order to show\nthe efficacy of our flow map generation method, we com-\npute the mean PSNR between the generated flow maps and\nthe ground-truth flow maps. The results for three different\nnumbers of hints are shown in table 2. This is expected given that the Eulerian\n3673\nImage with Arrows Refined Dense Flow Dense Flow(a)\n(b)\n(d)(c)Figure 6. It is clear that our meth-\nods beat all the baselines in terms of FVD, while the PSNR\nis comparable across the different methods except Endo et\nal is the least of all. Qualitative Results: From figures 3 and 4, it is clearly\nvisible that dense refined flow FR\nDresembles the average\nground-truth optical flow much more accurately than the\nsparse refined flow FR\nS. The generated videos using our\nmethods and baseline are provided in supplementary. modified 21.4\nOur (FR\nD) 25.82\nVanilla Eulerian 18.28\nTable 2. The refined dense flow is\nsignificantly better than the refined sparse flow, thus corroborating\nthe need for the exponential operations before refinement. tion that is generated. The video generated using the dense\nflow has artifacts wherein the waterfalls tend to shift pro-\ngressively towards the right (due to dense flow having the\nsame flow values for waterfall and lake), whereas the video\ngenerated using the refined flow is very realistic and resem-\nbles the actual downward motion of waterfall observed in\nreal-world videos. We see that GF\nproduces results that respect the input arrow directions and\nare not just based on the input image, showing the robust-\nness and the generalizability of our method. We proposed a simple yet powerful method to approximate\nthe constant flow field governing the motion with simple\nexponential operations on the user-provided flow hints, and\nfurther show that in order to obtain a better flow field we\nneed to refine using a network the approximation for dense\nflow field rather than just the sparse hints. Through quanti-\ntative experiments, we show that our method performs bet-\nter than all baselines for a various number of motion hints. 2\n[3] Lluis Castrejon, Nicolas Ballas, and Aaron Courville. 1, 2\n[4] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian\nCurless, David H Salesin, and Richard Szeliski. In ACM SIG-\nGRAPH 2005 Papers , pages 853\u2013860. ACM Transactions on Graphics (Proceedings of ACM SIG-\nGRAPH Asia 2019) , 38(6):175:1\u2013175:19, 2019. In International Conference on\nMachine Learning , pages 3233\u20133246. 1, 2\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. 1, 2\n[10] Zekun Hao, Xun Huang, and Serge Belongie. 1, 2, 3, 7\n[11] Aleksander Holynski, Brian L. Curless, Steven M. Seitz, and\nRichard Szeliski. arXiv preprint arXiv:1705.06950 ,\n2017. 7\n[14] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin\nLu, and Ming-Hsuan Yang. Springer, 2020. 1, 2\n[17] Simon Niklaus and Feng Liu. 4, 5\n[18] Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng,\nJunjie Yan, and Xiaogang Wang. Video generation from sin-\ngle semantic label map. 1, 2\n[21] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Springer, 2015. 3\n[22] Karen Simonyan and Andrew Zisserman. arXiv\npreprint arXiv:1409.1556 , 2014. arXiv preprint arXiv:1812.01717 , 2018. 1, 2\n[26] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu\nLin, and Honglak Lee. arXiv preprint arXiv:1910.12713 , 2019. arXiv preprint arXiv:1808.06601 , 2018. 2\n[29] Yue Wu, Rongrong Gao, Jaesik Park, and Qifeng Chen. 2\n[30] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Springer, 2020. 2\n3676",
        "Conclusion": "Hao et. 2. Chuang et al. Li et al. Lo-\ngacheva et al. 3. Finally, to generate output video\nframes, we use a UNet [21] based frame generator to gen-\nerate video frames by warping the input image with FR\nDat\ndifferent resolutions of feature maps (Section 3.5). modi-\nfied\u2019. 3.3. It is clear that that dense flow computed using simple functions\nclosely approximates the ground-truth flow, and the refined dense\nflow best resembles the ground-truth flow among all. 4. 4.3. Baselines\nWe compare our final method with five different base-\nlines, one of our own, Endo et al. Endo et al. It clearly shows\nour method that uses refined flow and frame generator performs\nthe best in terms of FVD, thus showing the high-fidelity anima-\ntions generated by our method. 4.4. In addition, our final method (Our ( FR\nD+ frame generator))\nis better than just (Our ( FD+ frame generator), thus but-\ntressing the need for refining the approximated dense flow. 5. Conclusions and Limitations\nWe propose a method to animate images that contain\nfluid elements like water, fire, smoke, given a user-provided\nmask and flow hints in the forms of speed and direction. 2005. PMLR, 2020.",
        "label": 1
    },
    {
        "Abstract": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 1\u201317\nDecember 7-11, 2022 \u00a92022 Association for Computational Linguistics\nGenerative Knowledge Graph Construction: A Review\nHongbin Ye1,2, Ningyu Zhang1,2\u2217, Hui Chen3, Huajun Chen1,2\n1Zhejiang University & AZFT Joint Lab for Knowledge Engine\n2Hangzhou Innovation Center, Zhejiang University\n3Alibaba Group\n{yehongbin,zhangningyu,huajunsir}@zju.edu.cn ,weidu.ch@alibaba-inc.com\nAbstract\nGenerative Knowledge Graph Construction\n(KGC) refers to those methods that leverage the\nsequence-to-sequence framework for building\nknowledge graphs, which is flexible and can be\nadapted to widespread tasks. \":\nNamed Entity Recognition aims to identify the\ntypes of entities, e.g., \u2018Steve Job \u2019, \u2018Steve Wozniak \u2019\n\u21d2PERSON , \u2018Apple \u2019\u21d2ORG;\nRelation Extraction aims to identify the relation-\nship of the given entity pair \u27e8Steve Job ,Apple\u27e9as\nfounder ;\nEvent Extraction aims to identify the event type\nasBusiness Start-Org where \u2018 co-founded \u2019 trig-\ngers the event and ( Steve Jobs ,Steve Wozniak ) are\nparticipants in the event as AGENT andApple as\nORGrespectively. 7 Limitations\nIn this study, we provide a review of generative\nKGC. CoRR , abs/2112.04426. Gsum: A general\nframework for guided neural abstractive summariza-\ntion. NAACL , abs/2108.12724. CoRR , abs/1909.05858. CoRR ,\nabs/2111.01243. CoRR , abs/2011.01675. Boosting factual correctness of\nabstractive summarization with knowledge graph. CoRR , abs/2003.08612. CoRR ,\nabs/2202.05786.",
        "Methodology": "We\npresent the advantages and weaknesses of each\nparadigm in terms of different generation tar-\ngets and provide theoretical insight and empiri-\ncal analysis. Based on the review, we suggest\npromising research directions for the future. Our contributions are threefold: (1) We present\na detailed, complete taxonomy for the genera-\ntive KGC methods; (2) We provide a theoretical\nand empirical analysis of the generative KGC\nmethods; (3) We propose several research di-\nrections that can be developed in the future. How-\never, high-quality KGs rely almost exclusively on\nhuman-curated structured or semi-structured data. Conven-\ntionally, KGC is solved by employing task-specific\ndiscriminators for the various types of information\nin a pipeline manner (Angeli et al., 2015; Luan\net al., 2018; de S\u00e1 Mesquita et al., 2019; Zhang\net al., 2022a), typically including (1) entity discov-\nery or named entity recognition (Sang and Meulder,\n2003), (2) entity linking (Milne and Witten, 2008),\n(3) relation extraction (Zelenko et al., 2003) and (4)\nevent extraction (Du and Cardie, 2020). However,\nthis presents limitations of error population and\npoor adaptability for different tasks. {United states, Country-President ,\u00a0Joe Biden }DelinearizationFigure 1: Discrimination and generation methodologies\nfor relation extraction. Some generative KGC methods based on the\nsequence-to-sequence (Seq2Seq) framework are\nproposed to overcome this barrier. Early work\n(Zeng et al., 2018) has explored using the gener-\native paradigm to solve different entity and rela-\ntion extraction tasks. Hence, more generative\nKGC works (Yan et al., 2021a; Paolini et al., 2021;\nLu et al., 2022) have been proposed, showing ap-\npealing performance in benchmark datasets. The target triple is preceded\nby the tag <triple>, and the head entity, tail entity,\nand relations are also specially tagged, allowing the\nstructural knowledge (corresponding to the output)\nto be obtained by inverse linearization. We propose to organize relevant work by\nthe generation target of models and also present the\naxis of the task level (Figure 3):\n\u2022Comprehensive review with new tax-\nonomies . We conduct the first comprehensive\nreview of generative KGC together with new\ntaxonomies. We provide in-depth theoretical and empiri-\ncal analysis for typical generative KGC meth-\nods, illustrating the advantages and disadvan-\ntageous of different methodologies as well as\nremaining issues (\u00a74). We provide\ncomprehensive coverage of emerging areas,\nincluding prompt-based learning. Related work As this topic is relatively nascent,\nonly a few surveys exist. Closest to our work, Ji\net al. (2022) covers methods for knowledge graph\nconstruction, representation learning, and applica-\ntions, which mainly focus on general methods for\nKGC. (2021) survey\nrecent work that uses these large language mod-\nels to solve tasks via text generation approaches,\nwhich has overlaps in generation methodologies\nfor information extraction. Different from those\nsurveys, in this paper, we conduct a literature re-\nview on generative KGC, hoping to systematically\nunderstand the methodologies, compare different\nmethods and inspire new ideas. 1https://github.com/zjunlp/Generative_KG_\nConstruction_Papers\n/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000028/uni00000051/uni00000057/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000002f/uni0000004c/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004a/uni00000028/uni00000059/uni00000048/uni00000051/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000031/uni00000044/uni00000050/uni00000048/uni00000047/uni00000003/uni00000028/uni00000051/uni00000057/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000035/uni00000048/uni00000046/uni00000052/uni0000004a/uni00000051/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000025/uni0000004f/uni00000044/uni00000051/uni0000004e/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni0000002c/uni00000051/uni00000047/uni0000004c/uni00000046/uni00000048/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000036/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000058/uni00000055/uni00000048/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000026/uni00000052/uni00000053/uni0000005c/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048Figure 2: Sankey diagram of knowledge graph construc-\ntion tasks with different generative paradigms. 2.2 Discrimination and Generation\nMethodologies\nIn this section, we introduce the background of\ndiscrimination and generation methodologies for\nKGC. The goal of the discrimination model is to\npredict the possible label based on the characteris-\ntics of the input sentence. As shown in Figure 1,\ngiven annotated sentence xand a set of potentially\noverlapping triples tj={(s, r, o )}inx, we aim to\nmaximize the data likelihood during the training\nprocess:\npcls(t|x) =/productdisplay\n(s,r,o)\u2208tjp((s, r, o )|xj) (1)\nAnother method of discrimination is to output\ntags using sequential tagging for each position i\n(Zheng et al., 2017; Dai et al., 2019; Yu et al., 2020;\nLi et al., 2020b; Liu et al., 2021a). The\nsize of a set of pre-defined relations is |R|, and\nthe related role orders are represented by \"1\" and\n\"2\". During the training model, we maximize the\nlog-likelihood of the target tag sequence using the\nhidden vector hiat each position i:\nptag(y|x) =exp(hi, yi)/summationtext\ny\u2032\u2208Rexp (exp( hi, y\u2032\ni))(2)For the generation model, if xis the input sen-\ntence and ythe result of linearized triplets, the tar-\nget for the generation model is to autoregressively\ngenerate ygiven x:\npgen(y|x) =len(y)/productdisplay\ni=1pgen(yi|y<i, x)(3)\nBy fine-tuning seq2seq model (e.g. 2.3 Advantages of the Generation Methods\nWhile the previous discriminative methods (Wei\net al., 2020; Shang et al., 2022) extracts relational\ntriples from unstructured text according to a pre-\ndefined schema to efficiently construct large-scale\nknowledge graphs, these elaborate models focus on\nsolving a specific task of KGC, such as predicting\nrelation and event information from a segment of\ninput text which often requires multiple models to\nprocess. In contexts with nested\nlabels in NER (Strakov\u00e1 et al., 2019), the proposed\ngenerative method implicitly models the structure\nbetween named entities, thus avoiding the com-\nplex multi-label mapping. In short, new\ndirections can be explored for some hard-to-solve\nproblems through paradigm shifts. Note that the discriminative and generative meth-\nods are not simply superior or inferior due to the\nproliferation of related studies. As shown in Figure 2, these paradigms\nhave demonstrated strong dominance in many\nmainstream KGC tasks. 3.1 Copy-based Sequence\nThis paradigm refers to developing more robust\nmodels to copy the corresponding token (entity) di-\nrectly from the input sentence during the generation\nprocess. As shown in Figure 4,\nthe model copies the head entity from the input sen-\ntence and then the tail entity. Similarly, relations\nare generated from target vocabulary, which is re-\nstricted to the set of special relation tokens. In order to identify a rea-\nsonable triple extraction order, Zeng et al. (2019)\nconverts the triplet generation process into a re-\ninforcement learning process, enabling the copy\nmechanism to follow an efficient generative order. Since the entity copy mechanism relies on unnat-\nural masks to distinguish between head and tail\nentities, Zeng et al. (2020) maps the head and tail\nentities to fused feature space for entity replication\nby an additional nonlinear layer, which strengthens\nthe stability of the mechanism. (2021) proposes a TOP-\nk copy mechanism to alleviate the computational\ncomplexity of entity pairs. In order to avoid introducing noise, it utilizes the\nevent schema to constrain decoding space, ensuring\nthe output text is semantically and structurally legit-\nimate. (2021) introduce a contrastive\nlearning framework with a batch dynamic attention\nmasking mechanism to overcome the contradiction\nin meaning that generative architectures may pro-\nduce unreliable sequences (Zhu et al., 2020). triplet decomposition method for the relation ex-\ntraction task, which is flexible and can be adapted\nto unified domains or longer documents. (2019)\nproposes a flattened encoding algorithm, which\noutputs multiple NE tags following the BILOU\nscheme. The multi-label of a word is a concatena-\ntion of all intersecting tags from highest priority\nto lowest priority. (2022) extends the domain to structural\nheterogeneous information extraction by proposing\na unified task-agnostic generation framework. The output sequence\ncopies all words in the input sentence, as it helps\nto reduce ambiguity. In addition, this paradigm\nuses square brackets or other identifiers to spec-\nify the tagging sequence for the entity of interest. Meanwhile, the\nlabeled words are described with natural words\nso that the potential knowledge of the pre-trained\nmodel can be leveraged (Paolini et al., 2021). To retrieve en-\ntities by generating their unique names, Cao et al. (2021) extends the autoregressive framework to\ncapture the relations between context and entity\nAnti-Ethiopia riots erupted in Mogadishu  , the capital of\nSomalia  , on Friday , while masked gunmen emerged ...Generation Model9944 9944Somalia Mogadishu Somalia Mogadishu/location/location/ contains /location/country/capitalDecoding\nSomalia\nMogadishu/location/location/ contains\n/location/country/capitalFigure 7: Indice-based Sequence. 3.4 Indice-based Sequence\nThis paradigm generates the indices of the words\nin the input text of interest directly and encodes\nclass labels as label indices. As the output is strictly\nrestricted, it will not generate indices that corre-\nsponding entities do not exist in the input text,\nexcept for relation labels. Nayak and Ng (2020)\napply the method to the relation extraction task, en-\nabling the decoder to find all overlapping tuples\nwith full entity names of different lengths. As\nshown in Figure 7, given the input sequence x,\nthe output sequence yis generated via the indices:\ny= [b1, e1, t1, . , b k, ek, tk]where\nbiandeiindicates the begin and end indices of a\nentity tuple, tiis the index of the entity type, and k\nis the number of entity tuples. The hidden vector\nis computed at decoding time by the pointer net-\nwork (Vinyals et al., 2015) to get the representation\nof the tuple indices. (2021a) applies the method to a role-filler\nentity extraction task by implicitly capturing noun\nphrase coreference structure. 3.5 Blank-based Sequence\nThis paradigm refers to utilizing templates to\ndefine the appropriate order and relationship for\nthe generated spans. (2021b) explores a\nblank-based form for event extraction tasks which\nincludes special tokens representing event informa-\ntion such as event types. (2021b) frames\ndocument-level event argument extraction as con-5Generation Model\nDocument: Elliott testified that on April 15, McV eigh came into the\nbody shop and <tgr>  reserved <tgr>  the truck, to be picked up at 4pm\ntwo days later ...\nTemplate: <arg1>  bought, sold, or traded <arg3>  to <arg2> in\nexchange for <arg4>  for the benefit of <arg5>  at <arg6>  place.+Elliott  bought, sold or traded  truck  to McVeigh in exchange for $280.32  for\nthe benefit of at body shop  place. ditional generation given a template and introduces\nthe new document-level informative to aid the gen-\neration process. (2022) designs a language-\nagnostic template to represent the event argument\nstructures, which facilitate the cross-lingual trans-\nfer. Instead of conventional heuristic threshold\ntuning, Ma et al. (2022) proposes an effective yet\nefficient model PAIE for extracting multiple argu-\nments with the same role. A unifying theme across\nmany of these methods is that of end-to-end archi-\ntecture or the idea that the knowledge extraction\ncan be redefined as text sequence to structure gen-\neration task. Generative models can decode and\ncontrol extraction targets on demand for different\nspecific tasks, scenarios, and settings (i.e., different\nschema). However, due to the different forms of\nspecific KGC tasks, there is still some disagreement\nin the utilization of the generation paradigms. In principle, we believe that the closer the out-\nput form is to natural language, the smaller the\ngap between the generative model and the training\ntask. We observe that the blank-based paradigm\nhas a clear advantage in this scope, which uses\nmanually constructed templates to make the output\nclose to natural language fluency. Due to the application of the constraint de-\ncoding mechanism, some structure-based methods\ncan be reduced to the same decoding space as the\ncopy-based methods. We believe that architec-\ntures with the ability to organize information more\nflexibly have excellent cross-task migration capa-\nbilities such as structure-based, label-based and\nblank-based paradigms. Totally in line with recent trends in NLP, a grow-\ning number of unified generation strategies require\nmore universal architectures (Deng et al., 2021; Li\net al., 2021a), as they allow a remarkable degree\nof output flexibility. We think that future research\nshould focus on unifying cross-task models and\nfurther improving decoding efficiency. \"SU\" indicates semantic utilization,\n\"SS\" indicates search space, \"AS\" indicates application scope, and \"TS\" indicates template cost. We divide the\ndegree into three grades :L (low), M (middle), and H (high), and the \u2191indicates that the higher grade performance is\nbetter while the \u2193is the opposite. Notably, with\nsmall output space (e.g., methods with the indice-\nbased sequence in \u00a73.4), the model can converge\nfaster. In short,\nthe design of output space is vital for generative\nKGC, and it is necessary to balance parametric\noptimization as well as semantic utilization. For inference, we argue that sequence decoding\nin the generation is an essential procedure for gen-\nerative KGC. Given the probabilistic nature of q,\nthe decoding process will select words that maxi-\nmize the probability of the resulting string. On the other hand, non-autoregressive parallel de-\ncoding has also been leveraged for generative KGC. (2021) formulates end-to-end knowledge\nbase population as a direct set generation problem,\navoiding considering the order of multiple facts. Note that the decoding mechanism plays a vital\nrole in inference speed and quality. We argue that\nit is necessary to develop sophisticated, efficientdecoding strategies (e.g., with guidance from KG)\nfor generative KGC. 2) Although the discrimination meth-\nods obtain good performance, the performance of\nthe generation methods has been improved more\nvastly in recent years, so we have reason to believe\nthat they will have greater application scope in the\nnear future. In addition, we also show the perfor-\nmance of the non-autoregressive method on two\ndatasets, and we discuss the promising value of this\nmethod in \u00a7 5. The top section refers to the discrimination models, and the bottom section\nindicates generation models. \"*\" refers to the non-autoregressive models. The top section refers to the discrimination models, and the bottom section\nindicates the generation models. performance with advanced discriminative models,\nnoting that non-autoregressive methods have better\ndecoding efficiency and training efficiency. From Table 3, we observe that generation meth-\nods can obtain comparable performance compared\nwith discrimination models on event extraction\ntasks. In\naddition, owing to the complete template design of\nthe Blank-based approach, PLMs can understandcomplex task knowledge, structural knowledge of\nthe extraction framework, and label semantics in a\nnatural language manner. Most of the recent\ngenerative KGC frameworks face serious homog-\nenization with Transformer. Considering the target reli-\nability of generation methods, more sophisticated\nstrategies can be leveraged to control the quality of\ngenerative KGC, including: 1) Control code con-\nstruction (Keskar et al., 2019; Dou et al., 2021);\n2) Decoding strategy such as introducing external\nfeedback (Holtzman et al., 2018) and generative\ndiscriminator (Krause et al., 2021); 3) Loss func-\ntion design (Chan et al., 2021); 4) Prompt design\n(Brown et al., 2020; Qian et al., 2022); 5) Retrieval\naugmentation (Li et al., 2022b); 6) Write-then-Edit\nstrategy (Dathathri et al., 2020); 7) Diffusion pro-\ncess (Li et al., 2022c; Gong et al., 2022). In practical applications, it\nis essential to reduce data annotation and training\ncosts. One idea is to freeze most of the genera-\ntion model parameters (Liu et al., 2021b; Li and\nLiang, 2021; Chen et al., 2022b) or leverage prompt\nlearning (Chen et al., 2022e). Universal Deployment. Furthermore, it is efficient for real-world deploy-\nment when we can provide a single model to sup-\nport widespread KGC tasks (Zhang et al., 2020a). However, the autoregressive decoder gen-\nerates each token based on previously generated\ntokens during inference, and this process is not par-\nallelizable. Therefore, it is beneficial to develop\na fast inference model for generative KGC. (2020) utilizes the transformer-\nbased non-autoregressive decoder (Gu et al., 2018)as a triple set generator that can predict all triples\nat once. Likewise, the semi-autoregressive decoding (Wang\net al., 2018) preserves the autoregressive approach\nwithin the block to ensure consistency while im-\nproving the tuple output efficiency. Additionally,\npathways (Barham et al., 2022) can dynamically\nassign competencies to different parts of the neural\nnetwork, which is faster and more efficient as it\ndoes not activate the entire network for each task. Note that the generative paradigm for KGC has\nthe potential advantages of unifying different tasks\nand better utilizing semantic information. In the\nfuture, we envision a more potent synergy between\nthe methodologies from the NLG and knowledge\ngraph communities. On the converse, we expect symbolic structure in\nKG can have potential guidance for text generation. Moreover, we only\nreview the works within five years, mainly from the\nACL, EMNLP, NAACL, COLING, AAAI, IJCAI,\netc. We will continue adding more related works\nwith more detailed analysis. Acknowledgment\nWe want to express gratitude to the anonymous\nreviewers. This work was supported by the\nNational Natural Science Foundation of China\n(No.62206246, 91846204 and U19B2027), Zhe-\njiang Provincial Natural Science Foundation of\nChina (No. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021 , pages 4402\u2013\n4417. Bootstrapped self\ntraining for knowledge base population. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 375\u2013385. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi. Pere-Llu\u00eds Huguet Cabot and Roberto Navigli. Autoregressive entity retrieval. Decoupling knowledge from\nmemorization: Retrieval-augmented prompt learning. Rela-\ntion extraction as open-book examination: Retrieval-\nenhanced prompt tuning. In WWW \u201922: The ACM Web\nConference 2022, Virtual Event, Lyon, France, April\n25 - 29, 2022 , pages 2778\u20132788. Joint extraction\nof entities and overlapping relations using position-\nattentive sequence labeling. In The Thirty-Third\nAAAI Conference on Artificial Intelligence, AAAI\n2019, The Thirty-First Innovative Applications of\nArtificial Intelligence Conference, IAAI 2019, The\nNinth AAAI Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019 , pages 6300\u2013\n6308. Filipe de S\u00e1 Mesquita, Matteo Cannaviccio, Jordan\nSchmidek, Paramita Mirza, and Denilson Barbosa. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 749\u2013758. Compression, transduc-\ntion, and creation: A unified framework for evalu-\nating natural language generation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021 , pages 7580\u20137605. Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\nJiang, and Graham Neubig. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2021, Online, June 6-11, 2021 ,\npages 4830\u20134842. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 671\u2013683. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, EACL 2021, Online, April 19 - 23,\n2021 , pages 634\u2013644. Ph.D. thesis, University of Bologna, Italy. A sequence-to-sequence approach for document-\nlevel relation extraction. Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K.\nLi, and Richard Socher. Learn-\ning to write with cooperative discriminators. I-Hung Hsu, Kuan-Hao Huang, Elizabeth Boschee,\nScott Miller, Prem Natarajan, Kai-Wei Chang, and\nNanyun Peng. Multilin-\ngual generative language models for zero-shot cross-\nlingual event argument extraction. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022 , pages\n4633\u20134646. Document-level entity-based extraction as tem-\nplate generation. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021 , pages\n5257\u20135269. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. Gedi: Genera-\ntive discriminator guided sequence generation. Im-\nproving candidate retrieval with entity profile gen-\neration for wikidata entity linking. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020 , pages 7871\u20137880. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022 , pages 8335\u20138351. Document-\nlevel event argument extraction by conditional gener-\nation. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2021, Online, June 6-11, 2021 ,\npages 894\u2013908. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021 , pages 4582\u2013\n4597. A joint neural model for information extraction with\nglobal features. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020 ,\npages 1641\u20131651. Dynamic prefix-tuning for generative template-based\nevent extraction. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022 , pages 5216\u20135228. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018 , pages 1247\u20131256. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021 , pages 4829\u2013\n4839. InProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021 , pages\n2795\u20132806. Uni-\nfied structure generation for universal information\nextraction. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022 , pages 5755\u20135772. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018 , pages 3219\u20133232. PAIE: prompting argument interaction\nfor event argument extraction. In Proceedings of the 17th ACM\nConference on Information and Knowledge Manage-\nment, CIKM 2008, Napa Valley, California, USA,\nOctober 26-30, 2008 , pages 509\u2013518. Effective mod-\neling of encoder-decoder architecture for joint entity\nand relation extraction. Giovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai,\nC\u00edcero Nogueira dos Santos, Bing Xiang, and Ste-\nfano Soatto. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Hyspa: Hybrid span generation\nfor scalable text-to-graph extraction. Don\u2019t parse, generate! Explanation graph generation via pre-trained\nlanguage models: An empirical study with con-\ntrastive learning. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022 , pages 1190\u20131208. CoRR , abs/2203.05412. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA , volume 97 of Proceedings of Ma-\nchine Learning Research , pages 5926\u20135936. Joint entity\nand relation extraction with set prediction networks. Set generation networks for\nend-to-end knowledge base population. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021 , pages 9650\u20139660. Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza\nKheradpisheh, Timoth\u00e9e Masquelier, and Anthony\nMaida. InProceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 5783\u20135788. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, Brussels, Bel-\ngium, October 31 - November 4, 2018 , pages 479\u2013\n488. Tplinker:\nSingle-stage joint extraction of entities and relations\nthrough token pair linking. A novel cascade binary tagging\nframework for relational triple extraction. A unified generative framework for\naspect-based sentiment analysis. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021 , pages 2416\u20132429. A unified generative\nframework for various NER subtasks. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021 , pages 5808\u20135822. Bridging text\nand knowledge with multi-prototype embedding for\nfew-shot relational triple extraction. Kernel methods for relation ex-\ntraction. Distant supervision for relation extraction via\npiecewise convolutional neural networks. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020 , pages 9507\u2013\n9514. Learning the\nextraction order of multiple relational facts in a sen-\ntence with reinforcement learning. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019 , pages 367\u2013377. Extracting relational facts by an\nend-to-end neural model with copy mechanism. Ningyu Zhang, Xin Xu, Liankuan Tao, Haiyang Yu,\nHongbin Ye, Shuofei Qiao, Xin Xie, Xiang Chen,\nZhoubo Li, Lei Li, et al. Process. Minimize ex-\nposure bias of seq2seq models in joint entity and\nrelation extraction. Joint extraction\nof entities and relations based on a novel taggingscheme. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers , pages 1227\u20131236. Claret: Pre-training a\ncorrelation-aware context-to-event transformer for\nevent-centric generation and classification. Chenguang Zhu, William Hinthorn, Ruochen Xu,\nQingkai Zeng, Michael Zeng, Xuedong Huang, and\nMeng Jiang. The time for each paper is based on its first arXiv version (if\nit exists) or estimated submission time. Works in red consider copy-based sequence methods; works in\nblue consider structure-linearized sequence methods; works in green consider label-augmented sequence\nmethods; works in orange consider indice-based sequence methods; works in purple consider blank-based\nsequence methods.",
        "Results and Findings": "\u2217Corresponding author. (b) T agging Model\n(c) Generation ModelCountry-President\nNoneNone\nExtracted Results\n{United states, Country-President , Joe Biden}\nInput T ext: The\u00a0 \u00a0 United States\u00a0President\u00a0 Joe\u00a0 \u00a0 \u00a0 \u00a0Biden\u00a0visited\u00a0Samsung\u00a0 \u00a0. Powered by fast advances\nof generative pre-training such as T5 (Raffel et al.,\n2020), and BART (Lewis et al., 2020), Seq2Seq\nparadigm has shown its great potential in unifying\nwidespread NLP tasks. Despite the\nsuccess of numerous generative KGC approaches,\nthese works scattered among various tasks have not\nbeen systematically reviewed and analyzed.1Present work In this paper, we summarize re-\ncent progress in generative KGC (An timeline of\ngenerative KGC can be found in Appendix A) and\nmaintain a public repository for research conve-\nnience1. \u2022Wide coverage on emerging advances and\noutlook on future directions . Knowledge Graph Completion aims to complete\nincomplete triples \u27e8Steve Job ,create ,?\u27e9for blank\nentities Apple ,NeXT Inc. andPixar .2Generative KGC Taxonomy\nGeneration\nTargetCopy-based\nSequenceCopyRE (Zeng et al., 2018), CopyRRL (Zeng et al., 2019), CopyMTL (Zeng et al., 2020), TEMPGEN (Huang et al., 2021),\nSeq2rel (Giorgi et al., 2022)\nStructure-based\nSequenceSeq2Seq4ATE (Ma et al., 2019), Nested-seq (Strakov\u00e1 et al., 2019), CGT (Zhang et al., 2021b; Ye et al., 2021), PolicyIE\n(Ahmad et al., 2021), Text2Event (Lu et al., 2021), HySPA (Ren et al., 2021), REBEL (Cabot and Navigli, 2021), SQUIRE\n(Bai et al., 2022), GenKGC (Xie et al., 2022), EPGEL (Lai et al., 2022), HuSe-Gen (Saha et al., 2022), UIE (Lu et al., 2022),\nDEEPSTRUCT (Wang et al., 2022), De-Bias (Zhang et al., 2022b), KGT5 (Saxena et al., 2022), KG-S2S (Chen et al., 2022a)\nLabel-based\nSequenceANL (Athiwaratkun et al., 2020a), GENRE (Cao et al., 2021), TANL (Paolini et al., 2021)\nIndice-based\nSequencePNDec (Nayak and Ng, 2020), SEQ2SEQ-PTR (Rongali et al., 2020), GRIT (Du et al., 2021a), UGF for NER\n(Yan et al., 2021b), UGF for ABSA (Yan et al., 2021a)\nBlank-based\nSequenceCOMET (Bosselut et al., 2019), BART-Gen (Li et al., 2021b), GTT (Du et al., 2021b), DEGREE (Hsu et al., 2022), ClarET\n(Zhou et al., 2022), GTEE (Liu et al., 2022), X-GEAR (Huang et al., 2022), PAIE (Ma et al., 2022)Generation\nTasksNamed Entity\nRecognitionNested-seq (Strakov\u00e1 et al., 2019), ANL (Athiwaratkun et al., 2020a), TANL (Paolini et al., 2021), HySPA (Ren et al., 2021),\nUGF for NER (Yan et al., 2021b), DEEPSTRUCT (Wang et al., 2022), De-Bias (Zhang et al., 2022b), UIE (Lu et al., 2022)\nRelation ExtractionCopyRE (Zeng et al., 2018), CopyRRL (Zeng et al., 2019), PNDec (Nayak and Ng, 2020), CopyMTL (Zeng et al., 2020),\nCGT (Zhang et al., 2021b; Ye et al., 2021), TANL (Paolini et al., 2021), HySPA (Ren et al., 2021), TEMPGEN\n(Huang et al., 2021), REBEL (Cabot and Navigli, 2021), DEEPSTRUCT (Wang et al., 2022), UIE (Lu et al., 2022),\nSeq2rel (Giorgi et al., 2022)\nEvent ExtractionCGT (Zhang et al., 2021b; Ye et al., 2021), TANL (Paolini et al., 2021), BART-Gen (Li et al., 2021b), GTT (Du et al., 2021b),\nGRIT (Du et al., 2021a), Text2Event (Lu et al., 2021), DEGREE (Hsu et al., 2022), ClarET (Zhou et al., 2022), GTEE\n(Liu et al., 2022), X-GEAR (Huang et al., 2022), DEEPSTRUCT (Wang et al., 2022), PAIE (Ma et al., 2022), UIE\n(Lu et al., 2022)\nEntity Linking GENRE (Cao et al., 2021), EPGEL (Lai et al., 2022)\nKnowledge Graph\nCompletionCOMET (Bosselut et al., 2019), SQUIRE (Bai et al., 2022), GenKGC (Xie et al., 2022),, HuSe-Gen (Saha et al., 2022),\nClarET (Zhou et al., 2022), KGT5 (Saxena et al., 2022), KG-S2S (Chen et al., 2022a)\nFigure 3: Taxonomy of Generative Knowledge Graph Construction. MASS (Song\net al., 2019) , T5 (Raffel et al., 2020) , and BART\n(Lewis et al., 2020) ) on such a task, using the cross-\nentropy loss, we can maximize the log-likelihood\nof the generated linearized triplets. In addition, generative models can be pre-trained in\nmultiple downstream tasks by structurally consis-\ntent linearization of the text, which facilitates the\ntransition from traditional understanding to struc-\ntured understanding and increases knowledge shar-\ning (Wang et al., 2022). Moreover, Wang et al. 3.3 Label-augmented Sequence\nThis paradigm refers to utilizing the extra mark-\ners to indicate specific entities or relationships. As\nshown in Figure 6, Athiwaratkun et al. The relevant labels are separated by the separator\n\"|\" within the enclosed brackets. name by effectively cross-encoding both. . . . . In addition,\nDu et al. As shown in Figure 8, the template\nrefers to a text describing an event type, which adds\nblank argument role placeholders. Huang et al. 3)Application scope refers to the range of KGC\ntasks that can be applied. We observe that most paradigms do not\nrequire complex template design and rely only on\nlinear concatenation to meet the task requirement. . .\u27e9decomposed by words yt:\npgen(y|x) =len(y)/productdisplay\ni=1pgen(yi|y<i, x)(4)\nwhere yconsists of all possible strings that can\nbe constructed from words in the model\u2019s vocabu-\nlaryV. Note that the output ycan take on a vari-\nety of forms depending on the task, e.g., entities,\nrelational triples, or an event structure. 4.2 Empirical Analysis\nTo investigate the effect of different generation\nmethods, we conduct an analysis of the experi-\nmental results of existing generative KGC work. Table 2 shows the performance of dis-\ncrimination models and generative models on the\nNYT datasets. We can observe that: 1) Structure-\nbased and label-based methods both achieve simi-\nlar extraction performance compared with all dis-\ncrimination models on NYT datasets. We observe that parallel generation\nof the unordered triple set can obtain comparable\n2Results are taken from existing papers.7Type ModelsNYT\nP R F\nDiscriminationCasRel (Wei et al., 2020) 89.7 89.5 89.6\nTPLinker (Wang et al., 2020) 91.4 92.6 92.0\nOneRel (Shang et al., 2022) 92.8 92.9 92.8\nCopy-basedCopyRE (Zeng et al., 2018) 61.0 56.6 58.7\nCopyRRL (Zeng et al., 2019) 77.9 67.2 72.1\nCopyMTL (Zeng et al., 2020) 75.7 68.7 72.0\nStructure-basedCGT (Ye et al., 2021) 94.7 84.2 89.1\nREBEL (Cabot and Navigli, 2021) 91.5 92.0 91.8\nUIE (Lu et al., 2022) - - 93.5\nDEEPSTRUCT (Wang et al., 2022) - - 93.9\nLabel-based TANL (Paolini et al., 2021) - - 90.8\nIndice-based PNDec (Nayak and Ng, 2020) 89.3 78.8 83.8\nOthers*SPN (Sui et al., 2020) 93.3 91.7 92.5\nSeq2UMTree (Zhang et al., 2020b) 79.1 75.1 77.1\nTable 2: Main results of NYT dataset. Type ModelsTrigger Argument\nId Cl Id Cl\nDiscriminationJMEE (Liu et al., 2018) 75.9 73.7 68.4 60.3\nDYGIE++ (Wadden et al., 2019) - 69.7 53.0 48.8\nOneIE (Lin et al., 2020) 78.6 75.2 60.7 58.6\nQAEE (Du and Cardie, 2020) 75.8 72.4 55.3 53.3\nMQAEE (Li et al., 2020a) 74.5 71.7 55.2 53.4\nRCEE (Liu et al., 2020) - 74.9 - 63.6\nStructure-basedTEXT2EVENT (Lu et al., 2021) - 71.9 - 53.8\nUIE (Lu et al., 2022) - 73.4 - 54.8\nDEEPSTRUCT (Wang et al., 2022) 73.5 69.8 59.4 56.2\nLabel-based TANL (Paolini et al., 2021) 72.9 68.4 50.1 47.6\nBlank-basedBART-Gen (Du et al., 2021b) 74.4 71.1 55.2 53.7\nDEGREE (Hsu et al., 2022) - 73.3 - 55.8\nGTEE (Liu et al., 2022) - 72.6 - 55.8\nPAIE (Ma et al., 2022) - - 75.7\u221772.7\u2217\nTable 3: F1 results (%) of ACE-2005. LGG22F030011), Ningbo Natural Sci-\nence Foundation (2021J190), and Yongjiang Talent\nIntroduction Programme (2021A-156-G).9References\nWasi Uddin Ahmad, Jianfeng Chi, Tu Le, Thomas Nor-\nton, Yuan Tian, and Kai-Wei Chang. Gabor Angeli, Victor Zhong, Danqi Chen, Arun Te-\njasvi Chaganty, Jason Bolton, Melvin Jose Johnson\nPremkumar, Panupong Pasupat, Sonal Gupta, and\nChristopher D. Manning. 2015. NIST. Ben Athiwaratkun, C\u00edcero Nogueira dos Santos, Jason\nKrone, and Bing Xiang. Augmented natu-\nral language for generative sequence labeling. Ben Athiwaratkun, Cicero Nogueira dos Santos, Jason\nKrone, and Bing Xiang. Augmented natu-\nral language for generative sequence labeling. Yushi Bai, Xin Lv, Juanzi Li, Lei Hou, Yincen Qu, Zelin\nDai, and Feiyu Xiong. CoRR , abs/2201.06206. Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay\nGhemawat, Steven Hand, Dan Hurt, Michael Isard,\nHyeontaek Lim, Ruoming Pang, Sudip Roy, Bren-\nnan Saeta, Parker Schuh, Ryan Sepassi, Laurent El\nShafey, Chandramohan A. Thekkath, and Yonghui\nWu. mlsys.org. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. In Proceedingsof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n4762\u20134779. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Nicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. OpenReview.net. Alvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang,\nand Jie Fu. OpenReview.net. Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. Xiang Chen, Lei Li, Shumin Deng, Chuanqi Tan,\nChangliang Xu, Fei Huang, Luo Si, Huajun Chen,\nand Ningyu Zhang. Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang,\nShumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. In SIGIR \u201922: The 45th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, Madrid,\nSpain, July 11 - 15, 2022 , pages 2443\u20132448. Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and\nJun Zhao. 2015. In ACL (1) ,\npages 167\u2013176. Jason P. C. Chiu and Eric Nichols. 2016. Trans. Assoc. Dai Dai, Xinyan Xiao, Yajuan Lyu, Shan Dou, Qiao-\nqiao She, and Haifeng Wang. AAAI Press. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. Plug and play language models:\nA simple approach to controlled text generation. OpenReview.net. Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P.\nXing, and Zhiting Hu. Diffusion models beat gans on image synthesis. Xinya Du and Claire Cardie. Xinya Du, Alexander M. Rush, and Claire Cardie. Xinya Du, Alexander M. Rush, and Claire Cardie. InProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pages\n909\u2013914. John M. Giorgi, Gary D. Bader, and Bo Wang. 2018. OpenReview.net.11Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine\nBosselut, David Golub, and Yejin Choi. 2018. Kuan-Hao Huang, I-Hung Hsu, Prem Natarajan, Kai-\nWei Chang, and Nanyun Peng. Kung-Hsiang Huang, Sam Tang, and Nanyun Peng. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Mart-\ntinen, and Philip S. Yu. ,\n33(2):494\u2013514. Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\nCaiming Xiong, and Richard Socher. OpenReview.net. Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq R. Joty, Richard Socher,\nand Nazneen Fatema Rajani. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 16-20 November, 2021 , pages\n4929\u20134952. Tuan Lai, Heng Ji, and ChengXiang Zhai. Bei Li, Quan Du, Tao Zhou, Yi Jing, Shuhan Zhou, Xin\nZeng, Tong Xiao, JingBo Zhu, Xuebo Liu, and Min\nZhang. Fayuan Li, Weihua Peng, Yuguang Chen, Quan Wang,\nLu Pan, Yajuan Lyu, and Yong Zhu. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, Online Event, 16-20 November 2020 ,\nvolume EMNLP 2020 of Findings of ACL , pages\n829\u2013838. Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. CoRR , abs/2202.01110. Juan Li, Ruoxu Wang, Ningyu Zhang, Wen Zhang, Fan\nYang, and Huajun Chen. Junyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaox-\nuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu,\nWayne Xin Zhao, and Ji-Rong Wen. Sha Li, Heng Ji, and Jiawei Han. Prefix-tuning:\nOptimizing continuous prompts for generation. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani,\nPercy Liang, and Tatsunori B. Hashimoto. Diffusion-lm improves controllable text generation. CoRR , abs/2205.14217. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and\nXuan Zhu. 2015. AAAI Press. Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. Jian Liu, Yubo Chen, Kang Liu, Wei Bi, and Xiaojiang\nLiu. Kun Liu, Yao Fu, Chuanqi Tan, Mosha Chen, Ningyu\nZhang, Songfang Huang, and Sheng Gao. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pages\n3437\u20133445. Xiao Liu, Heyan Huang, Ge Shi, and Bo Wang. Xiao Liu, Zhunchen Luo, and Heyan Huang. 2018. Jointly multiple events extraction via attention-based\ngraph information aggregation. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong\nTang, Annan Li, Le Sun, Meng Liao, and Shaoyi\nChen. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu\nLin, Xianpei Han, Le Sun, and Hua Wu. 2018. Dehong Ma, Sujian Li, Fangzhao Wu, Xing Xie,\nand Houfeng Wang. Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi\nChen, Kun Wang, and Jing Shao. David N. Milne and Ian H. Witten. ACM.13Bonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heintz, and Dan Roth. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020 , pages 8528\u20138535. AAAI Press. , 25(69):13\u2013\n41. OpenReview.net. Jing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu\nChen. Controllable natural language genera-\ntion with contrastive prefixes. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. J. Mach. Res. , 21:140:1\u2013140:67. Liliang Ren, Chenkai Sun, Heng Ji, and Julia Hock-\nenmaier. In Findings\nof the Association for Computational Linguistics:\nACL/IJCNLP 2021, Online Event, August 1-6, 2021 ,\nvolume ACL/IJCNLP 2021 of Findings of ACL ,\npages 4066\u20134078. Subendhu Rongali, Luca Soldaini, Emilio Monti, and\nWael Hamza. In WWW \u201920: The Web Conference\n2020, Taipei, Taiwan, April 20-24, 2020 , pages 2962\u2013\n2968. ACM / IW3C2. Swarnadeep Saha, Prateek Yadav, and Mohit Bansal. Erik F. Tjong Kim Sang and Fien De Meulder. Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. Yuming Shang, Heyan Huang, and Xian-Ling Mao. Wei Shen, Jianyong Wang, and Jiawei Han. 2015. Data\nEng., 27(2):443\u2013460. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. Jana Strakov\u00e1, Milan Straka, and Jan Hajic. Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Xian-\ngrong Zeng, and Shengping Liu. Dianbo Sui, Chenhao Wang, Yubo Chen, Kang Liu, Jun\nZhao, and Wei Bi. Deep learning in spiking neural net-\nworks. 2015. David Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. Chunqi Wang, Ji Zhang, and Haiqing Chen. 2018. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. In\nFindings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021 , volume ACL/IJCNLP 2021 of Findings\nof ACL , pages 1405\u20131418. Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen\nLiu, Hongsong Zhu, and Limin Sun. Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and\nYi Chang. Xin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui\nChen, Feiyu Xiong, Mosha Chen, and Huajun Chen. ACM.Canwen Xu and Julian J. McAuley. CoRR , abs/2202.07101. Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng\nZhang. Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng\nZhang, and Xipeng Qiu. Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha\nChen, Chuanqi Tan, Fei Huang, and Huajun Chen. In Thirty-Fifth AAAI Conference\non Artificial Intelligence, AAAI 2021, Thirty-Third\nConference on Innovative Applications of Artificial\nIntelligence, IAAI 2021, The Eleventh Symposium\non Educational Advances in Artificial Intelligence,\nEAAI 2021, Virtual Event, February 2-9, 2021 , pages\n14257\u201314265. AAAI Press. Haiyang Yu, Ningyu Zhang, Shumin Deng, Hongbin Ye,\nWei Zhang, and Huajun Chen. Dmitry Zelenko, Chinatsu Aone, and Anthony\nRichardella. J. Mach. Res. Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Daojian Zeng, Haoran Zhang, and Qianying Liu. AAAI Press.15Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu,\nShengping Liu, and Jun Zhao. Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,\nand Jun Zhao. 2018. Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and\nHaipeng Ding. AI Open ,\n2:14\u201335. Ningyu Zhang, Shumin Deng, Zhen Bi, Haiyang Yu,\nJiacheng Yang, Mosha Chen, Fei Huang, Wei Zhang,\nand Huajun Chen. Ningyu Zhang, Hongbin Ye, Shumin Deng, Chuanqi\nTan, Mosha Chen, Songfang Huang, Fei Huang, and\nHuajun Chen. IEEE ACM Trans. Ranran Haoran Zhang, Qianying Liu, Aysa Xuemo Fan,\nHeng Ji, Daojian Zeng, Fei Cheng, Daisuke Kawa-\nhara, and Sadao Kurohashi. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020 , volume EMNLP 2020\nofFindings of ACL , pages 236\u2013246. Shuai Zhang, Yongliang Shen, Zeqi Tan, Yiquan Wu,\nand Weiming Lu. Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing\nHao, Peng Zhou, and Bo Xu. 2017. Yucheng Zhou, Tao Shen, Xiubo Geng, Guodong Long,\nand Daxin Jiang. Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang,\nPenglei Sun, Xuwu Wang, Yanghua Xiao, and\nNicholas Jing Yuan.",
        "Conclusion": "In this study, we\nsummarize the recent compelling progress in\ngenerative knowledge graph construction. Final Results:\u00a0\n {United states, Country-President ,\u00a0Joe Biden }\nInput T ext: The\u00a0 \u00a0 United States\u00a0President\u00a0 Joe Biden\u00a0visited\u00a0Samsung\u00a0 \u00a0. We review the research with dif-\nferent generation targets for KGC with a com-\nprehensive comparison and summary (\u00a73). Zhu et al. Zeng et al. (2018) designs an end-to-end\nmodel based on a copy mechanism to solve the\nThe man returned to Los Angeles from Mexico following\nhis capture T uesday by bounty hunters.Generation Model((Transport  returned (Artifact The man) (Destination Los\nAngeles) (Origin Mexico))  \n(Arrest-Jail  capture (Person The man) (T ime T uesday)\n(Agent bounty hunters))Root\nTransport Arrest-Jail\nArtifact\nDestinationOriginreturned\n... Lu et al. Lou et al. Besides, Zhang et al. (2021b); Ye et al. Similarly, Zhang et al. (2022);\nLu et al. , b i, ei, ti, . Besides, Yan et al. Du et al. Li et al. The output se-\nquences are sentences where the blank placeholders\nare replaced by specific event arguments. Besides,\nHsu et al. Sui et al. Sui et al. Zhang et al. To sum up, the non-autoregressive\napproach applied to KGC proves to be effective in\nsolving the exposure bias and overfitting problems. 6 Conclusion and Vision\nIn this paper, we provide an overview of generative\nKGC with new taxonomy, theoretical insight and\nempirical analysis, and several research directions. 2021. 2020a. 2020b. 2022. 2022. 2021. 2019. Tom B. 2020. 2021. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November,\n2021 , pages 2370\u20132381. 2021. 2021. 2022a. 2022b. 2022c. 2022d. ACM. 2022e. ACM. Linguistics , 4:357\u2013370. 2019. 2020. 2019. 2021. 2021. 2021. 2020. 2021a. 2021b. Andrea Galassi. 2021. 2022. 2022. 2022. 2022. 2021. 2022. IEEE Trans. Neural Networks Learn. Syst. 2019. 2020. 2021. 2022. In Findings of\nthe Association for Computational Linguistics: ACL2022, Dublin, Ireland, May 22-27, 2022 , pages 3696\u2013\n3711. 2020. 2022a. 2020a. 2022b. 2020b. 2021a. 2021b. 2021. 2022c. 2020. 2020. Event extraction as machine reading com-\nprehension. 2021a. 2022. 2021b. GPT\nunderstands, too. 2021. 2021. 2022. 2019. 2022. 2008. 2021. 2020. Pablo Negro and Claudia Pons. 2022. 2021. 2022. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\nDublin, Ireland, May 22-27, 2022 , pages 2912\u20132924. 2020. Learn. 2021. 2020. 2022. 2003. ACL. 2022. Sequence-to-sequence knowledge graph com-\npletion and question answering. 2022. IEEE Trans. Knowl. 2019. PMLR. 2019. 2020. 2021. 2019. Neural Networks , 111:47\u201363.14Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2019. 2022. InFindings of the Association for Computational\nLinguistics: ACL 2022, Dublin, Ireland, May 22-27,\n2022 , pages 803\u2013823. 2021. 2020. 2020. 2022. 2022. 2021a. 2021b. 2021. 2020. 2003. Learn. , 3:1083\u20131106. 2020. 2019. 2021a. 2020a. 2022a. 2021b. Audio Speech Lang. , 29:3077\u20133088. 2020b. 2022b. 2022. 2020. 2022.",
        "label": 2
    },
    {
        "Abstract": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 18\u201329\nDecember 7-11, 2022 \u00a92022 Association for Computational Linguistics\nCDC ONV : A Benchmark for Contradiction Detection in\nChinese Conversations\nChujie Zheng1\u2217Jinfeng Zhou1,2\u2217Yinhe Zheng3Libiao Peng3Zhen Guo4\nWenquan Wu4Zheng-Yu Niu4Hua Wu4Minlie Huang1,3\u2020\n1The CoAI Group, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems,\n1Beijing National Research Center for Information Science and Technology, DCST, Tsinghua University, Beijing 100084, China\n2College of Intelligence and Computing, Tianjin University, Tianjin, China\n3Lingxin AI, Beijing 100084, China4Baidu Inc., China\nchujiezhengchn@gmail.com jfzhou.mail@gmail.com aihuang@tsinghua.edu.cn\n{guozhenguozhen, wuwenquan01, niuzhengyu, wu_hua}@baidu.com\nAbstract\nDialogue contradiction is a critical issue in\nopen-domain dialogue systems. \ud835\udc4f2: \u4e0d\u77e5\u9053,\u4f60\u8981\u544a\u8bc9\u6211\u4e48? Besides the language differ-\nence, CDC ONV is distinguished from DECODE in\ntwo aspects: (1) Apart from History Contradiction,\nCDC ONV additionally covers two contradiction\ncategories: Intra-sentence Contradiction and Role\nConfusion, which are also typical and common\nin human-bot conversations (\u00a73). \ud835\udc4f1: \u6211\u559c\u6b22\u72d7,\u4f60\u5462? (I don\u2019t. (You should usually be quite free, right?) This work does not use any\ndemographic or identity characteristics. arXiv\npreprint arXiv:2109.09519 . ases may not be eliminated completely.",
        "Methodology": "In this work, we propose a bench-\nmark for Contradiction Detection in Chinese\nConv ersations, namely CDC ONV . To efficiently construct\ntheCDC ONV conversations, we devise a series\nof methods for automatic conversation gener-\nation, which simulate common user behaviors\nthat trigger chatbots to make contradictions. We conduct careful manual quality screening\nof the constructed conversations and show that\nstate-of-the-art Chinese chatbots can be eas-\nily goaded into making contradictions. 1Our data and codes are available at https:\n//www.github.com/thu-coai/CDConv and\nhttps://github.com/PaddlePaddle/Knover/\ntree/dygraph/projects/cdconv\n Non-contradiction \ud835\udc621: \u4f60\u559c\u6b22\u5403\u9762\u6761\u5417\uff1f(Do you like noodles?) \ud835\udc622: \u4f60\u4e0d\u559c\u6b22\u9762\u6761\u5417? I like all the other animals)  {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u90a3\u4f60\u559c\u6b22\u770b\u4ec0\u4e48\u7c7b\u578b\u7684\u7535\u5f71\uff1f\", \"b1\": \"\u79d1\u5e7b,\u52a8\u4f5c,\u6050\u6016,\u559c\u5267\u7247\u90fd\u53ef\u4ee5\", \"u2\": \"\u4f60\u4e0d\u559c\u6b22\u4ec0\u4e48\u7c7b\u578b\u7684\u7535\u5f71\uff1f\", \"b2\": \"\u6050\u6016,\u8840\u8165,\u8840\u8165\u66b4\u529b\u7684,\u8fd8\u6709\u6076\u5fc3\u7684\", \"label\": 3, \"persona\": 2} {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u90a3\u4f60\u559c\u6b22\u770b\u4ec0\u4e48\u7c7b\u578b\u7684\u7535\u5f71\uff1f\", \"b1\": \"\u79d1\u5e7b,\u52a8\u4f5c,\u6050\u6016,\u559c\u5267\u7247\u90fd\u53ef\u4ee5\", \"u2\": \"\u4f60\u4e0d\u559c\u6b22\u4ec0\u4e48\u7c7b\u578b\u7684\u7535\u5f71\uff1f\", \"b2\": \"\u4e0d\u559c\u6b22\u97e9\u56fd\u7535\u5f71\", \"label\": 0} {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u90a3\u4f60\u559c\u6b22\u770b\u4ec0\u4e48\u7c7b\u578b\u7684\u7535\u5f71\uff1f\", \"b1\": \"\u79d1\u5e7b,\u52a8\u4f5c,\u6050\u6016,\u559c\u5267\u7247\u90fd\u53ef\u4ee5\", \"u2\": \"\u4f60\u4e0d\u559c\u6b22\u4ec0\u4e48\u7c7b\u578b\u7684\u7535\u5f71\uff1f\", \"b2\": \"\u6211\u4e0d\u559c\u6b22\u5267\u60c5\u7247\", \"label\": 0}  {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u4f60\u96be\u9053\u4e0d\u559c\u6b22\u5979\u5417\uff1f\", \"b1\": \"\u4e0d\u559c\u6b22,\u6211\u6709\u559c\u6b22\u7684\u4eba\", \"u2\": \"\u4f60\u559c\u6b22\u5979\u5417\uff1f\", \"b2\": \"\u4e0d\u559c\u6b22\u6211\u8fd8\u8fd9\u4e48\u7ea0\u7ed3\u554a? \", \"label\": 3, \"persona\": 2} {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u4f60\u96be\u9053\u4e0d\u559c\u6b22\u5979\u5417\uff1f\", \"b1\": \"\u4e0d\u559c\u6b22,\u6211\u6709\u559c\u6b22\u7684\u4eba\", \"u2\": \"\u4f60\u559c\u6b22\u5979\u5417\uff1f\", \"b2\": \"\u6211\u4e5f\u4e0d\u77e5\u9053,\u6211\u611f\u89c9\u6211\u4e0d\u559c\u6b22\u4ed6\u4e86\", \"label\": 1} {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u4f60\u96be\u9053\u4e0d\u559c\u6b22\u5979\u5417\uff1f\", \"b1\": \"\u4e0d\u559c\u6b22,\u6211\u6709\u559c\u6b22\u7684\u4eba\", \"u2\": \"\u4f60\u559c\u6b22\u5979\u5417\uff1f\", \"b2\": \"\u4e0d\u559c\u6b22,\u4e0d\u8ba8\u538c\", \"label\": 0} {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u4f60\u96be\u9053\u4e0d\u559c\u6b22\u5979\u5417\uff1f\", \"b1\": \"\u4e0d\u559c\u6b22,\u6211\u6709\u559c\u6b22\u7684\u4eba\", \"u2\": \"\u4f60\u559c\u6b22\u5979\u5417\uff1f\", \"b2\": \"\u4e0d\u559c\u6b22,\u53ea\u662f\u6709\u597d\u611f\", \"label\": 0}  {\"model\": \"plato\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u4f60\u559c\u6b22\u5e72\u4ec0\u4e48\u5e73\u65f6\uff1f\", \"b1\": \"\u73a9\u6e38\u620f,\u901b\u8857,\u770b\u7535\u5f71\", \"u2\": \"\u4f60\u4e0d\u559c\u6b22\u505a\u4ec0\u4e48\uff1f\", \"b2\": \"\u4e0d\u559c\u6b22\u901b\u8857,\u592a\u7d2f\u4e86\", \"label\": 3, \"persona\": 2}  {\"model\": \"eva\", \"method\": \"\u53cd\u4e49-\u5426\u5b9a\u8bcd\", \"u1\": \"\u90a3\u4f60\u4e0b\u73ed\u4e4b\u540e\u6700\u559c\u6b22\u5e72\u7684\u4e8b\u662f\u4ec0\u4e48\uff1f\", \"b1\": \"\u5403\u996d\u7761\u89c9\u6253\u8c46\u8c46\", \"u2\": \"\u4e0b\u73ed\u540e\u4f60\u6700\u4e0d\u559c\u6b22\u505a\u4ec0\u4e48\uff1f\", \"b2\": \"\u4e0d\u559c\u6b22\u7761\u89c9\uff0c\u4e5f\u4e0d\u559c\u6b22\u5403\u4e1c\u897f\", \"label\": 3, \"persona\": 2}      Figure 1: Dialogue contradiction detection requires the\nfull contextual information (including u1andu2) rather\nthan only the bot\u2019s utterances (i.e., b1andb2). Specifically, the contextualization nature of con-\nversations indicates the necessity of considering\nand modeling contextual information. However, given u1, the\nactual meaning of b1should be \u201c I like dogs, cats\u201d\nandb1andb2are thus contradictory. The\nExtrinsic type targets the contradiction between a conversation and external information (e.g., profiles or facts),\nwhile Intrinsic targets the contradiction inside a conversation. In this work, we propose a benchmark for\nContradiction Detection in Chinese Conv ersations,\nnamely CDC ONV . It contains 12K multi-turn con-\nversations with human-annotated contradiction la-\nbels (\u00a73). Different from previous work (e.g., Nie\net al. 2021), we\nautomatically construct the CDC ONV conversa-\ntions combined with elaborate manual inspection\n(\u00a74.1). Specifically, we first devise a series of au-\ntomatic methods to generate conversations (\u00a74.2),\nwhich simulate the common user behaviors that\ntrigger chatbots to make contradictions. We then\nconduct careful human screening and annotation\nfor the constructed conversations to ensure the data\nquality (\u00a74.3). However,\nthere is still much room for future research in di-\nalogue modeling, integrating commonsense and\nworld knowledge, and reasoning. Our contributions are summarized as follows:\n\u2022We propose CDC ONV, a benchmark for contra-\ndiction detection in Chinese conversations. \u2022We present a series of methods by simulating\ncommon user behaviors to automatically trig-\nger chatbots to make contradictions. We demon-\nstrate the effectiveness of these trigger methods\nthrough detailed human annotation. (No, I mean you can come and see me!) \ud835\udc622: \u4f60\u6709\u5b69\u5b50\u5417? \ud835\udc4f1: \u5f00\u5b66\u521d\u4e09,\u4f60\u5462? \u628a\u201c\u6211\u4e5f\u662f\u201d\u5220\u9664  {\"file\": \"5nd\", \"model\": \"eva\", \"method\": \"\u540c\u4e49-\u56de\u8bd1\", \"u1\": \"\u4f60\u90fd\u6709\u5b69\u5b50\u4e86\u5417\uff1f\", \"b1\": \"\u6ca1\u6709\u554a\uff0c\u6211\u662f\u8bf4\u4f60\u6765\u627e\u6211\u73a9! \", \"label\": 1} {\"file\": \"5nd\", \"model\": \"eva\", \"method\": \"\u540c\u4e49-\u56de\u8bd1\", \"u1\": \"\u4f60\u597d\uff0c\u5728\u5fd9\u4e48\uff1f\", \"b1\": \"\u8fd8\u597d\uff0c\u4f60\u5462\uff1f\", \"u2\": \"\u4f60\u597d\uff0c\u4f60\u5fd9\u5417\uff1f\", \"b2\": \"\u4e0d\u5fd9\uff0c\u53ea\u662f\u5f88\u5fd9! Red texts denote the parts that\nmake contradiction. Blue texts illustrate the trigger methods (i.e., how u2are constructed). CMNLI (Xu et al., 2020) and OCNLI (Hu et al.,\n2020) datasets. (2019) constructed the DNLI dataset where the\ndialogue utterances and the persona descriptions\nfrom PersonaChat (Zhang et al., 2018) are used\nto form sentence pairs. (2019) simi-\nlarly synthesized the InferConvAI dataset through\nautomatic manipulation with dialogue utterances. One potential\nlimitation of Extrinsic dialogue contradiction de-\ntection is that it may rely on static and manually\ncurated external information (e.g., profiles), which\ncould be insufficient in open-domain dialogue. (2) Instead of\nbeing human-written, the contradiction cases in\nCDC ONV are constructed by simulating the user\nbehaviors that trigger chatbots to make contradic-\ntions (\u00a74.2), which are closer to the real scenario\nof human-bot conversation.20\ud835\udc4f\ud835\udc5b\ud835\udc62\ud835\udc5b\ud835\udc4f\ud835\udc5b\u2212!\ud835\udc62\ud835\udc5b\u2212! \u2192\nIntra\n-\nsentence\nContradiction\n\u2192\nRole\nConfusion\n\u2192\nHistory\nContradiction\u2192DialogFlow\nFigure 3: Diagram of contradiction categories. Combine\nthe definitions below for a clearer understanding. , u n, bn, where ukandbkdenote the\nkth-turn utterances from the user and the chatbot\nrespectively. In the preliminary study, we manually inspected\n200 multi-turn human-bot conversations with two\nChinese open-domain chatbots: EV A (Zhou et al.,\n2021; Gu et al., 2022) and PLATO (Bao et al.,\n2021a,b). However, if bnmakes\nRole Confusion (i.e., bnis more likely to be a user\u2019s reply to\nbn\u22121than a bot\u2019s reply to un), the premise of History Contra-\ndiction will not hold and such a case will be judged as Role\nConfusion rather than History Contradiction. :NoIntra-sentenceContradiction?\u00d8Annotate\ud835\udc4f\":\n(1)\nIntra\n-\nsentence,\n(2)\nRole,\n(3)\nHistory,\n(4)\nIncoherent(if\ud835\udc62\"isquestioning)\u00d8Annotatepersonalabelsif\n(3)Construct\ud835\udc62!Figure 4: The collection procedure of CDC ONV. CDC ONV can serve\nas a good start point for investigating them. 4 Data Collection\n4.1 Collection Procedure\nWe automatically constructed the CDC ONV con-\nversations along with elaborate manual inspection. We narrow down the conversations in CDC ONV to\n2-turn ones ( n= 2). 2.Using one of the trigger methods in \u00a74.2, we\nautomatically constructed u2based on u1orb1\nand generated the chatbot\u2019s next reply b2. and are fluent and understandable,\nand (2) if b1does not make Intra-sentence Con-\ntradiction (to ensure a valid dialogue history). The unqualified conversations were removed.21Methodsu2Not\nFluentEV A PLATO\nb1\nIntrab2 b1\nIntrab2\nIntra Role History Incoh Intra Role History Incoh\nShort - 0.04 0.00 0.14 0.04 0.00 0.01 0.01 0.27 0.03 0.00\nInquiring (Bot) 0.19 0.08 0.09 0.02 0.31 0.03 0.03 0.03 0.10 0.17 0.08\nInquiring (User) 0.16 0.04 0.03 0.06 0.31 0.16 0.01 0.01 0.12 0.22 0.22\nInquiring (User-M) 0.13 0.02 0.06 0.00 0.62 0.01 0.01 0.03 0.03 0.43 0.09\nParaphrasing 0.06 0.06 0.07 0.01 0.24 0.00 0.02 0.02 0.07 0.21 0.05\nPerturb (Synonym) 0.22 0.05 0.08 0.00 0.25 0.02 0.02 0.02 0.05 0.18 0.13\nPerturb (Antonym) 0.39 0.06 0.08 0.01 0.32 0.07 0.01 0.03 0.03 0.16 0.10\nPerturb (Negative) 0.31 0.05 0.10 0.01 0.28 0.03 0.02 0.04 0.04 0.15 0.08\nMacro-Average 0.21 0.05 0.06 0.03 0.30 0.04 0.02 0.02 0.09 0.19 0.09\nTable 2: Annotation statistics for each trigger method. The proportions about b2are calculated after the unqualified conversations were filtered out (in the\n3rd step in \u00a74.1). The proportions of ethical risk and non-fluent b1, b2are omitted since they are all close to 0. 4.Considering the full contextual information, hu-\nman annotators marked whether b2makes a con-\ntradiction based on the categories in \u00a73. That\nis, according to the order in \u00a73, once a con-\ntradiction of some category is recognized, the\nsubsequent categories will not be judged. Note\nthat the cases, where b2does not answer the\nquestioning u2and responds incoherently (e.g.,\nunnaturally transition the topic), were addition-\nally marked and filtered out. Collecting u1We collected the human-written\nutterances from DuPersona, a crowd-sourced Chi-\nnese open-domain dialogue corpus3. We used\nthose utterances that contain second-person nouns\nand \u201c?\u201d as u1, since noticed that such question-\ning utterances would elicit chatbots to talk specific\ninformation about themselves and could avoid un-\ninformative or meaningless replies. They are both pre-trained on\nmassive Chinese social media corpora. 4.2 Trigger Methods\nOur inspection on contradiction cases (\u00a73) also re-\nvealed that chatbots are more prone to making con-\ntradictions under several specific user behaviors:\n(1) the user input is short and uninformative, (2) the\nuser inquires about the dialogue history (similarly\nnoticed by Li et al. 2021), and (3) the user asks for\nsimilar information in the context. By simulating\nthese user behaviors, we devise a series of methods\nto automatically construct u2. Note that the automatic construction of u2suggests\nthe necessity of inspecting if it is fluent and under-\nstandable, which is thus an important step to ensure\ndata quality (\u00a74.1). \u00d8 (Modified) \ud835\udc622: \u4f60\u77e5\u9053\u83ca\u82b1\u5728\u4ec0\u4e48\u5b63\u8282\u5f00\u653e\u5417? It simulates a user\u2019s in-\nquiry about the contents of previous conversations. We first extracted named entities in b1(about the\nbot) or u1(about the user) using HanLP4(He and\nChoi, 2021). Then we leveraged an open-sourced\nquestion generation model5to generate questions\nabout the extracted entities, which were used as u2. Paraphrasing u2expresses the same meaning to\nu1in a different way. It simulates a user\u2019s clarifica-\ntion question to the previous questions. We paraphrased u1through back-translation as\nu2. The Chinese u1was first translated to English\nand then back-translated to Chinese. We used the\nBaidu translation API and removed those u2that\nwere identical to u1. We perturbed u1in three ways. For Synonym and\nAntonym, there are 2.3/3.7 words per u1on average\nthat can be replaced with their synonyms/antonyms. We randomly\nreplaced the words in u1with their negatives using\nthe negative dictionary or inserted negatives before\nthe verbs in u1. Since we noticed that negatives\nwould greatly impair the fluency of u2, we addi-\ntionally applied back-translation to u2to improve\nits fluency. However, due to the characteristics of the Chinese\nlanguage and the difference in individual habits of\nlanguage usage and communication, the annotation\ncriteria of the annotators may somewhat vary and\nneed to be calibrated with our assistance. We ap-\nplied the following mechanisms for quality control:\nAnnotator Training All the annotators were re-\nquired to take a training tutorial, which consists of\n50 conversations for pilot annotation. We provided\nfeedback to help them calibrate the annotation cri-\nteria. Multi-person Annotation In the formal annota-\ntion, each conversation was annotated by two dif-\nferent annotators. Spot Check To more effectively calibrate the an-\nnotation criteria, we conducted annotation batch\nby batch and randomly sampled 100 conversations\neach batch for spot check. We provided feedback to\nthe annotators and instructed them to amend their\nannotations. After each revision we would conduct\nspot check again until the pass rate reached 95%. Except for the first two batches,\nall subsequent batches directly passed the first spot\nchecks. It\ncontains 11,660 conversations, where the average\nlengths of u1, b1, u2, b2are 16.4, 12.1, 11.1, 11.6\nrespectively. The ratio of positive and negative\nsamples is 1.68 (7,309 / 4,351). Both positive and\nnegative samples include conversations constructed\nusing various trigger methods, which suggests a\nhigh diversity of CDC ONV. To shed light on the trigger methods and the chat-\nbot behaviors, we show in Table 2 the comprehen-\nsive annotation statistics. For the trigger methods ,\nthey all can effectively trigger dialogue contradic-\ntions. EV A is more prone to making Intra-sentence Con-\ntradiction ( b1/b2) and History Contradiction ,\nwhile PLATO makes more Role Confusion and\nincoherent b2. We speculate that their different\nbehaviors may result from the gaps in model archi-\ntectures and training corpora. Yes\nYes\nYesNon-contradictionNoNoNoFigure 6: Overview of the Hierarchical method. 5 Experiments\n5.1 Setups\nWe randomly split CDC ONV into the train-\ning/validation/test sets with the ratio of 6/2/2. The\n2-class one detects whether b2makes a contradic-\ntion, while the 4-class one recognizes the contra-\ndiction category (the three categories in \u00a73 along\nwith a non-contradiction one). We measure model\nperformance using Accuracy andMacro-F1 . 5.2 Compared Methods\nWe experimented with three popular Chinese pre-\ntrained models: BERT, RoBERTa (Cui et al., 2021)\nand ERNIE (Sun et al., 2019). The BERT and RoBERTa are both\npre-trained with whole word masking while ERNIE\nwith the different knowledge masking strategies. This method follows\nthe NLI framework adopted in previous work\n(Williams et al., 2018; Welleck et al., 2019; Nie\net al., 2021) where contradiction detection is per-\nformed between a pair of sentences. \u2022Flatten : The flattened whole conversation is\ntaken as the model input, that is, u1, b1, u2and\nb2. This method utilizes contextual information\nfor contradiction detection in a naive way. Each classifier targets a specific\ncontradiction category, takes the corresponding\nlevel of contextual information as input, and is\nfine-tuned with 2-class samples: the samples of\nthe targeted contradiction category vs. all the\nother samples. Once some contradiction category\nis detected, it is then directly output, otherwise\nnon-contradiction will be finally output. In prior to fine-tuning, we pre-trained all the\nmodels on the Chinese NLI pre-training corpus,\nwhich includes two widely used Chinese NLI\ndatasets: CMNLI (Xu et al., 2020) and OCNLI\n(Hu et al., 2020). We merged the \u201centailment\u201d and\n\u201cneutral\u201d labels as the \u201cnon-contradiction\u201d one. We employed\nthe AdamW (Loshchilov and Hutter, 2018) opti-\nmizer with batch size 32 and learning rate 5e-5, and\nused the linear learning rate scheduler with warmup\nproportion 0.1. Each model was fine-tuned for 5\nepochs and the checkpoint achieving the highest\nMacro-F1 was used for test. (2) Hierarchical consistently performs best and\nboosts all the fine-grained results. Their highest Macro-F1 are\n33.0 and 49.5 respectively, which are far inferior\nto Non-contradiction (85.1) and History Contradic-\ntion (71.0). \ud835\udc622: \u4f60\u4e0d\u559c\u6b22\u52a8\u7269\u5417? \ud835\udc621: \u4f60\u5e73\u5e38\u5e94\u8be5\u633a\u6e05\u95f2\u5427? There are 10 hours between 6.30 pm and 8.30 am. Another important\nreason may be that these pre-trained models still do\nnot have a good ability of dialogue representation,\nwhich may be alleviated by additional pre-training\non dialogue corpora. 5.5 Error Analysis and Discussion\nWe manually inspected the cases misclassified by\nthe four RoBERTa Hierarchical models (trained\nwith four random seeds). Though innate to human, these capabilities are still\nlargely lacked by even gigantic deep neural mod-\nels (Marcus, 2018; Choi, 2022). These challenges\nof dialogue contradiction detection manifest that\nfurther exploration is worthy. By simulating common user behaviors that\ntrigger chatbots to make contradictions, we col-\nlect 12K conversations annotated with three typical\ncontradiction behaviors. We did not\ndirectly contact the annotators and their privacy\ncan be well preserved. The u2utterances either\ncome from DuPersona or are constructed using pub-\nlicly available resources (corpora, models or API,\n\u00a74.2). Due to the potential ethical risks in\nthese utterances, we have censored and filtered out\nconversations that contained unsafe or unethical\ncontents through human inspection. Acknowledgements\nThis work was supported by the National Sci-\nence Foundation for Distinguished Young Scholars\n(with No. 61936010 and regular project with\nNo. This work was also supported\nby the Guoqiang Institute of Tsinghua University,\nwith Grant No. 2019GQG1 and 2020GQG0005,\nand sponsored by Tsinghua-Toyota Joint Research\nFund. PLATO: Pre-trained dialogue genera-26tion model with discrete latent variable. PLATO-2: Towards building an open-\ndomain chatbot via curriculum learning. Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao,\nAida Amini, Qazi Mamunur Rashid, Mike Green,\nand Kelvin Guu. In ICML . In NAACL . Daniel De Freitas, Minh-Thang Luong, David R. So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V . Logic and conversation. Eva2.0:\nInvestigating open-domain chinese dialogue sys-\ntems with large-scale pre-training. arXiv preprint\narXiv:2203.09313 . Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. arXiv preprint\narXiv:2202.03629 . Empiri-\ncal study on blenderbot 2.0 errors analysis in terms\nof model, data and user-centric approach. Decoupled\nweight decay regularization. In ICLR . CI-ToD: Towards Benchmark-\ning Consistency for Task-oriented Dialogue System. InEMNLP . In EACL . In NAACL . Haoyu Song, Yan Wang, Wei-Nan Zhang, Zhengyu\nZhao, Ting Liu, and Xiaojiang Liu. Pro-\nfile Consistency Identification for Open-domain Dia-\nlogue Agents. arXiv preprint\narXiv:1904.09223 .27Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser, and Illia Polosukhin. Sean Welleck, Jason Weston, Arthur Szlam, and\nKyunghyun Cho. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In NAACL . open-domain conversation with\nlong-term persona memory. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and William B. Dolan. arXiv preprint\narXiv:2202.13047 . arXiv preprint\narXiv:2108.01547 . For this reason, we (1) focus\non the three typical contradiction categories in the\nmanually inspected contradiction cases (\u00a73), and\n(2) construct conversations by simulating common\nuser behaviors that trigger contradictions. It indi-\ncates that these methods may somewhat impair the\nnaturalness of conversations. To address this, we\nconducted elaborated manual inspection (the 3rd\nand 4nd steps in \u00a74.1) to filter out the conversations\ncontaining non-fluent or incoherent replies. To alleviate this,\nwe have adopted the mode of multi-person anno-\ntation, conducted spot check for each annotation\nbatch, and required the pass rates to reach 95% to\nensure data quality (\u00a74.3). We especially point out\nthat, despite the mode of multi-person annotation,\nthere may still exist biases in the annotation results\nregarding \u201cfluency\u201d (\u00a74.1). Note that the last line of each model corresponds to the results of Sentence Pair in Table 4.",
        "Results and Findings": "The con-\ntextualization nature of conversations makes\ndialogue contradiction detection rather chal-\nlenging. Unfortunately, state-of-\nthe-art open-domain chatbots, such as BlenderBot\n(Roller et al., 2021), EV A (Zhou et al., 2021; Gu\net al., 2022) and PLATO (Bao et al., 2021b), still of-\nten behave inconsistently with their role or identity\nand produce utterances that are self-contradictory\n\u2217Equal contribution. \u2020Corresponding author. Despite the above challenge, existing datasets for\ncontradiction detection (Dziri et al., 2019; Welleck18Lang Task Input Task Type Contradiction Categories\nMNLI (2018) En Sentence Pair - -\nCMNLI (2020), OCNLI (2020) Zh Sentence Pair - -\nDNLI (2019), InferConvAI (2019) En Sentence Pair - -\nKvPI (2020) Zh Conversation & Profile Extrinsic Profile\nDIALFACT (2022) En Conversation Extrinsic Fact\nCI-ToD (2021) En Conversation & KB Int & Ext Query, History & KB\nDECODE (2021) En Conversation Intrinsic History\nCDC ONV (Ours) Zh Conversation Intrinsic Intra-sentence, Role, History\nTable 1: Comparison of CDC ONV with related benchmarks / datasets for (dialogue) contradiction detection. et al., 2019) usually only consider the textual entail-\nment relationship between two isolated sentences\n(Dagan et al., 2005), which is largely insufficient\nfor dialogue contradiction detection due to the ne-\nglect of contextual information. Results show that prop-\nerly modeling contextual information is critical\nfor dialogue contradiction detection. \u2022We evaluate popular Chinese pre-trained models\nonCDC ONV. Results show the importance of\nproperly modeling contextual information in dia-\nlogue contradiction detection, while this task is\nstill far from solved and requires further study. \ud835\udc4f1: \u6ca1\u6709\u554a,\u6211\u662f\u8bf4\u4f60\u6765\u627e\u6211\u73a9! \ud835\udc4f1: \u5403\u4e86\u5440,\u90fd\u5feb\u5230\u516c\u53f8\u4e86 (Yes, I\u2019m about to arrive at the company) \ud835\udc622: \u4f60\u5403\u65e9\u9910\u4e86\u5417? \", \"u2\": \"\u4f60\u6709\u5b69\u5b50\u5417\uff1f\", \"b2\": \"\u6ca1\u6709\uff0c\u6211\u53ea\u6709\u4e00\u4e2a\u5973\u513f! . . That is, bnis more likely to be a user\u2019s reply to\nbn\u22121rather than a bot\u2019s to un. respectively. Each value means the proportion of the corresponding\nannotation label. Specifi-\ncally, we adopted single-label annotation. This is due\nto our observation that these crowd-sourced utter-\nances are of higher quality compared to social me-\ndia posts (e.g., Weibo and Douban) and contain\nrich persona information, which is in line with the\nstyle and content of general chitchat. If their results were inconsistent,\na third annotator would be asked to re-annotate and\ndiscuss the case with the first two annotators to\nreach a consensus. 6https://github.com/guotong1988/\nchinese_dictionary23EV A PLATO Total\n# Conversations 5,458 6,202 11,660\n# Positive 3,233 4,076 7,309\n# Negative 2,225 2,126 4,351\nTrigger Methods (Positive / Negative Samples)\n# Short 429 / 91 692 / 304 1,121 / 395\n# Inquiring (Bot) 764 / 577 845 / 406 1,609 / 983\n# Inquiring (User) 127 / 116 131 / 106 258 / 222\n# Inquiring (User-M) 251 / 552 477 / 541 728 / 1,093\n# Paraphrasing 962 / 448 846 / 389 1,808 / 837\n# Perturb (Synonym) 288 / 145 376 / 147 664 / 292\n# Perturb (Antonym) 185 / 143 319 / 103 504 / 246\n# Perturb (Negative) 227 / 153 390 / 130 617 / 283\nContradiction Categories (of Negative Samples)\nIntra-sentence 17.3% 6.8% 12.2%\nRole 5.8% 29.9% 17.6%\nHistory 76.9% 63.3% 70.2%\nPersona Labels (of History Contradiction)\nAttributes 48.8% 46.2% 47.7%\nOpinions 22.2% 20.7% 21.5%\nExperiences 26.3% 31.5% 28.6%\nUnrelated 2.7% 1.6% 2.2%\nTable 3: Statistics of CDC ONV. We thus design a hierarchical\nmodeling method, which consists of three sepa-\nrately fine-tuned 2-class classifiers in sequential24Models Methods2-class 4-class 4-class (Fine-grained F1)\nAcc F1 Acc F1 Non Intra Role History\nBERTSentence Pair 75.3 73.8 72.3 54.5 81.0 24.0 48.5 64.4\nFlatten77.6 75.8 73.6 54.6 81.8 28.5 38.8 69.1\n+2.3 +2.0 +1.3 +0.1 +0.8 +4.6 -9.7 +4.7\nHierarchical77.9 75.9 75.2 56.6 83.1 30.0 44.2 68.9\n+2.6 +2.1 +3.0 +2.1 +2.1 +6.0 -4.3 +4.5\nRoBERTaSentence Pair 75.7 73.7 72.2 55.1 81.2 29.1 46.5 63.4\nFlatten78.6 77.0 75.7 56.8 84.1 28.8 43.3 70.9\n+2.9 +3.2 +3.4 +1.7 +2.8 -0.3 -3.2 +7.5\nHierarchical80.4 78.1 77.8 59.3 85.1 33.0 48.1 71.0\n+4.7 +4.4 +5.5 +4.3 +3.9 +3.9 +1.7 +7.6\nERNIESentence Pair 77.5 75.7 75.0 56.9 83.3 28.7 48.9 66.8\nFlatten78.6 76.7 75.8 56.6 83.8 30.9 41.0 70.8\n+1.1 +1.0 +0.8 -0.3 +0.5 +2.2 -7.8 +4.0\nHierarchical79.6 77.5 76.6 59.0 84.3 32.7 49.5 69.6\n+2.1 +1.8 +1.7 +2.1 +1.1 +4.0 +0.6 +2.8\nTable 4: Experimental results. We reported the aver-\nage results of four random seeds, where each run\ntook about 3 minutes on a single Tesla V100 GPU. 5.4 Results\nTable 4 shows the results of the 2-class setting, the\n4-class setting, and the fine-grained F1 scores of allthe categories of the 4-class setting. We have three\nmajor observations:\n(1) Sentence Pair performs worse than Flatten\nand Hierarchical. \ud835\udc4f1: \u662f\u554a,\u6211\u662f\u56de\u65cf\u7684 (Yes, I\u2019m from Hui nationality) \ud835\udc622: \u4f60\u662f\u56de\u65cf\u4eba\u58eb\u5417? Yulin is a city in Shaanxi, China. Figure 7 shows the results\nof error analysis. Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang,\nWenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao\nXu. Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang,\nWenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinx-\nian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin,\nand Zhengyu Niu. Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and\nZiqing Yang. Ido Dagan, Oren Glickman, and Bernardo Magnini. Dialog inpainting: Turning\ndocuments into dialogs. Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and\nOsmar Zaiane. Nouha Dziri, Andrea Madotto, Osmar Za\u00efane, and\nAvishek Joey Bose. arXiv preprint arXiv:2001.09977 . Yuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke,\nChujie Zheng, Zheng Zhang, Jianzhu Yao, Xiaoyan\nZhu, Jie Tang, and Minlie Huang. Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. Han He and Jinho D. Choi. Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKuebler, and Larry Moss. TOIS . Survey of halluci-\nnation in natural language generation. Jungseob Lee, Midan Shim, Suhyune Son, Yujin Kim,\nChanjun Park, and Heuiseok Lim. arXiv\npreprint arXiv:2201.03239 . Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng,\nand Jie Zhou. Ilya Loshchilov and Frank Hutter. 2018. Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. 2018. arXiv preprint arXiv:1801.00631 . Yixin Nie, Mary Williamson, Mohit Bansal, Douwe\nKiela, and Jason Weston. Libo Qin, Tianbao Xie, Shijue Huang, Qiguang Chen,\nXiao Xu, and Wanxiang Che. Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric Michael Smith, Y .-Lan Boureau,\nand Jason Weston. Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason\nWeston. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2017. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. Jing Xu, Arthur D. Szlam, and Jason Weston. Liang Xu, Xuanwei Zhang, Lu Li, Hai Hu, Chenjie Cao,\nWeitang Liu, Junyi Li, Yudong Li, Kai Sun, Yechen\nXu, Yiming Cui, Cong Yu, Qianqian Dong, Yin Tian,\nDian Yu, Bo Shi, Jun jie Zeng, Rongzhao Wang, Wei-\njian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Quanbei\nZhao, Cong Yue, Xinrui Zhang, Zhen-Yi Yang, Kyle\nRichardson, and Zhenzhong Lan. Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu,\nHua Wu, Haifeng Wang, and Shihang Wang. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Chujie Zheng, Sahand Sabour, Jiaxin Wen, and\nMinlie Huang. Hao Zhou, Pei Ke, Zheng Zhang, Yuxian Gu, Yinhe\nZheng, Chujie Zheng, Yida Wang, Chen Henry\nWu, Hao Sun, Xiaocong Yang, Bosi Wen, Xi-\naoyan Zhu, Minlie Huang, and Jie Tang. Fluency and Coherence of Conversations From\nTable 2, we observed that Inquiring (User) results\nin more incoherent b2. Although we have tried our\nbest to unify the annotation criteria through con-\nstant feedback and quality check (\u00a74.3), these bi-28Models Pre-training Fine-tuning2-class 4-class\nAcc F1 Acc F1\nBERTCMNLI - 64.9 62.6 - -\nOCNLI - 64.5 61.0 - -\nCMNLI + OCNLI - 65.4 62.6 - -\n- CDC ONV 72.3 70.1 69.2 51.7\nCMNLI CDC ONV 76.1 / +3.8 74.8 / +4.6 71.5 / +2.3 53.8 / +2.1\nOCNLI CDC ONV 74.8 / +2.5 72.4 / +2.3 72.0 / +2.7 52.6 / +0.9\nCMNLI + OCNLI CDC ONV 75.3 / +3.0 73.8 / +3.6 72.3 / +3.0 54.5 / +2.8\nRoBERTaCMNLI - 64.8 62.2 - -\nOCNLI - 64.0 56.5 - -\nCMNLI + OCNLI - 65.6 62.4 - -\n- CDC ONV 72.1 69.9 69.2 50.7\nCMNLI CDC ONV 76.5 / +4.5 74.5 / +4.6 72.4 / +3.2 54.1 / +3.4\nOCNLI CDC ONV 74.1 / +2.1 72.4 / +2.5 70.6 / +1.4 48.5 / -2.1\nCMNLI + OCNLI CDC ONV 75.7 / +3.6 73.7 / +3.9 72.2 / +3.1 55.1 / +4.4\nERNIECMNLI - 64.7 61.8 - -\nOCNLI - 64.8 57.9 - -\nCMNLI + OCNLI - 64.6 61.5 - -\n- CDC ONV 74.3 72.3 72.4 54.1\nCMNLI CDC ONV 77.4 / +3.1 76.0 /+3.7 74.2 / +1.7 52.6 / -1.5\nOCNLI CDC ONV 75.4 / +1.2 73.1 / +0.7 72.8 / +0.4 53.5 / -0.6\nCMNLI + OCNLI CDC ONV 77.5 /+3.2 75.7 / +3.4 75.0 /+2.5 56.9 /+2.8\nTable 5: Experimental results of NLI pre-training with the method Sentence Pair in \u00a75.2. Among the results of\nfine-tuning on CDC ONV, the performance increases and decreases compared to no NLI pre-training are marked. Observation 2 : NLI pre-training\ngenerally leads to improvements, and using both CMNLI and OCNLI for pre-training gives the best performance\nunder the 4-class setting.",
        "Conclusion": "\ud835\udc4f1: \u6211\u559c\u6b22\u5403\u7c73\u996d! \ud835\udc4f2: \u4e0d\u559c\u6b22! (I dislike!) \ud835\udc4f2: \u4e0d\u559c\u6b22\u732b,\u5176\u4ed6\u7684\u90fd\u559c\u6b22 (I dislike cats. Hence, b2is compatible with b1and does\nnot make a contradiction. We validate the effectiveness of the\ntrigger methods and show that state-of-the-art Chi-\nnese open-domain chatbots (EV A and PLATO) canbe easily goaded into making contradictions (\u00a74.4). Finally, we evaluate popular Chinese pre-trained\nmodels on CDC ONV (\u00a75). It blooms in autumn so beautifully.) Dziri et al. \ud835\udc62!:Human-writtenUtterance\ud835\udc4f! It blooms in autumn so beautifully.) (3) Negative . Finally, we conducted five batches of annotation\nwith incremental batch sizes (17K annotated con-\nversations in total). (I like dogs. \ud835\udc4f2: \u6211\u4e0d\u559c\u6b22,\u6211\u559c\u6b22\u72d7\u72d7! I like dogs!) \ud835\udc4f2: \u6211\u662f\u554a!\u6211\u662f\u9655\u897f\u6986\u6797\u7684 (I am! Hui is an ethnic minority in China. 6 Conclusion\nIn this work, we present CDC ONV, a benchmark\nfor contradiction detection in Chinese conversa-\ntions. 61876096). 2020. In ACL. 2021a. In Findings\nof ACL . 2021b. Yejin Choi. 2022. Daedalus . 2021. TASLP . 2005. 2022. 2019. 2021. Le. 2020. Herbert P Grice. 1975. Brill. 2022. 2022. In ACL. 2021. 2020. In Findings of\nEMNLP .Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. 2022. 2022. 2021. In Findings\nof ACL . 2019. Gary Marcus. 2021. In ACL. 2021. 2021. 2022. 2020. 2019. 2019. In ACL. 2022a. In ACL. 2020. InCOLING . 2022b. Long time no see! In Findings of ACL . In ACL. 2020. In ACL. 2022. 2021.",
        "label": 2
    },
    {
        "Abstract": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 30\u201345\nDecember 7-11, 2022 \u00a92022 Association for Computational Linguistics\nTransformer Feed-Forward Layers Build Predictions by\nPromoting Concepts in the Vocabulary Space\nMor Geva\u2217 \u2217,1Avi Caciularu\u2217,2,\u2020Kevin Ro Wang3Yoav Goldberg1,2\n1Allen Institute for AI2Bar-Ilan University3Independent Researcher\nmorp@allenai.org ,{avi.c33,kevinrowang,yoav.goldberg}@gmail.com\nAbstract\nTransformer-based language models (LMs) are\nat the core of modern NLP, but their inter-\nnal prediction construction process is opaque\nand largely not understood. 1 (Geva et al., 2021):\np\u2113\ni=softmax (Ex\u2113\ni)\n\u02dc p\u2113\ni=softmax (E\u02dc x\u2113\ni). In the rest of the paper, we refer\nto the vectors v\u2113\niasvalue vectors , and to their\nweighted form m\u2113\niv\u2113\niassub-updates . p\u2113:cow, cat, dog, goat, horse, bear\n\u02dc p\u2113:dog, cat, goat, horse, cow, bear\nElimination :cowis eliminated from rank 1 in p\u2113to 5 in \u02dc p\u2113. 7This is a simplification. an analysis of BERT\u2019s attention. ArXiv , abs/2002.05202. arXiv\npreprint arXiv:1907.01470 . The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. GeLU (Hendrycks and Gimpel, 2016) in GPT2 ),\ncan result in negative values of m\u2113\ni.\nEmpirically, we observe that in some cases sub-\nupdates with negative coefficients do appear as part\nof the 10 most dominant sub-updates in GPT2 . if you notice a pattern that occurs only in 3 words, then please ignore it).",
        "Methodology": "In this work,\nwe make a substantial step towards unveiling\nthis underlying prediction process, by reverse-\nengineering the operation of the feed-forward\nnetwork (FFN) layers, one of the building\nblocks of transformer models. We view the\ntoken representation as a changing distribution\nover the vocabulary, and the output from each\nFFN layer as an additive update to that distribu-\ntion. In this work, we investigate\nhow outputs from the FFN layers are utilized inter-\nnally to build predictions. First, each FFN layer\n\u2217Equal contribution. \u2020Work done during an internship at AI2. Feed-forward\nlayers apply additive updates (A) to the token represen-\ntation x, which can be interpreted as a distribution over\nthe vocabulary (B). Second, the token representation\nacross the layers can be translated at any stage to a\ndistribution over the output vocabulary (Geva et al.,\n2021) (Fig. We then decompose the FFN update (\u00a73), in-\nterpreting it as a collection of sub-updates, each\ncorresponding to a column in the second FFN ma-\ntrix (Fig. 1, C) that scales the token probabilities\nin the output distribution. 1, D), and (b)\nFFN updates rely primarily on token promotion30(rather than elimination), namely, tokens in the top\nof the output distribution are those pushed strong\nenough by sub-updates (\u00a75). In \u00a76.1, we show how we can\nintervene in the prediction process, in order to ma-\nnipulate the output distribution in a direction of our\nchoice. Also, in\n\u00a76.2, we show that dominant sub-updates provide\na useful signal for predicting an early exit point,\nsaving 20% of the computation on average. Such LMs are\ncomposed of intertwined multi-head self-attention\n(MHSA) layers and FFN layers (Vaswani et al.,\n2017), with residual connections (He et al., 2016)\nbetween each pair of consecutive layers. The LM\nprediction is obtained by projecting the output vec-\ntor from the final layer to an embedding matrix\nE\u2208R|V|\u00d7d, with a hidden dimension d, to get a\ndistribution over a vocabulary V(after softmax). Given a sequence w=\u27e8w1, ..., w t\u27e9of input to-\nkens, the model creates a contextualized represen-\ntation xi\u2208Rdfor each token wi\u2208w, that is being\nupdated throughout the layers. In this work, we ana-\nlyze the updates applied by the FFN layers and how\nthey construct the model prediction. Concretely,\neach FFN layer \u2113= 1, ..., L processes x\u2113\niand pro-\nduces an output o\u2113\ni, which is then added to x\u2113\nitoyield an updated representation \u02dc x\u2113\ni:\no\u2113\ni=FFN\u2113(x\u2113\ni)\n\u02dc x\u2113\ni=x\u2113\ni+o\u2113\ni\nThe updated representation \u02dc x\u2113\nithen goes through\na MHSA layer,2yielding the input x\u2113+1\nifor the\nnext FFN layer. The evolving representation in\nthis process (i.e. (1)\nTo analyze the FFN updates, we read from the\nrepresentation at any layer a distribution over the\noutput vocabulary, by applying the same projection\nas in Eq. Importantly, by linearity:\nE\u02dc x\u2113\ni=Ex\u2113\ni+Eo\u2113\ni,\nimplying that o\u2113\nican be interpreted as an additive\nupdate in the vocabulary space. In this work, we\ntake this a step further, and decompose the update\no\u2113\niinto a set of smaller sub-updates. x\u2113:=x\u2113\niandp\u2113:=p\u2113\ni. FFN Outputs as Linear Vector Combinations. Previous work\nproposed this module can be cast as an emulated\nneural key-value memory (Sukhbaatar et al., 2015,\n2019), where rows in W\u2113\nKand columns in W\u2113\nVare\nviewed as keys and values, respectively (Geva et al.,\n2021). For an input x\u2113, the keys produce a vector of\ncoefficients m\u2113:=f/parenleftbig\nW\u2113\nKx\u2113/parenrightbig\n\u2208Rdm, that weighs\nthe corresponding values in W\u2113\nV. Denoting by k\u2113\ni\nthei-th row of W\u2113\nKand by v\u2113\nithei-th column of\nW\u2113\nV, we can then use the following formulation:\nFFN\u2113(x\u2113) =dm/summationdisplay\ni=1f(x\u2113\u00b7k\u2113\ni)v\u2113\ni=dm/summationdisplay\ni=1m\u2113\niv\u2113\ni. Therefore, a FFN update can be viewed as a col-\nlection of sub-updates, each corresponding to a\nweighted value vector in the FFN output. A transformer\nLM with L= 10, dm= 3000 will have 30,000\nvalue vectors, and every token that passes through\nthe transformer will weight these value vectors dif-\nferently, resulting in 30,000sub-updates, where\nonly a few of the sub-updates have high weights. Consider a sub-update m\u2113\niv\u2113\nifor a given\ninput, we can estimate its influence on the repre-\nsentation x\u2113(before the FFN update) by analyzing\nthe change it induces on the output distribution. This implies that each sub-update m\u2113\niv\u2113\niintro-\nduces a scaling factor to the probability of every\ntoken wbased on its dot product with ew. Thus, the projection r\u2113\ni=Ev\u2113\ni\u2208R|V|induces a\nranking over the vocabulary that allows compar-\ning the scores by v\u2113\niw.r.t different tokens. \u2022The term m\u2113\niis the dynamic coefficient ofv\u2113\ni,\nwhich is fixed for all tokens for a given input. Thus, these coefficients allow comparing the con-\ntribution of value vectors in a specific update. Overall, the scaling factor ew\u00b7m\u2113\niv\u2113\nican be viewed\nas the effective score given by a value vector v\u2113\nito\na token wfor a given input. In the next sections, we use these observations to\nanswer two research questions of (a) What informa-\ntion is encoded in sub-updates and what tokens do\nthey promote? (\u00a74) and (b) How do FFN updates\nbuild the output probability distribution? To this end, we\nmanually inspect the top-scoring tokens by value\nvectors and check if they express interpretable con-\ncepts. Concretely, we consider two representative\nLMs (details below), and for each vector v\u2113\nicom-\npute a ranking over the vocabulary by sorting the\nprojection r\u2113\ni(\u00a73). Then, we try to detect patterns\nin the top-scoring tokens of each value vector. We let experts\n(NLP graduate students) annotate concepts by iden-\ntifying common patterns among the top-30 scor-\ning tokens of each value vector. For a set of to-\nkens, the annotation protocol includes three steps\nof: (a) Identifying patterns that occur in at least\n4 tokens, (b) describing each recognized pattern,\nand (c) classifying each pattern as either \u201cseman-\ntic\u201d(e.g., mammals), \u201csyntactic\u201d (e.g., past-tense\nverbs), or \u201cnames\u201d . Further details, including the\ncomplete instructions and a fully annotated exam-\nple can be found in App. corpus (Merity et al., 2017) with word-level to-\nkenization ( |V|= 267 ,744), and GPT2 (Rad-\nford et al., 2019), a 12-layer LM trained on WEB-\nTEXT (Radford et al., 2019) with sub-word to-\nkenization ( |V|= 50 ,257).GPT2 uses the\nGeLU activation function (Hendrycks and Gim-\npel, 2016), while WIKILM uses ReLU, and in\ncontrast to GPT2 ,WIKILMdoes not apply layer\nnormalization after FFN updates. WIKILM de-\nfines d= 1024 , dm= 4096 andGPT2 defines\nd= 768 , dm= 3072 , resulting in a total of 65k\nand36kvalue vectors, respectively. For our experi-\nments, we sample 10 random vectors per layer from\neach model, yielding a total of 160 and 120 vectors\nto analyze from WIKILMandGPT2 , respectively. We validate our\napproach by comparing concepts in top-tokens of\nvalue vectors and 10 random vectors from a normal\ndistribution with the empirical mean and standard\ndeviation of the real vectors. Updates vs. Sub-Updates. We justify the FFN\noutput decomposition by analyzing concepts in the\ntop-tokens of 10 random FFN outputs per layer\n(Tab. Also, we observe very few concepts\n(<4%) in the last two layers of WIKILM. We\naccount this to extreme sub-updates that dominate\nthe layer\u2019s output (\u00a75.2). 2 shows a breakdown of the annotations across\nlayers, for WIKILM andGPT2 . In both models\nand across all layers, a substantial portion (40%-\n70% in WIKILMand 20%-65% in GPT2 ) of the\ntop-tokens were associated with well-defined con-\ncepts, most of which were classified as \u201csemantic\u201d . (2021), who observed that value vec-\ntorsin the upper layers represent next-token distri-\nbutions that follow specific patterns. In\npractice, we find that this task is hard for humans,4\nas it requires reasoning over a set of tokens without\nany context, while tokens often correspond to un-\ncommon words, homonyms, or sub-words. More-\nover, some patterns necessitate world knowledge\n(e.g. Providing additional context\nand token-related information are possible future\ndirections for improving the annotation protocol. If sub-\nupdates indeed encode concepts, then we can not\nonly interpret their contribution to the prediction,\nbut also intervene in this process, by increasing the\n4A sub-update annotation took 8.5minutes on average.p\u2113:cow, cat, dog, goat, horse, bear\n\u02dc p\u2113:dog, cat, goat, horse, cow, bear\nSaturation :dogis promoted from rank 3 in p\u2113to rank 1 in\n\u02dc p\u2113, to be the top-candidate until the last layer. weights of value vectors that promote tendencies\nof our choice. In this section,\nwe show that sub-updates systematically configure\nthe prediction via promotion of candidate tokens. This sug-\ngests three mechanisms by which tokens are pushed\nto the top of the output distribution \u2013 promotion ,\nwhere sub-updates increase the probability of fa-\nvorable tokens, elimination , where sub-updates de-\ncrease candidate probabilities, or a mixture of both. To test what mechanism holds in practice, we ana-\nlyze the scores sub-updates assign to top-candidate\ntokens by the representation. As the experiments\ndo not involve human annotations, we use a larger\nGPT2 model with L= 24, d= 1024 , dm= 4096 . We start by comparing the sub-updates\u2019 scores\nto a reference token in two types of events:\n\u2022Saturation (Tab. functional\nallFigure 3: Mean, maximum and minimum scores assigned by the 10 most dominant sub-updates in each layer to the\ntop-candidate token, in GPT2 (left) and WIKILM(right). Solid (dashed) lines exclude (include) functional value\nvector groups. The y-axis in both plots is cut for readability, as the max. didate until the last layer. where the top candidate was dropped\nbehind other candidates to have a rank >1. Over-\nall, our analysis covers 1909 ( WIKILM) and\n1996 (GPT2) elimination events. We compute the mean, maximum, and minimum\nscores of the reference token by the 10 most domi-\nnant sub-updates in each event, and average over\nall the events. As a baseline, we compute the scores\nby 10 random sub-updates from the same layer. In both models, to-\nkens promoted to the top of the distribution receive\nhigher maximum scores than tokens eliminated\nfrom the top position ( 1.2\u21920.5inWIKILMand\n8.5\u21924.0inGPT2 ), indicating they are pushed\nstrongly by a few dominant sub-updates. In contrast to promoted tokens,\nwhere the maximum scores are substantially higher\nthan the minimal scores ( 1.2vs.\u22120.8inWIKILM\nand8.5vs.\u22124.9inGPT2 ), for eliminated tokens,\nthe scores are similar in their magnitude ( \u00b10.5in\nWIKILMand4.0vs.\u22123.6inGPT2 ). Last, scores\nby random sub-updates are dramatically lower in\nmagnitude, showing that our choice of sub-updates\nis meaningful and that higher coefficients translate\nto greater influence on the output distribution. This suggests that FFN updates work in a pro-\nmotion mechanism, where top-candidate tokens are\nthose being pushed by dominant sub-updates . 5.2 Sub-Updates Across Layers\nTo analyze the FFN operation in different layers,\nwe break down the top-candidate scores per layer. Formally, let w\u2113=argmax (p\u2113)be the top candi-\ndate at layer \u2113(before the FFN update) for a given\ninput, we extract the scores ew\u2113\u00b7m\u2113\niv\u2113\niby the 10\nmost dominant sub-updates and compute the mean,\nminimum and maximum scores over that set. This suggests that the to-\nken promotion mechanism generally holds across\nlayers. However, scores diverge in the last layers of\nboth models, with strong negative minimum scores,\nindicating that the probability of the top-candidate\nis pushed down by dominant sub-updates. We next\nshow that these large deviations in positive and neg-\native scores (Fig. To analyze the extreme\nFFN updates, we first cluster the value vectors to\ndiscover high-level trends. More interestingly, the value vectors that pro-\nmote unlikely tokens can be viewed as \u201csaturation\nvectors\u201d , which propagate the distribution without\nchanging the top tokens. 6.1 Zero-Shot Toxic Language Suppression\nLMs are known to generate toxic, harmful language\nthat damages their usefulness (Bender et al., 2021;\nMcGuffie and Newhouse, 2020; Wallace et al.,\n2019). Method. \u201csafe\u201d and\u201cthank\u201d ), or (b) grading\nthe tokens with the Perspective API and selecting\nnon-toxic value vectors (see details in App. We turn on these value vectors by setting their co-\nefficients to 3, a relatively high value according to\nFig. We compare our method with two baselines:\n1.Self-Debiasing (SD) (Schick et al., 2021): SD\ngenerates a list of undesired words for a given\nprompt by appending a self-debiasing input ,\nwhich encourages toxic completions, and cal-\nculating which tokens are promoted compared\nto the original prompt. These undesired words\u2019\nprobability are then decreased according to a\ndecay constant \u03bb, which we set to 50 (default). We evaluate our method on the chal-\nlenging subset of REALTOXIC PROMPTS (Gehman\net al., 2020), a collection of 1,225 prompts that tend\nto yield extremely toxic completions in LMs, using\nthe Perspective API, which grades text according\nto six toxicity attributes. A score of >0.5indicates\na toxic text w.r.t to the attribute. We use GPT2 and, following Schick\net al. (2021), generate continuations of 20 tokens. Moreover, inducing sub-\nupdates that promote \u201csafety\u201d related concepts is\nmore effective than promoting generally non-toxic\nsub-updates. However, our method resulted in a\nperplexity increase greater than this induced by SD,\nthough the increase was still relatively small. In this section, we test our hypothesis that domi-\nnant FFN sub-updates can signal a saturation event\n(\u00a75.2), to create a simple and effective early exiting\nmethod that does not involve any external model\ntraining. Method. First,\nfor every example, we map the top-10 dominant\nsub-updates at each layer to their corresponding\nclusters. Then, for every layer \u2113, we split all the\nsets of clusters at that layer into two sets, T\u2113and\nN\u2113, based on whether saturation occurred or not\n(e.g., T5stores all the sets that were active in a sat-\nuration event at layer 5). We train layer-wise binary classifiers\nover the representation and FFN updates x\u2113,o\u2113,\nand\u02dc x\u2113, using logistic regression. As in our method,\nthe labels are determined according to saturation\nevents in the training data (see App. During\ninference, we execute the computation through the\nlayers, and halt according to the layer classifier. We split N\u2113by saturation layers\nand require a bigger intersection with T\u2113at all the layers.Method Accuracy Saved Layers\nBinary classifiers using x\u211394.4\u00b16.4 18.8% 3.0\u00b10.4\nBinary classifiers using o\u211392.9\u00b15.4 19.4% 3.1\u00b10.3\nBinary classifiers using \u02dc x\u211394.4\u00b16.4 18.8% 3.0\u00b10.4\nSub-updates rule 94.1 \u00b11.4 20.0% 3.2\u00b10.3\nTable 6: Early exit evaluation results on W IKILM. Each method is evaluated by accu-\nracy, i.e., the portion of examples for which exiting\nat the predicted layer yields the final model pre-\ndiction, and by computation efficiency , measured\nby the amount of saved layers for examples with\ncorrect prediction. Moreover, just by observing the dominant\nFFN sub-updates, it performs on-par with the pre-\ndiction rules relying on the representation and FFN\noutput vectors. In contrast, our approach aims to\ninterpret the model parameters and their utilization\nin the prediction process. More recently, a surge of works have investi-\ngated the knowledge captured by the FFN layers\n(Da et al., 2021; Jiang et al., 2020; Dai et al., 2022;37Yao et al., 2022; Meng et al., 2022; Wallat et al.,\n2020). These works show that the FFN layers store\nvarious types of knowledge, which can be located\nin specific neurons and edited. Unlike these works,\nwe focus on the FFN outputs and their contribution\nin the prediction construction process. We study the FFN output as a linear combina-\ntion of parameter vectors, termed values, and the\nmechanism by which these vectors update the token\nrepresentations. Limitations\nOur study focused on the operation of FFN lay-\ners in building model predictions. Future work\nshould further analyze the interplay between these\nlayers and other components in the network, such\nas attention-heads. In our analysis, we decomposed the computation\nof FFN layers into smaller units, corresponding to\nsingle value vectors. However, it is possible that\nvalue vectors are compositional in the sense that\ncombinations of them may produce new meanings. Still, we argue that analyzing individual value vec-\ntors is an important first step, since (a) the space of\npossible combinations is exponential, and (b) our\nanalysis suggests that aggregation of value vectors\nis less interpretable than individual value vectors\n(\u00a74.1). Thus, this approach opens new directionsfor interpreting the contribution of FFN layers to\nthe prediction process in transformer LMs. In addition, we chose to examine the broad fam-\nily of decoder-based, auto-regressive LMs, which\nhave been shown to be extremely effective for\nmany NLP tasks, including few- and zero-shot\ntasks (Wang et al., 2022). RoBERTa (Liu et al., 2019)) and mod-\nels trained with different objective functions (e.g. Scaling our\nannotation protocol would enable a more refined\nmap of the concepts, knowledge and structure cap-\ntured by LMs. Furthermore, since our concept\ninterpretation approach relies on manual inspection\nof sets of tokens, its success might depend on the\nmodel\u2019s tokenization method. In this work, we an-\nalyzed models with two different commonly-used\ntokenizers, and future research could verify our\nmethod over other types of tokenizations as well. It should be made clear that our method\nfor toxic language suppression only reduces the\nprobability of toxic language generation and does\nnot eliminate it. As such, this method (as well as\nour early-exit method) should not be used in the\nreal world without further work and caution. More broadly, our work suggests a general ap-\nproach for modifying LM predictions in particular\ndirections, by changing the weights of FFN sub-\nupdates. While this is useful for mitigating biases,\nit also has the potential for abuse. Moreover, our work primar-\nily proposes an interpretation for FFN sub-updates,\nwhich also could be used to identify abusive inter-38ventions. Acknowledgements\nWe thank Shauli Ravfogel, Tal Schuster, and\nJonathan Berant for helpful feedback and construc-\ntive suggestions. This project has received funding\nfrom the Computer Science Scholarship granted by\nthe S\u00e9phora Berrebi Foundation, the PBC fellow-\nship for outstanding PhD candidates in Data Sci-\nence, and the European Research Council (ERC)\nunder the European Union\u2019s Horizon 2020 research\nand innovation programme, grant agreement No. arXiv preprint\narXiv:1607.06450 . Kevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing , pages 5484\u20135495, Online and Punta Cana,\nDominican Republic. Deep residual learning for image recogni-\ntion. arXiv preprint\narXiv:1606.08415 . Ridge re-\ngression: Biased estimation for nonorthogonal prob-\nlems. Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui\nQin, Kyle Richardson, Sean Welleck, Hannaneh Ha-\njishirzi, Tushar Khot, Ashish Sabharwal, Sameer\nSingh, and Yejin Choi. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 3631\u20133643, Seattle,\nUnited States. CascadeBERT: Ac-\ncelerating inference of pre-trained language models\nvia calibrated complete models cascade. Locating and editing factual knowl-\nedge in gpt. Stephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. Modern hierarchical, ag-\nglomerative clustering algorithms. arXiv preprint\narXiv:1109.2378 . Understanding\nlearning dynamics of language models with SVCCA. Consistent accelerated inference via\nconfident adaptive transformers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing , pages 4962\u20134979, Online\nand Punta Cana, Dominican Republic. Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lam-\nple, Herve Jegou, and Armand Joulin. Aug-\nmenting self-attention with persistent memory. Ian Tenney, Dipanjan Das, and Ellie Pavlick. Journal of the Royal Statistical\nSociety: Series B (Methodological) , 58(1):267\u2013288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , pages 4396\u20134406, Hong Kong,\nChina. In Proceed-\nings of the 2019 Conference on Empirical Methods40in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 2153\u20132162, Hong\nKong, China. In Proceedings of the Third\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP , pages 174\u2013183, On-\nline. In Proceedings of\nthe 39th International Conference on Machine Learn-\ning, volume 162 of Proceedings of Machine Learning\nResearch , pages 22964\u201322984. BERxiT: Early exiting for BERT with better\nfine-tuning and extension to regression. We mea-\nsure similarity of the top-30 tokens in each projection\nwith IoU. A Appendix\nA.1 Value Vectors Projection Method\nOur interpretation method of sub-updates is based\non directly projecting value vectors to the embed-\nding matrix, i.e. for a value vand embedding\nmatrix E, we calculate Ev(\u00a74). However, in some\nLMs like GPT2 , value vectors in each layer are\nadded to the token representation followed by a\nlayer normalization (LN) (Ba et al., 2016). As a baseline, we also compare\nEv\u2113\niwith random vectors, initialized from a normal\ndistribution with the empirical mean and standard\ndeviation of the value vectors. 4 shows that\nLN does not change the projection substantially,\nwith an overlap of 64.5%of the top-30 tokens on\naverage, suggesting that the same concepts are pro-\nmoted in both cases. Please note these are the instructions\nprovided for annotations of WIKILM, which uses\nword-level tokenization. For the description of a token\nw, we first try to extract the definition of wfrom\nWordnet.8Ifwdoes not exist in Wordnet, as often\nhappens for names of people and places, we then\nsearch for win Wikipedia9and extract a short (pos-\nsibly noisy) description if the query was successful. The overall\ncontribution of the top-10 dominant sub-updates is\ncomputed by summing their contributions. Note\nthat we take the absolute value of the coeffi-\ncients |m\u2113\ni|, since some activation functions (e.g. We further attribute this to the success of GeLU in\ntransformer models (Shazeer, 2020), as it increases\nthe expressiveness of the model by allowing re-\nversing the scores value vectors induce over the\nvocabulary. Clearly, for all\nthe layers, the contribution of the dominant sub-\nupdates exceeds the contribution of random sub-\nupdates. Observe that, even though they cover\nonly 0.24% of the value vectors, the contribution of\ndominant sub-updates is typically around 5%, and\nin some layers (e.g. You should only count patterns that occur in at least 4\nwords (i.e. To complete the task, please do the following:\n1. Give an ID to every identified pattern (1,2,...)\n2. Assign a pattern ID to every word in the list, or -1/leave empty if no pattern applies to\nthe word. For every identified pattern specify whether the pattern is semantic or syntactic and\n(optional) write a short description of the pattern. contribution. This demonstrates that analyzing the\ntop-10 dominant sub-updates can shed light on the\nway predictions are built through the layers. We selected a\nsmall set of 10 value vectors whose top-scoring\ntokens were coherent and seemed to promote differ-\nent kinds of non-toxic tokens. The list of manually\npicked vectors is provided in Tab. Importantly,\nthe search process of all vectors was a one-time\neffort that took <5minutes in total. We chose\nthe value vectors in a greedy-manner, without addi-\ntional attempts to optimize our choice. To select 10 non-toxic value vectors based on an\nautomatic toxicity metric, we used the Perspective\nAPI. Concretely, we concatenated the top-30 tokens\nby each value vector and graded the resulting text\nwith the toxicity score produced by the API. Then,\nwe sampled 10 random vectors with a toxicity score\n<0.1(a score of <0.5indicates a non-toxic text). A.5 Early Exit Details\nThis section provides further details and analysis\nregarding our early exit method and the baselines\nwe implemented. Method Implementation. Baselines\u2019 Implementation. We train each bi-\nnary classifier using 8k training examples, based\non the standardized forms of each feature vec-\ntor. We also used a inversely proportional loss coeffi-\ncient according to the class frequencies. In order to achieve high accuracy, we further\ncalibrate a threshold per classifier for reaching the\nmaximal F 1score for each layer. This calibration\nis done after training each classifier, over a set of\n1000 validation examples. We investi-\ngate the potential of performing early exit for WIK-\nILMandGPT2 . In GPT2 ,\n34.15% of the examples require the full compu-\ntation using all the model layers, while for WIK-\nILM, this holds for only 15.22% of the examples. Notably, early fixation events in GPT2 are less\ncommon than in WIKILM, possibly due to the\nlarger number of layers the prediction construction\nis spread over. 12345678910111213141516\nlayer0102030% contributionWikiLM\ntop-10 rand-10\n123456789101112131415161718192021222324\nlayer051015% contributionGPT2\ntop-10 rand-10\nFigure 6: Relative contribution to the FFN output of the 10 most dominant and 10 random sub-updates in each layer,\nof W IKILM (left) and GPT2 (right). Repetitions in the projections are a result of special characters not being shown.",
        "Results and Findings": "Recent work showed\nthat these layers play an important role in LMs,\nacting as memories that encode factual and linguis-\ntic knowledge (Geva et al., 2021; Da et al., 2021;\nMeng et al., 2022). residual stream \n675a\n589she ordered a \npancake \nfewcoffee \npancake few\ncoffee FFN \nlayer \nfruit, apples, \nsnack, vitamins, \nberries, oats, \nyogurt, tea, \u2026 \n(br eakfast) \nv1 v2 vd m \n(B)(C )(D) (A)\nxx\u0303Figure 1: Illustration of our findings. 1, A). 1, B). We reason that the additive com-\nponent in the update changes this distribution (\u00a72),\nnamely, FFN layers compute updates that can be\ninterpreted in terms of the output vocabulary . Through a series of\nexperiments, we find that (a) sub-update vectors\nacross the entire network often encode a small-set\nof human-interpretable well-defined concepts, e.g. Overall, these find-\nings allow fine-grained interpretation of the FFN\noperation, providing better understanding of the\nprediction construction process in LMs. Beyond interpretation, our findings also have\npractical utility. Specifically, we show that increasing the\nweight of only 10 sub-updates in GPT2 reduces\ntoxicity in its generations by almost 50%. Our findings shed light\non the prediction construction process in modern\nLMs, suggesting promising research directions for\ninterpretability, control, and efficiency. Note that \u02dc pL\ni=y. By projecting\nthe sub-updates to the vocabulary we find that they\noften express human-interpretable concepts. Each FFN at layer \u2113consists of two linear trans-\nformations with a point-wise activation function in\nbetween (bias terms are omitted):\nFFN\u2113(x\u2113) =f/parenleft\uf8ecig\nW\u2113\nKx\u2113/parenright\uf8ecig\nW\u2113\nV,\n2In some LMs, e.g. Concretely, we isolate the effect of m\u2113\niv\u2113\nion the\nprobability p\u2113\nwofw\u2208 V:3\np/parenleftbig\nw|x\u2113+m\u2113\niv\u2113\ni, E/parenrightbig\n=exp/parenleftbig\new\u00b7x\u2113+ew\u00b7m\u2113\niv\u2113\ni/parenrightbig\nZ/parenleftbig\nE(x\u2113+m\u2113\niv\u2113\ni))\n\u221dexp/parenleftbig\new\u00b7x\u2113/parenrightbig\n\u00b7exp/parenleftbig\new\u00b7m\u2113\niv\u2113\ni/parenrightbig\n,(2)\nwhere ewis the embedding of w, andZ/parenleftbig\n\u00b7/parenrightbig\nis the\nconstant softmax normalization factor. In App. A.1, we verify empiri-\ncally that our findings hold also when LN is applied.\u2022The term ew\u00b7v\u2113\nican be viewed as a static score\nofwthat is independent of the input to the model. A.2. We conduct our experiments over\ntwo auto-regressive decoder LMs: The model\nof Baevski and Auli (2019) (dubbed WIKILM),\na 16-layer LM trained on the WIKITEXT-1033212345678910111213141516\nlayer020406080100% tokensWikiLM\nsemantic syntactic names N/A\n123456789101112\nlayer020406080100% tokensGPT2\nsemantic syntactic N/AFigure 2: Portion of top-scoring tokens by value vectors in WIKILM andGPT2 , that were associated with a\nsemantic or syntactic concept, a name, or could not be matched to any concept (\u201cN/A\u201d). Concept Sub-update top-scoring tokens\nGPT2v3\n1018 Measurement semantic kg, percent, spread, total, yards, pounds, hours\nv8\n1900 WH-relativizers syntactic which, whose, Which, whom, where, who, wherein\nv11\n2601 Food and drinks semantic drinks, coffee, tea, soda, burgers, bar, sushi\nWIKILMv1\n1 Pronouns syntactic Her, She, Their, her, she, They, their, they, His\nv6\n3025 Adverbs syntactic largely, rapidly, effectively, previously, normally\nv13\n3516 Groups of people semantic policymakers, geneticists, ancestries, Ohioans\nTable 1: Example value vectors in GPT2 and W IKILM promoting human-interpretable concepts. We observe that a sub-\nstantially higher portion of top-tokens were associ-\nated to a concept in value vectors compared to the\nrandom ones (Tab. 2): 55.1%vs.22.7%inWIK-\nILM, and 37% vs.16% inGPT2 . Notably, no semantic nor syntac-\ntic concepts were identified in WIKILM\u2019s random\nvectors, and in GPT2 , only 4%of the tokens were\nmarked as semantic concepts in the random vectors\nversus 24.9%in the value vectors. Moreover, de-\ncomposing the FFN outputs is necessary for fine-\ngrained interpretation of sub-updates.33GPT2 W IKILM\nFFN sub-updates 36.7% 55.1%\n+stopwords concepts 37% 55.1%\nRandom sub-updates 16% 22.7%\nFFN updates 11.8% 19.7%\n+stopwords concepts 46% 39.4%\nTable 2: Portion of top-scoring tokens associated with\na concept, for FFN updates and sub-updates in WIK-\nILM andGPT2 , and for random vectors. For FFN\nupdates/sub-updates, we show results with and without\ncounting concepts marked as stopwords. Also, we observe that the top-tokens of a single\nvalue vector were associated with 1.5(WIKILM)\nand1.1(GPT2 ) concepts on average, showing that\nsub-updates across all layers encode a small-set of\nwell-defined concepts . These findings expand on previous results by\nGeva et al. Our results,\nwhich hold across all the layers , suggest that these\nvectors represent general concepts rather than pri-\noritizing specific tokens. negative polarity items). This of-\nten leads to undetectable patterns, suggesting that\nthe overall results are an underestimation of the true\nconcept frequency. We demonstrate this in \u00a76.1. 3, up): The update p\u2113\u2192\u02dc p\u2113\nwhere the final token predicted by the model (i.e.,\nw=argmax (y)) was promoted to be the top can-\n5Data is segmented into sentences (Geva et al., 2021).34123456789101112131415161718192021222324\nlayer15\n10\n5\n05101520top-10 values' scores for\nthe top candidateGPT2\nmean score\nmin score\nmax score     \nw.o. functional\nall\n12345678910111213141516\nlayer4\n2\n024top-10 values' scores for\nthe top candidateWikiLM\nmean score\nmin score\nmax score     \nw.o. (min.) Sub-updates Event Max. WIKILM, dominantsaturation 1.2<0.01\u22120.8\nelimination 0.5\u22120.01\u22120.5\nWIKILM, randomsaturation 0.02<0.01\u22120.02\nelimination 0.02<0.01\u22120.02\nGPT2, dominantsaturation 8.5 1 .3\u22124.9\nelimination 4.0 0 .1\u22123.6\nGPT2, randomsaturation 0.2 0 .01\u22120.2\nelimination 0.1<0.01\u22120.1\nTable 4: Maximum, mean, and minimum scores of ref-\nerence tokens in saturation and elimination events, by\nthe 10 most dominant and 10 random sub-updates. We analyze saturation\nevents induced by the FFN before the last layer,\ncovering 1184 and 1579 events in WIKILMand\nGPT2, respectively. 3, bottom): The update p\u2113\u2192\n\u02dc p\u2113with the largest increase in the top candidate\u2019s\nrank, i.e. Moreover,\ntokens eliminated from the top of the distribution re-\nceive near-zero mean scores, by both dominant andrandom sub-updates, suggesting they are not being\neliminated directly. Fig. 3 shows that, in both models, until the last\nfew layers (23-24 in GPT2 and 14-16 in WIKILM),\nmaximum and minimum scores are distributed\naround non-negative mean scores, with prominent\npeaks in maximum scores (layers 3-5 in GPT2 and\nlayers 4-11 in WIKILM). We use agglomerative\nclustering (M\u00fcllner, 2011) to learn 10k clusters\nfor each model, based on the cosine distance ma-\ntrixD, where D(\u21131,i1),(\u21132,i2)= 1\u2212cos(v\u21131\ni1,v\u21132\ni2),\n\u2200i1, i2\u2208 {1,\u00b7\u00b7\u00b7, dm},\u2200\u21131, \u21132\u2208 {1,\u00b7\u00b7\u00b7, L}.6\nThen, we search for clusters that are frequently\nactive in extreme updates, by (a) extracting sub-\nupdates where the scores for the top-candidate pass\na certain threshold ( \u00b110forGPT2 and\u00b15for\nWIKILM), and (b) counting the appearances of\neach cluster in the layer sub-updates. 6 Applications\nWe leverage our findings for controlled text genera-\ntion (\u00a76.1) and computation efficiency (\u00a76.2). We utilize our findings to create a simple,\nintuitive method for toxic language suppression. We find\nvalue vectors that promote safe, harmless concepts\nby extracting the top-tokens in the projections of all\n6We experimented with k= 3e2,1e3,3e3,1e4,3e4, and\nchoose k= 1e4based on manual inspection.the value vectors and either (a) manually searching\nfor vectors that express a coherent set of positive\nwords (e.g. Evaluation. Finding the non-toxic sub-updates man-\nually was intuitive and efficient (taking <5min-\nutes). 5 shows that activation of only 10\nvalue vectors (0.01%) substantially decreases toxi-\ncity (\u219347%), outperforming both SD ( \u219337%) and\nWORDFILTER (\u219320%). Such methods\noften use small neural models to determine when to\nstop the execution process (Schwartz et al., 2020b;36Model Toxicity Severe Sexually Threat Profanity Identity PPL\ntoxicity explicit attack\nGPT2 58.5% 49.2% 34.1% 16.4% 52.5% 16.8% 21.7\n\u219110 Manual Pick \u219347% 30.8% \u219350% 24.8% \u219340% 20.4% \u219363% 6.0% \u219347% 27.9% \u219348% 8.8% 25.3\n\u219110 API Graded \u219310% 52.7% \u219311% 44% \u21933%33.2% \u219319% 13.3% \u21939%47.6% \u21939%15.3% 23.8\nSD \u219337% 37.2% \u219346% 26.4% \u219336% 21.7% \u219352% 7.8% \u219339% 32% \u219350% 8.4% 23.9\nWORDFILTER \u219320% 46.9% \u219334% 32.4% \u219336% 21.9% \u2193<1% 16.3% \u219338% 32.3% \u219313% 14.7% -\nTable 5: Evaluation results on the challenging subset of REALTOXIC PROMPTS , showing the percentage of toxic\ncompletions for 6 toxicity attributes, as well as language model perplexity (\u201cPPL\u201d). Elbayad et al., 2020; Hou et al., 2020; Xin et al.,\n2020, 2021; Li et al., 2021; Schuster et al., 2021). For the experiments, we use WIKILM,\nwhere saturation events occur across all layers\n(statistics for WIKILMandGPT2 are in App. A.5). A.5). Evaluation. We run each method with five\nrandom seeds and report the average scores. 6 shows that our method obtains\na high accuracy of 94.1%, while saving 20% of\ncomputation on average without changing the pre-\ndiction. This demonstrates the utility of\nsub-updates for predicting saturation events, and\nfurther supports our hypothesis that FFN updates\nplay a functional role in the prediction (\u00a75.2). Our analysis of transformer-based LMs provides\na more detailed understanding of their internal pre-\ndiction process, and suggests new research direc-\ntions for interpretability, control, and efficiency, at\nthe level of individual vectors. While these models share\nthe same building blocks of all transformer-based\nLMs, it will be valuable to ensure that our findings\nstill hold for other models, such as encoder-only\nLMs (e.g. 802774 (iEXTRACT). 2016. Alexei Baevski and Michael Auli. Emily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and\nAntoine Bosselut. Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. Samuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages\n3356\u20133369, Online. Mor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Dan Hendrycks and Kevin Gimpel. 2016. Arthur E Hoerl and Robert W Kennard. Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li,\nJie Zhou, and Xu Sun. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021 , pages 475\u2013486, Punta Cana, Do-\nminican Republic. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. arXiv preprint arXiv:1907.11692 . Kris McGuffie and Alex Newhouse. arXiv preprint arXiv:2009.06807 . Kevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. arXiv preprint arXiv:2202.05262 . 2017. Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. OpenAI\nblog, 1(8):9. Naomi Saphra and Adam Lopez. InProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 3257\u20133267,\nMinneapolis, Minnesota. Timo Schick, Sahana Udupa, and Hinrich Sch\u00fctze. Tal Schuster, Adam Fisch, Tommi Jaakkola, and Regina\nBarzilay. Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. Roy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith. Noam M. Shazeer. S. Sukhbaatar, J. Weston, and R. Fergus. 2015. BERT rediscovers the classical NLP pipeline. 2017. Elena V oita, Rico Sennrich, and Ivan Titov. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. Jonas Wallat, Jaspreet Singh, and Avishek Anand. Thomas Wang, Adam Roberts, Daniel Hesslow,\nTeven Le Scao, Hyung Won Chung, Iz Beltagy, Julien\nLaunay, and Colin Raffel. Ji Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin. Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou,\nand Lei Li. arXiv preprint arXiv:2111.05193 . Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tade-\npalli, Stefan Lee, and Zhaopeng Tu. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 4799\u20134811, Online. Yunzhi Yao, Shaohan Huang, Ningyu Zhang, Li Dong,\nFuru Wei, and Huajun Chen. arXiv preprint arXiv:2201.05742 .41123456789101112131415161718192021222324\nlayer020406080100IoU of value projectionsGPT2\nnormalized randomFigure 4: Similarity of projections to E, ofGPT2 value\nvectors with and without layer normalization, and of\nvalue vectors and randomly-initialized vectors. To test that, we compare the top-30 scoring\ntokens by Ev\u2113\niand by E\u00b7LayerNorm (v\u2113\ni), for\ni= 1, ..., d mand\u2113= 1, ..., L , using Intersection\nover Union (IoU). Fig. A.3 Sub-Update Contribution in FFN\nOutputs\nIn this section, we justify our choice along the pa-\nper of looking at the top-10 dominant sub-updates. Fig. Patterns can be\nsemantic\n(e.g. Please note that some of the words might be uncommon words that you are not familiar with. We used k= 2e2to cluster the top-10dominant value vectors, but observed that other k\nvalues yielded similar results. We considered a hyperparameter sweep, us-\ning 8-fold cross-validation, with l2orl1regu-\nlarization (lasso (Tibshirani, 1996) or ridge (Ho-\nerl and Kennard, 1970)), regularization coef-\nficients C\u2208 {1e\u22123,1e\u22122,1e\u22121,1,1e1,1e2,1e3},\nand took the best performing model for each layer. tation saving potential, as well as more saturation\nevents per layer.44Value Top-10 Tokens\nv14\n1853transparency, disclosure, clearer, parency, iquette,\nhumility, modesty, disclosures, accountability, safer\nv15\n73respectful, honorable, healthy, decent, fair, erning,\nneutral, peacefully, respected, reconc\nv15\n1395safe, neither, safer, course, safety, safe, Safe,\napologize, Compact, cart\nv16\n216refere, Messages, promises, Relations, accept, acceptance,\nAccept, assertions, persistence, warn\nv17\n462should, should, MUST, ought, wisely, Should, SHOULD,\nsafely, shouldn, urgently\nv17\n3209peaceful, stable, healthy, calm, trustworthy, impartial,\nstability, credibility, respected, peace\nv17\n4061Proper, proper, moder, properly, wisely, decency, correct,\ncorrected, restraint, professionalism\nv18\n2921thank, THANK, thanks, thank, Thank, apologies, Thank,\nthanks, Thanks, apologise\nv19\n1891thanks, thank, Thanks, thanks, THANK, Thanks, Thank, Thank,\nthank, congratulations\nv23\n3770free, fit, legal, und, Free, leg, pless, sound, qualified,\nFree\nTable 8: The 10 manually picked value vectors used for toxic language suppression and the top-10 tokens in their\nprojection to the vocabulary. These\nvectors were found by manually searching for non-toxic words such as \u201csafe\u201d and\u201cpeace\u201d in the projections to the\nvocabulary.",
        "Conclusion": "1. In conclusion, we investigate the mechanism in\nwhich FFN layers update the inner representations\nof transformer-based LMs. The\noutput probability distribution is obtained from the\nfinal representation of the token, i.e.,\ny=softmax (E\u02dc xL\ni). Terminology. 2). Overall, this shows that projecting sub-updates\nto the vocabulary provides a meaningful interface\nto the information they encode. 1. A.3). scores reach 100 (-6). Mean Min. Tab. 4 shows the results. stopwords). end-of-sentence period), suggesting that these\nvalue vectors might configure \u201ceasy\u201d model predic-\ntions. Indeed, these vectors are\nin the last layers, where often the model already\nstores its final prediction (Geva et al., 2021). A.4). 3. Results. Tab. Results. Tab. 8 Conclusions\nUnderstanding the inner workings of transformers\nis valuable for explainability to end-users, for de-\nbugging predictions, for eliminating undesirable\nbehavior, and for understanding the strengths and\nlimitations of NLP models. We show that value vectors often\nencode human-interpretable concepts and that these\nconcepts are promoted in the output distribution. Finally, our annotation effort was made for the\nevaluation of our hypothesis that sub-updates en-\ncode human-interpretable concepts. 2019. 2021. 2020. 2019. 2021. 2022. 2019. 2020. 2021. 2020. 2021. 1970. 2020. 2020. 2022. 2021. 2019. 2020. 2022. Daniel M\u00fcllner. 2011. 2019. 2019. 2021. 2021. 2020a. Green AI. 2020b. 2020. 2019. 2019. Robert Tibshirani. 1996. 2019. 2019. 2020. 2022. PMLR. 2020. 2021. 2021. 2020. 2022. 5. 7. 3. 8. Tab.",
        "label": 2
    },
    {
        "Abstract": "Adversarial Attacks on Neural Networks for Graph Data\nDaniel Z\u00fcgner Amir Akbarnejad Stephan G\u00fcnnemann\nTechnical University of Munich, Germany\n{zuegnerd,amir.akbarnejad,guennemann}@in.tum.de\nABSTRACT\nDeep learning models for graphs have achieved strong performance\nfor the task of node classification. One of the most frequently applied\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. For discrete data, where gradients are undefined,\nsuch an approach is suboptimal. Since this is out of the scope\nof the current paper, we do not discuss these approaches here. For Cora-ML, the features correspond to\nthe presence of words in the abstracts of papers. set of nodes with same label), we plot the top-10 features that have\nbeen manipulated by the techniques (these account for roughly\n50% of all perturbations). Limited Knowledge. InAAAI. In AAAI. In AAAI. In AAAI.",
        "Methodology": "In this work, we introduce the first study of adversarial\nattacks on attributed graphs, specifically focusing on models ex-\nploiting ideas of graph convolutions. We generate adversarial perturbations targeting the\nnode\u2019s features and the graph structure, thus, taking the dependen-\ncies between instances in account. Moreover, we ensure that the\nperturbations remain unnoticeable by preserving important data\ncharacteristics. To cope with the underlying discrete domain we\npropose an efficient algorithm Nettack exploiting incremental\ncomputations. Even more, our attacks are transferable: the learned\nattacks generalize to other state-of-the-art node classification mod-\nels and unsupervised approaches, and likewise are successful even\nwhen only limited knowledge about the graph is given. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Request permissions from permissions@acm.org. KDD \u201918, August 19\u201323, 2018, London, United Kingdom\n\u00a92018 Copyright held by the owner/author(s). $15.00\nhttps://doi.org/10.1145/3219819.3220078\nTrain node classification model[ ][ ][ ][ ]........\nNode classification[ ][ ][ ][ ]..xx....\n[ ]..[ ]..\nTarget gets misclassifiedperturbationtarget nodeattacker node\nNettack Nettack-In. While many classical approaches have been introduced in the\npast to tackle the node classification problem [ 8,21], the last years\nhave seen a tremendous interest in methods for deep learning on\ngraphs [5,7,25]. The strength of these methods \u2014 beyond their non-linear, hi-\nerarchical nature \u2013 relies on their use of the graphs\u2019 relational\ninformation to perform classification: instead of only considering\nthe instances individually (nodes and their features), the relation-\nships between them are exploited as well (the edges). Put differently:\nthe instances are not treated independently; we deal with a certain\nform of non-i.i.d. data where so-called network effects such as\nhomophily [21] support the classification. However, there is one big catch: Many researchers have no-\nticed that deep learning architectures for classical learning tasks\ncan easily be fooled/attacked [ 14,30] . Such negative\nresults significantly hinder the applicability of these models, lead-\ning to unintuitive and unreliable results, and they additionally open\nthe door for attackers that can exploit these vulnerabilities. In this work, we close this gap and we investigate whether such\nmanipulations are possible. On the other hand, the propagation of\ninformation might also lead to cascading effects, where manipu-\nlating a single instance affects many others. Indeed, compared to\nthe existing works on adversarial attacks, our work significantly\ndiffers in various aspects. (2) While existing works were limited to ma-\nnipulating an instance itself to enforce its wrong prediction1, the\nrelational effects give us more power: by manipulating one instance,\nwe might specifically misguide the prediction for another instance. Figure 1 illus-\ntrates the goal of our work and shows the result of our method on\nthe Citeseer network. But likewise, constructing them is far more challenging. Here, the train and test\ndata are used jointly to learn a new classification model before the\npredictions are performed on the specific test data. The model has to be (re)trained on the manipulated data. Given these challenges, we propose a principle for adversarial\nperturbations of attributed graphs that aim to fool state-of-the\nart deep learning models for graphs. In particular, we focus on\nsemi-supervised classification models based on graph convolutions\nsuch as GCN [19] and Column Network (CLN) [ 28] \u2013 but we will\nalso showcase our methods\u2019 potential on the unsupervised model\nDeepWalk [ 27]. By default, we assume an attacker with knowledge\nabout the full data, which can, however, only manipulate parts of\nit. This assumption ensures reliable vulnerability analysis in the\nworst case. But even when only parts of the data are known, our\nattacks are still successful as shown by our experiments. We introduce new\ntypes of attacks where we explicitly distinguish between the\nattacker and the target nodes. Our attacks can manipulate the\ngraph structure and node features while ensuring unnoticeable\nchanges by preserving important data characteristics (e.g. Our methods\nenables incremental computations and exploits the graph\u2019s spar-\nsity for fast execution. We denote with xv\u2208{0,1}Dthe\nD-dim. we assume the node-ids\nto beV={1, . , N}and the feature-ids to be F={1, ...,D}. Given a subsetVL\u2286V of labeled nodes, with class labels from\nC={1,2, . , cK}, the goal of node classification is to learn a\nfunction\u0434:V\u2192C which maps each node v\u2208V to one class\ninC.2Since the predictions are done for the given test instances,\nwhich are already known before (and also used during) training,\nthis corresponds to a typical transductive learning scenario [8]. In this work, we focus on node classification employing graph\nconvolution layers. In particular, we will consider the well estab-\nlished work [19]. Here, the hidden layer l+1is defined as\nH(l+1 )=\u03c3\u0012\n\u02dcD\u22121\n2\u02dcA\u02dcD\u22121\n2H(l)W(l)\u0013\n, (1)\nwhere \u02dcA=A+INis the adjacency matrix of the (undirected) input\ngraph Gafter adding self-loops via the identity matrix IN.W(l)\nis the trainable weight matrix of layer l,\u02dcDii=P\nj\u02dcAij, and\u03c3(\u00b7)is\nan activation function (usually ReLU). In the first layer we have\nH(0)=X, i.e. using the nodes\u2019 features as input. Since the latent\nrepresentations Hare (recursively) relying on the neighboring ones\n(multiplication with \u02dcA), all instances are coupled together. The optimal parameters \u03b8are\n2Please note the difference to (structured) learning settings where we have multiple\nbut independent graphs as training input with the goal to perform a prediction for each\ngraph. In this work, the prediction is done per node (e.g. minimizing\nL(\u03b8;A,X)=\u2212X\nv\u2208VLlnZv,cv,Z=f\u03b8(A,X) (3)\nwhere cvis the given label of vfrom the training set. After training,\nZdenotes the class probabilities for every instance in the graph. 3 RELATED WORK\nIn line with the focus of this work, we briefly describe deep learning\nmethods for graphs aiming to solve the node classification task. In this work,\nwe focus on the second type of principles and additionally show\nthat our adversarial attack transfers to node embeddings as well. A more general setting is described in [ 13] and an overview\nof methods given in [7, 25]. Recently, deep neural networks have shown to be highly sensitive\nto these small adversarial perturbations to the data [ 14,30]. Many tasks and models have been shown to be sensitive to\nadversarial attacks; however, all assume the data instances to be in-\ndependent. The two dominant types of attacks are poisoning/causative\nattacks which target the training data (specifically, the model\u2019s train-\ning phase is performed after the attack) and evasion/exploratory at-\ntacks which target the test data/application phase (here, the learned\nmodel is assumed fixed). Deriving effective poisoning attacks is\nusually computationally harder since also the subsequent learning\nof the model has to be considered. In particular, attacks on the test data are\ncausative as well since the test data is used while training the model\n(transductive, semi-supervised learning). Indeed, almost all works exploit the gradient or\nother moments of a given differentiable (surrogate) loss function to\nguide the search in the neighborhood of legitimate perturbations\n[14,15,20,23,26]. Our work [ 6] considered noise in the graph structure\nto improve the robustness when performing spectral clustering. Similarly, to improve robustness of collective classification via asso-\nciative Markov networks, the work [ 31] considers adversarial noise\nin the features. They only use label smoothness and assume that\nthe attacker can manipulate the features of every instance. 2, our goal\nis to perform small perturbations on the graph G(0)=(A(0),X(0)),\nleading to the graph G\u2032= (A\u2032,X\u2032), such that the classification\nperformance drops. Specifically, our goal is to attack a specific\ntarget node v0\u2208V, i.e. we aim to change v0\u2019s prediction. Thus, we are\nnot limited to perturbing v0but we can achieve our aim by changing\nother nodes as well. Therefore, besides the target\nnode, we introduce the attacker nodes A\u2286V . The perturbations\nonG(0)are constrained to these nodes, i.e. it must hold\nX\u2032\nui,X(0)\nui\u21d2u\u2208A ,A\u2032\nuv,A(0)\nuv\u21d2u\u2208A\u2228v\u2208A (4)\nIf the target v0<A, we call the attack an influencer attack ,\nsincev0gets not manipulated directly, but only indirectly via some\ninfluencers. Given a graph G(0)=(A(0),X(0)), a target node v0,\nand attacker nodes A. Let colddenote the class for v0based on the\ngraph G(0)(predicted or using some ground truth). Determine\narg max\n(A\u2032,X\u2032)\u2208PG0\n\u2206,Amaxc,coldlnZ\u2217\nv0,c\u2212lnZ\u2217\nv0,cold\nsubject to Z\u2217=f\u03b8\u2217(A\u2032,X\u2032)with\u03b8\u2217=arg min\n\u03b8L(\u03b8;A\u2032,X\u2032)\nThat is, we aim to find a perturbed graph G\u2032that classifies v0as\ncnewand has maximal \u2019distance\u2019 (in terms of log-probabilities/logits)\ntocold. Note that for the perturbated graph G\u2032, the optimal param-\neters\u03b8\u2217are used, matching the transductive learning setting where\nthe model is learned on the given data. How can we ensure unnoticeable perturbations in our setting? Especially if a large \u2206is required due to complicated data,\nwe still want realistically looking perturbed graphs G\u2032. Therefore,\nour core idea is to allow only those perturbations that preserve\nspecific inherent properties of the input graph. If two networks show very different degree distributions,\nit is easy to tell them apart. Therefore, we aim to only generate per-\nturbations which follow similar power-law behavior as the input. For this purpose we refer to a statistical two-sample test for\npower-law distributions [ 2]. That is, we estimate whether the two\ndegree distributions of G(0)andG\u2032stem from the same distribution\nor from individual ones, using a likelihood ratio test. More precisely, the procedure is as follows: We first estimate\nthe scaling parameter \u03b1of the power-law distribution p(x)\u221dx\u2212\u03b1\nreferring to the degree distribution of G(0)(equivalently for G\u2032). Using this, we get estimates for the\nvalues\u03b1G(0)and\u03b1G\u2032. Similarly, we can estimate \u03b1comb using the\ncombined samples Dcomb =DG(0)\u222aD G\u2032.Given the scaling parameter \u03b1x, the log-likelihood for the sam-\nplesDxcan easily be evaluated as\nl(Dx)=|Dx|\u00b7log\u03b1x+|Dx|\u00b7\u03b1x\u00b7logdmin+(\u03b1x+1)X\ndi\u2208Dxlogdi(7)\nUsing these log-likelihood scores, we set up the significance test,\nestimating whether the two samples DG(0)andDG\u2032come from the\nsame power law distribution (null hypotheses H0) as opposed to\nseparate ones ( H1). That is, we formulate two competing hypotheses\nl(H0)=l(Dcomb )and l(H1)=l(DG(0))+l(DG\u2032)(8)\nFollowing the likelihood ratio test, the final test statistic is\n\u039b(G(0),G\u2032)=\u22122\u00b7l(H0)+2\u00b7l(H1). A typical p-value for rejecting the null hypothesis H0(i.e. con-\ncluding that both samples come from different distributions) is\n0.05, i.e., statistically, in one out of twenty cases we reject the null\nhypothesis although it holds (type I error ). In our adversarial at-\ntack scenario, however, we argue that a human trying to find out\nwhether the data has been manipulated would be far more conser-\nvative and ask the other way: Given that the data was manipulated,\nwhat is the probability of the test falsely not rejecting the null\nhypothesis (type II error ). While we cannot compute the type II error in our case easily,\ntype I and II error probabilities have an inverse relation in general. Thus, by selecting a very conservative p-value corresponding to a\nhigh type I error, we can reduce the probability of a type II error. We therefore set the critical p-value to 0.95, i.e. pre-\nserving the distribution of feature occurrences), we argue that such\na procedure is too limited. Such a change, however, is easily noticable. Thus, we refer to a test based on feature co-occurrence. Since designing a statistical test based on the co-occurences re-\nquires to model the joint distribution over features \u2013 intractable\nfor correlated multivariate binary data [ 24] \u2013 we refer to a deter-\nministic test. In this regard, setting features to 0 is uncritical since\nit does not introduce new co-occurences. We argue that adding a feature iis\nunnoticeable if the probability of reaching it by a random walker\nstarting at the features originally present for node uand performing\none step is significantly large. Formally, let Su={j|Xuj,0}be\nthe set of all features originally present for node u. We consider\naddition of feature i<Suto node uas unnoticeable if\np(i|Su)=1\n|Su|X\nj\u2208Su1/dj\u00b7Eij>\u03c3. in other\nnodes) have a high probability; they are less noticeable when being\nadded. Second, features ithat only co-occur with features j\u2208Su\nthat are not specific to the node u(e.g. features jwhich co-occur\nwith almost every other feature; stopwords) have low probability;\nadding iwould be noticeable. Using the above test, we only accept perturbations G\u2032=(A\u2032,X\u2032)\nwhere the feature values fulfill\n\u2200u\u2208V :\u2200i\u2208F :X\u2032\nui=1\u21d2i\u2208Su\u2228p(i|Su)>\u03c3 (12)\nIn summary, to ensure unnoticeable perturbations, we update\nour problem definition to:\nProblem 2. 5 GENERATING ADVERSARIAL GRAPHS\nSolving Problem 1/2 is highly challenging. Therefore, we propose a\nsequential approach, where we first attack a surrogate model, thus,\nleading to an attacked graph. This graph is subsequently used to\ntrain the final model. Indeed, this approach can directly be consid-\nered as a check for transferability since we do not specifically focus\non the used model but only on a surrogate one. To obtain a tractable surrogate model that\nstill captures the idea of graph convolutions, we perform a lin-\nearizion of the model from Eq. That is, we replace the non-\nlinearity\u03c3(. Since our goal is to maximize the difference in the log-probabilities\nof the target v0(given a certain budget \u2206), the instance-dependent\nnormalization induced by the softmax can be ignored. Thus, thelog-probabilities can simply be reduced to \u02c6A2XW. Accordingly,\ngiven the trained surrogate model on the (uncorrupted) input data\nwith learned parameters W, we define the surrogate loss\nLs(A,X;W,v0)=maxc,cold[\u02c6A2XW]v0c\u2212[\u02c6A2XW]v0cold (14)\nand aim to solve arg max\n(A\u2032,X\u2032)\u2208\u02c6PG0\n\u2206,ALs(A\u2032,X\u2032;W,v0). For this, we define scoring functions that evaluate the sur-\nrogate loss from Eq. (14) obtained after adding/removing a feature\nf=(u,i)or edge e=(u,v)to an arbitrary graph G=(A,X):\nsstruct (e;G,v0):=Ls(A\u2032,X;W,v0) (15)\nsf eat (f;G,v0):=Ls(A,X\u2032;W,v0) (16)\nwhere A\u2032:=A\u00b1e(i.e.a\u2032uv=a\u2032vu=1\u2212auv)3andX\u2032:=X\u00b1f(i.e. In detail, following a locally optimal strategy, we sequentially \u2019ma-\nnipulate\u2019 the most promising element: either an entry from the\nadjacency matrix or a feature entry (taking the constraints into ac-\ncount). That is, given the current state of the graph G(t), we compute\na candidate set Cstruct of allowable elements (u,v)whose change\nfrom 0 to 1 (or vice versa; hence the \u00b1sign in the pseudocode) does\nnot violate the constraints imposed by \u02c6PG0\n\u2206,A. Among these ele-\nments we pick the one which obtains the highest difference in the\nlog-probabilites, indicated by the score function sstruct (e;G(t),v0). Similar, we compute the candidate set Cf eat and the score func-\ntionsf eat (f;G(t),v0)for every allowable feature manipulation of\nfeature iand node u. Whichever change obtains the higher score is\npicked and the graph accordingly updated to G(t+1 ). This process\nis repeated until the budget \u2206has been exceeded. For this, we have to compute the class prediction (in the surrogate\nmodel) of node v0after adding/removing an edge (m,n). Since we\nare now optimizing w.r.t. (14)is a constant \u2013\nwe substitute it with C:=XW\u2208RN\u00d7K. The log-probabilities of\nnodev0are then given by \u0434v0=[\u02c6A2]v0\u00b7C\u2208R1\u00d7Kwhere [\u02c6A2]v0\ndenotes a row vector. Thus, we only have to inspect how this row\nvector changes to determine the optimal edge manipulation. An important observation to\nalleviate this problem is that in the used two-layer GCN the predic-\ntion for each node is influenced by its two-hop neighborhood only. That is, the above row vector is zero for most of the elements. Research Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2851 Algorithm 1: Nettack: Adversarial attacks on graphs\nInput: Graph G(0)\u2190 (A(0),X(0)), target node v0,\nattacker nodesA, modification budget \u2206\nOutput: Modified Graph G\u2032=(A\u2032,X\u2032)\nTrain surrogate model on G(0)to obtain W// Eq. Given an adjacency matrix A, and its corresponding\nmatrices \u02dcA,\u02c6A2,\u02dcD. Denote with A\u2032the adjacency matrix when adding\nor removing the element e=(m,n)from A. LetSandS\u2032be defined as S=PN\nk=1aukakv\n\u02dcdkandS\u2032=\nPN\nk=1a\u2032\nuka\u2032\nkv\n\u02dcd\u2032\nk. We have [\u02c6A]uv=\u02dcauvq\n\u02dcdu\u02dcdv. Ifu,v, then\n[\u02c6A2]uv=NX\nk=1[\u02c6A]uk[\u02c6A]kv=\u02dcauv\n\u02dcduq\n\u02dcdu\u02dcdv+\u02dcauv\n\u02dcdvq\n\u02dcdu\u02dcdv+1q\n\u02dcdu\u02dcdvS. After replacing S\u2032\u2212S=\u2212aumamv\n\u02dcdm+a\u2032\numa\u2032\nmv\n\u02dcd\u2032m\u2212aunanv\n\u02dcdn+a\u2032\nuna\u2032\nnv\n\u02dcd\u2032nin\nthe above equation, it is straightforward to derive Eq. Deriving\nthis equation for the case u=vis similar. (17) enables us to update the entries in \u02c6A2inconstant time ;\nand in a sparse and incremental manner. Remember that all \u02dcauv,\nauv, and a\u2032uvare either 1 or 0, and their corresponding matrices\nare sparse. (15) can be easily computed. Indeed, by fixing the class c,coldwith currently largest log-\nprobability score [\u02c6A2XW]v0c, the problem is linear in Xand every\nentry of Xacts independently. Thus, to find the best node and\nfeature (u\u2217,i\u2217)we only need to compute the gradients\n\u03d2ui=\u2202\n\u2202Xui\u0010\n[\u02c6A2XW]v0c\u2212[\u02c6A2XW]v0cold\u0011\n=[\u02c6A2]v0u\u0010\n[W]ic\u2212[W]icold\u0011\nand subsequently pick the one with the highest absolute value that\npoints into an allowable direction (e.g. if the feature was 0, the\ngradient needs to point into the positives). The value of the score\nfunction sf eat for this best element is then simply obtained by\nadding|\u03d2ui|to the current value of the loss function:\nLs(A,X;W,v0)+|\u03d2ui|\u00b7I[(2\u00b7Xui\u22121)\u00b7\u03d2ui<0]\nAll this can be done in constant time per feature. The elements\nwhere the gradient points outside the allowable direction should\nnot be perturbed since they would only hinder the attack \u2013 thus,\nthe old score stays unchanged. 5.2 Fast computation of candidate sets\nLast, we have to make sure that all perturbations are valid according\nto the constraints \u02c6PG0\n\u2206,A. 4 and 5 are easy\nto ensure. 4 can be precomputed. Likewise, the node-feature\ncombinations fulfilling the co-occurence test of Eq. 12 can be pre-\ncomputed. Thus, the set Cf eat only needs to be instantiated once. The significance test for the degree distribution, however, does\nnot allow such a precomputation since the underlying degree distri-\nbution dynamically changes. How can we efficiently check whether\na potential perturbation of the edge (m,n)still preserves a similar\ndegree distribution? Indeed, since the individual degrees only in-\nteract additively, we can again derive a constant time incremental\nupdate of our test statistic \u039b.\nTheorem 5.2. Denote with RG=P\ndi\u2208DGlogdithe sum of log degrees. Different variants of our method on the Cora data. 0 100 200\nStructure perturbations0.00.5\u039bNo constraint\nWith constraintFigure 3: Change in test\nstatistic \u039b(degree distr.) The term I[dm+1\u2212amn=dmin]\u00b7xwill be activated\n(i.e. non-zero) only in two cases: 1) amn=1(i.e.G\u2032=G\u2212e), and\ndm=dmin, then x<0and the update equation actually removes\nnode mfromDG. 2)amn=0(i.e.G\u2032=G+e), and dm=dmin\u22121,\nthen x>0and the update equation actually adds node mtoDG. A similar argumentation is applicable for node n. Accordingly, we\nhave that ne=|DG\u2032|. Similarly, one can show the valid incremental update for RG\u2032\nconsidering that only nodes with degree larger than dminare con-\nsidered and that dm+xis the new degree. Having incremental\nupdates for neandRG\u2032, the updates for \u03b1G\u2032andl(DG\u2032)follow\neasily from their definitions. \u25a1\nGiven G(t), we can now incrementally compute l(DG(t)\ne), where\nG(t)\ne=G(t)\u00b1e. Equivalently we get incremental updates for\nl(Dcomb )after an edge perturbation. Since RG(t)can\nbe incrementally updated to RG(t+1 )once the best edge perturbation\nhas been performed, the full approach is highly efficient. which edges/features are allowed\nto change) and the score functions can be incrementally computed\nand exploit the graph\u2019s sparsity, thus, ensuring scalability. The\nruntime complexity of the algorithm can easily be determined as:\nO(\u2206\u00b7|A|\u00b7 (N\u00b7thv0+D))\nwhere thv0indicates the size of the two-hop neighborhood of the\nnodev0during the run of the algorithm. In every of the \u2206many iterations, each attacker evaluates the po-\ntential edge perturbations ( Nat most) and feature perturbations ( Dat most). For the former, this requires to update the two-hop neigh-\nborhood of the target due to the two convolution layers. Assuming\nthe graph is sparse, thv0is much smaller than N. The feature pertur-\nbations are done in constant time per feature. Since all constraints\ncan be checked in constant time they do not affect the complexity. We only consider the largest con-\nnected component (LCC). Setup. We use the well-known Cora-ML andCiteseer networks\nas in [ 5], and Polblogs [1]. We split the network in labeled (20%) and unlabeled nodes\n(80%). We further split the labeled nodes in equal parts training and\nvalidation sets to train our surrogate model. That is, we remove\nthe labels from the validation set in the training procedure and\nuse them as the stopping criterion (i.e., stop when validation error\nincreases). The labels of the unlabeled nodes are never visible to\nthe surrogate model during training. We average over five different random initializations/ splits, where\nfor each we perform the following steps. We first train our surrogate\nmodel on the labeled data and among all nodes from the test set that\nhave been correctly classified, we select (i) the 10 nodes with highest\nmargin of classification, i.e. Then, we corrupt the input graph using the\nmodel proposed in this work, called Nettack for direct attacks, and\nNettack-In for influence attacks, respectively (picking 5 random\nnodes as attackers from the neighborhood of the target). Since no other competitors exist, we compare against two baselines:\n(i) Fast Gradient Sign Method (FGSM) [ 14] as a direct attack on\nv0(in our case also making sure that the result is still binary). Given our target node v0, in each step we randomly sample nodes\nufor which cv0,cuand add the edge u,vto the graph structure,\nassuming unequal class labels are hindering classification. 2 (left) we plot the\nsurrogate loss when performing a specific number of perturbations. Note that once the surrogate loss is positive, we realized a successful\nmisclassification. We analyze Nettack, and variants where we\nonly manipulate features or only the graph structure. Still, combining both\nis the most powerful, only requiring around 3 changes to obtain a\nmisclassification. 2 (right) we analyze our method when using a direct vs.\ninfluencer attack. Clearly, direct attacks need fewer perturbations\n\u2013 still, influencer attacks are also possible, posing a high risk in\nreal life scenarios. Thus, unnoticable perturbations can be generated. As seen the constraint we im-\npose has an effect on our attack;\nif not enforced, the power law\ndistribution of the corrupted graph becomes more and more dissim-\nilar to the original graph\u2019s. Further, we report for each feature its\noriginal occurence within the class. We see that the used features\nare indeed different \u2013 even more, the unconstrained version often\nuses words which are \u2019unlikely\u2019 for the class (indicated by the small\nnumbers). Our constraint ensures that the changes are more subtle. 5 we see that our algorithm scales linearly with the\nnumber of perturbations to the graph structure and the number of\ninfluencer nodes considered. Please note that we report runtime\nforsequential processing of candidate edges; this can however be\ntrivially parallelized. For this, we pursue the\napproach from before and use a budget of \u2206=dv0+2, where dv0is\nthe degree of target node we currently attack. This is motivated by\nthe observation that high-degree nodes are more difficult to attack\nthan low-degree ones. In the following we always report the score\nX=Z\u2217v0,cold\u2212max c,coldZ\u2217v0,cusing the ground truth label cold\nof the target. The smaller X,\nthe better. For values smaller than 0, the targets get misclassified. In the plot, every dot repre-\nsents one target node. Figure 6b and 6c show that the corruptions gen-\nerated by Nettack transfer to different (semi-supervised) graph\nconvolutional methods: GCN [ 19] and CLN [ 28]. Following [ 27],\nnode classification is performed by training a logistic regression on\nResearch Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2854 Nettack\nEvasionNettack\nPoisoningClean Nettack-In. Clean indicates the original data. the learned embeddings. In these plots, we\nalso compare against the two baselines Rnd andFGSM, both oper-\nating in the direct attack setting. Our adversarial perturbations on the sur-\nrogate model are transferable to all three models an on all datasets\nwe evaluated. 4 shows why this is the case: we plot the\ngradient vs. the actual change in loss when changing elements in A. One key advantage of Nettack is\nthat we can precisely and efficiently compute the change in Ls. We constructed these subgraphs by selecting\nnodes with increasing distance from v0, i.e. we first selected 1-hop\nneighbors, then 2-hop neighbors and so on, until we have reached\nAttack Cora Citese\ner Polblogs\nmethod GCN CLN DW GCN CLN DW GCN CLN DW\nClean 0.90\n0.84 0.82 0.88 0.76 0.71 0.93 0.92 0.63\nNettack 0.01 0.17 0.02 0.02 0.20 0.01 0.06 0.47 0.06\nFGSM 0.03 0.18 0.10 0.07 0.23 0.05 0.41 0.55 0.37\nRnd 0.61 0.52 0.46 0.60 0.52 0.38 0.36 0.56 0.30\nNett a\nck-In 0.67 0.68 0.59 0.62 0.54 0.48 0.86 0.62 0.91\nTable 3: Overview of results. 1(a) Direct attack\n20 40 60 80 100\nPerturbations0.4\n0.2\n0.00.20.40.60.81.0\nNettack 0.1\nNettack 0.2\nNettack 0.5\nNettack 0.75\nNettack 1 (b) Influence attack\nFigure 7: Attacks with limited knowledge about the data\nthe desired graph size. Note that Nettack\nhas always only seen the subgraph; and its surrogate model is also\nonly trained based on it. Clearly, if the attacker knows the full graph, the fewest number\nof perturbations is required. Here we require more perturbations and 75% of the\ngraph size for our attack to succeed. Still, this experiment indicates\nthat full knowledge is not required. Our attacks target the nodes\u2019 fea-\ntures and the graph structure. Exploiting the relational nature of the\ndata, we proposed direct and influencer attacks. To ensure unnotice-\nable changes even in a discrete, relational domain, we proposed to\npreserve the graph\u2019s degree distribution and feature co-occurrences. Even more, the\nattacks generalize to other node classification models. As future work we aim to derive extensions of\nexisting models to become more robust against attacks, and we aim\nto study tasks beyond node classification. Research Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2855 ACKNOWLEDGEMENTS\nThis research was supported by the German Research Foundation,\nEmmy Noether grant GU 1409/2-1, and by the Technical University\nof Munich - Institute for Advanced Study, funded by the German\nExcellence Initiative and the European Union Seventh Framework\nProgramme under grant agreement no 291763, co-funded by the\nEuropean Union. [4]Aleksandar Bojchevski and Stephan G\u00fcnnemann. Bayesian Robust Attrib-\nuted Graph Clustering: Joint Learning of Partial Anomalies and Group Structure. [5]Aleksandar Bojchevski and Stephan G\u00fcnnemann. Deep Gaussian Embed-\nding of Graphs: Unsupervised Inductive Learning via Ranking. [6]Aleksandar Bojchevski, Yves Matkovic, and Stephan G\u00fcnnemann. Robust\nSpectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent\nEmbeddings. In SIGKDD. [8]Olivier Chapelle, Bernhard Sch\u00f6lkopf, and Alexander Zien. SIAM review 51, 4 (2009), 661\u2013703. [11] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. [12] Dhivya Eswaran, Stephan G\u00fcnnemann, Christos Faloutsos, Disha Makhija, and\nMohit Kumar. In ICML. [14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. [15] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and\nPatrick McDaniel. [16] Aditya Grover and Jure Leskovec. In SIGKDD. [17] William L Hamilton, Rex Ying, and Jure Leskovec. [18] Bryan Hooi, Neil Shah, Alex Beutel, Stephan G\u00fcnnemann, Leman Akoglu, Mohit\nKumar, Disha Makhija, and Christos Faloutsos. BIRDNEST: Bayesian\nInference for Ratings-Fraud Detection. In SIAM SDM. Semi-supervised classification with graph\nconvolutional networks. Modeling Multivariate Correlated\nBinary Data. [25] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda,\nand Michael M Bronstein. [26] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,\nand Ananthram Swami. In SIGKDD. [28] Trang Pham, Truyen Tran, Dinh Q. Phung, and Svetha Venkatesh. Column\nNetworks for Collective Classification. [29] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and\nTina Eliassi-Rad. [30] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Google Inc, Joan Bruna,\nDumitru Erhan, Google Inc, Ian Goodfellow, and Rob Fergus. [31] Mohamad Ali Torkamani and Daniel Lowd. In ICML. [32] Florian Tram\u00e8r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-\nDaniel. arXiv preprint\narXiv:1704.03453 (2017).",
        "Results and Findings": "Yet, in domains where they are likely to be used, e.g. Our experimental study shows that accuracy of node\nclassification significantly drops even when performing only few\nperturbations. 2018. ACM, New York, NY, USA,\n10 pages. Publication rights licensed to the\nAssociation for Computing Machinery. ACM ISBN 978-1-4503-5552-0/18/08. . . Clean\ngraph-1.0-0.50.00.51.0Classi\ufb01cation margin\nmisclassi\ufb01ed\nnodesResults for attacking Citeseer dataFigure 1: Small perturbations of the graph structure and\nnode features lead to misclassification of the target. Specifically, approaches from the class of graph\nconvolutional networks [ 19,28] have achieved strong performance\nin many graph-learning tasks including node classification. How reliable are their results? \u2022Experiments: We show that our model can dramatically worsen\nclassification results for the target nodes by only requiring few\nchanges to the graph. We furthermore show that these results\ntransfer to other established models, hold for various datasets,\nand even work when only parts of the data are observed. . . . . \u03b8={W(1),W(2)}. instances within each task. nature of the data, v0\u2019s outcome not only depends on the\nnode itself, but also on the other nodes in the graph. (4)and(5). Unlike\nto image data, where this can easily be verified visually and by using\nsimple constraints, in the graph setting this is much harder mainly\nfor two reasons: (i) the graph structure is discrete preventing to use\ninfinitesimal small changes, and (ii) sufficiently large graphs are\nnot suitable for visual inspection. if we were to sample\ntwo degree sequences from the same power law distribution, we\nwere to reject the null hypothesis in 95%of the times and could\nthen investigate whether the data has been compromised based on\nthis initial suspicion. In our experiments we simply picked \u03c3to be half of\nthe maximal achievable probably, i.e. \u03c3=0.5\u00b71\n|Su|P\nj\u2208Su1/dj. In the following, we describe these two parts in detail. (13);\nt\u21900;\nwhile|A(t)\u2212A(0)|+|X(t)\u2212X(0)|<\u2206do\nCstruct\u2190candidate_edge_perturbations (A(t),A);\ne\u2217=(u\u2217,v\u2217)\u2190arg max\ne\u2208Cstructsstruct\u0010\ne;G(t),v0\u0011\n;\nCf eat\u2190candidate_feature_perturbations (X(t),A);\nf\u2217=(u\u2217,i\u2217)\u2190arg max\nf\u2208Cf eatsf eat\u0010\nf;G(t),v0\u0011\n;\nifsstruct (e\u2217;G(t),v0)>sf eat (f\u2217;G(t),v0)then\nG(t+1 )\u2190G(t)\u00b1e\u2217;\nelse G(t+1 )\u2190G(t)\u00b1f\u2217;\nt\u2190t+1;\nreturn : G(t)\n// Train final graph model on the corrupted graph G(t);\neven more important, we can derive an incremental update \u2013 we\ndon\u2019t have to recompute the updated [\u02c6A2]v0from scratch. It holds:\n[\u02c6A\u20322]uv=1q\n\u02dcd\u2032u\u02dcd\u2032v q\n\u02dcdu\u02dcdv[\u02c6A2]uv\u2212\u02dcauv\n\u02dcdu\u2212auv\n\u02dcdv+a\u2032\nuv\n\u02dcd\u2032v+\u02dca\u2032\nuv\n\u02dcd\u2032u\u2212\n\u2212aumamv\n\u02dcdm+a\u2032\numa\u2032\nmv\n\u02dcd\u2032m\u2212aunanv\n\u02dcdn+a\u2032\nuna\u2032\nnv\n\u02dcd\u2032n\u0013\n(17)\nwhere \u02dcd\u2032,a\u2032, and \u02dca\u2032, are defined as (using the Iverson bracket I):\n\u02dcd\u2032\nk=\u02dcdk+I[k\u2208{m,n}]\u00b7(1\u22122\u00b7amn )\na\u2032\nkl=akl+I[{k,l}={m,n}]\u00b7(1\u22122\u00b7akl)\n\u02dca\u2032\nkl=\u02dcakl+I[{k,l}={m,n}]\u00b7(1\u22122\u00b7\u02dcakl)\nProof. Having the above equation for \u02c6A\u20322, we get\n[\u02c6A\u20322]uv\u0010q\n\u02dcd\u2032u\u02dcd\u2032v\u0011\n\u2212[\u02c6A2]uv\u0010q\n\u02dcdu\u02dcdv\u0011\n=\nf\u02dca\u2032uv\n\u02dcd\u2032u\u2212\u02dcauv\n\u02dcdug\n+f\u02dca\u2032uv\n\u02dcd\u2032v\u2212\u02dcauv\n\u02dcdvg\n+(S\u2032\u2212S). Eq. 17 encompasses both\ncases. \u25a1Eq. Lete=(m,n)be a candidate edge perturbation, and dmanddnthe\ndegrees of the nodes in G. For G\u2032=G\u00b1ewe have:\n\u03b1G\u2032=1+nef\nRG\u2032\u2212nelog\u0010\ndmin\u22121\n2\u0011g\u22121(18)\nl\u0010\nDG\u2032\u0011\n=nelog\u03b1G\u2032+ne\u03b1G\u2032logdmin+\u0010\n\u03b1G\u2032+1\u0011\nRG\u2032(19)\nResearch Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2852 0 20 40 60 80 100\nPerturbations-100102030Surrogate lossLsNettack\nNettack-Struct\nNettack-Feat\nRnd. 0 20 40 60 80 100\nPerturbations-100102030Surrogate lossLsNettack-U\nNettack\nNettack-In-U\nNettack-In\nRnd.Figure 2: Average surrogate loss for increasing number of perturbations. Ls\n0 0.25 0.5 0.75 1\nAuvLsLs\n@Ls\n@AuvFigure 4: Gradient vs.\nactual loss\nwhere\nx=1\u22122\u00b7amn (20)\nne=|DG|+(I[dm+1\u2212amn=dmin]+I[dn+1\u2212amn=dmin])\u00b7x\nRG\u2032=RG\u2212I[dm\u2265dmin] logdm+I[dm+x\u2265dmin] log (dm+x)\n\u2212I[dn\u2265dmin] logdn+I[dn+x\u2265dmin] log (dn+x). Firstly, we show that if we incrementally compute ne\naccording to the update equation of Theorem 5.2, newill be equal\nto|DG\u2032|. Since all r.h.s. of the equa-\ntions above can be computed in constant time, also the test statistic\n\u039b(G(0),G(t)\ne)can be computed in constant time. they are clearly correctly classified,\n(ii) the 10 nodes with lowest margin (but still correctly classified)\nand (iii) 20 more nodes randomly. Research Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2853 Class: neural\nnetworks Class: theory Class: probabilistic models\nconstrained unconstrained constrained unconstrained constrained unconstrained\nprobabilistic 25 efforts 2 driven 3 designer 0 difference 2 calls 1\nprobability 38 david 0 increase 8 assist 0 solve 3 chemical 0\nbayesian 28 averages 2 heuristic 4 disjunctive 7 previously 12 unseen 1\ninference 27 accomplished 3 approach 56 interface 1 control 16 corporation 3\nprobabilities 20 generality 1 describes 20 driven 3 reported 1 fourier 1\nobservations 9 expectation 10 performing 7 refinement 0 represents 8 expressed 2\nestimation 35 specifications 0 allow 11 refines 0 steps 5 robots 0\ndistributions 21 family 10 functional 2 starts 1 allowing 7 achieving 0\nindependence 5 uncertain 3 11 3 restrict 0 task 17 difference 2\nvariant 9 observations 9 acquisition 1 management 0 expressed 2 requirement 1\nTable 2: Top-10 feature perturbations per class on Cora\n6.1 Attacks on the surrogate model\nWe start by analyzing different variants of our method by inspecting\ntheir influence on the surrogate model. In Fig. As seen,\nperturbations in the structure lead to a stronger change in the\nsurrogate loss compared to feature attacks. In Fig. Similarly, Table 2 illustrates the result for\nthe feature perturbations. Similar results were obtained for the runtime\nw.r.t. 6.2 Transferability of attacks\nAfter exploring how our attack affects the (fixed) surrogate model,\nwe will now find out whether our attacks are also successful on\nestablished deep learning models for graphs. In Figure 6a we evaluate Net-\ntack\u2019s performance for two attack types: evasion attacks, where\nthe model parameters (here of GCN [ 19]) are kept fix based on the\nclean graph; and poisoning attacks, where the model is retrained\nafter the attack (averaged over 10 runs). Since poisoining attacks are in general harder\nand match better the transductive learning scenario, we report in\nthe following only these results. Comparison. Clean Nettack-In.-1.0-0.50.00.51.0\nmisclassi\ufb01ed\nnodes (d) Poisoning of DeepWalk\nFigure 6: Results on Cora data using different attack algorithms. Lower scores are better. Again note: All these results are obtained using a challenging\npoisoning attack (i.e. In Table 3 we summarize the results for different datasets and\nclassification models. Here, we report the fraction of target nodes\nthat get correctly classified. Not surprisingly, influencer attacks lead to a lower\ndecrease in performance compared to direct attacks. We see that FGSM performs worse than Nettack, and we argue\nthat this comes from the fact that gradient methods are not optimal\nfor discrete data. Fig. [1;5] [6;10] [11;20] [21;100] [100; \u221e)\nClean 0.878 0.823 1.0 1.0 1.0\nNettack 0.003 0.009 0.014 0.036 0.05\nThe table shows results for different degree ranges. In the previous experiments, we have as-\nsumed full knowledge of the input graph, which is a reasonable\nassumption for a worst-case attack. In Fig. 7 we analyze the result\nwhen having limited knowledge: Given a target node v0, we pro-\nvided our model only subgraphs of increasing size relative to the\nsize of the Cora graph. 0.1\nRnd. 0.2\nRnd. 0.5\nRnd. 0.75\nRnd. Fig. In Fig. The political blogosphere and the\n2004 US election: divided they blog. 36\u201343. [2]Alessandro Bessi. 2015. arXiv preprint arXiv:1503.00643 (2015). [3]Battista Biggio, Giorgio Fumera, and Fabio Roli. IEEE TKDE 26, 4 (2014), 984\u2013996. 2018. 2738\u20132745. 2018. In ICLR. 2017. 737\u2013746. [7]Hongyun Cai, Vincent W Zheng, and Kevin Chang. 2018. 2006. The MIT Press. [9]Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto\nPerdisci, Manos Antonakakis, and Nikolaos Vasiloglou. 2017. arXiv preprint arXiv:1708.09056 (2017). [10] Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. 2016. Convolu-\ntional neural networks on graphs with fast localized spectral filtering. In NIPS. 3837\u20133845. 2017. PVLDB 10, 5 (2017), 625\u2013636. [13] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.\nDahl. 2017. 1263\u20131272. 2015. In ICLR. 2017. 62\u201379. 855\u2013864. 2017. In NIPS. 2016. 495\u2013503. [19] Thomas N Kipf and Max Welling. 2017. In ICLR. [20] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. In NIPS. 1885\u20131893. [21] Ben London and Lise Getoor. [22] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. [23] Shike Mei and Xiaojin Zhu. 2015. 2016. 2017. 2016. 372\u2013387. [27] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 701\u2013710. 2017. AI magazine 29,\n3 (2008), 93. In ICLR. 642\u2013650. 2017. [33] Mengchen Zhao, Bo An, Yaodong Yu, Sulin Liu, and Sinno Jialin Pan. 2018. 2628\u20132635. Research Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2856",
        "Conclusion": "Therefore, gradient based approaches [ 14,23] for finding\nperturbations are not suited. Due to the\nnon-i.i.d. 4.1. On the other hand, if our modified graph\u2019s\ndegree sequence passes this very conservative test, we conclude\nthat the changes to the degree distribution are unnoticeable. Thus, we obtain the desired result. 12). 2. x\u2032\nui=1\u2212xui). Approximate Solution. 17. Given this highly efficient update of [\u02c6A2]v0to[\u02c6A\u20322]v0,\nthe updated log-probabilities and, thus, the final score according to\nEq. 6). Larger is better. Proof. As seen, even when using our constraints, the attack is\nstill succesfull. Overall, we conclude that attacking the features and structure\nsimultaneously is very powerful; and the introduced constraints do\nnot hinder the attack while generating more realistic perturbations. Direct attacks are clearly easier than influencer attacks.Lastly, even though not our major focus, we want to analyze\nthe required runtime of Nettack. As seen, direct attacks are extremly succesful\n\u2013 even for the challening poisoning case almost every target gets\nmisclassified. We therefore conclude that our surrogate model and\nloss are a sufficient approximation of the true loss on the non-linear\nmodel after re-training on the perturbed data. Overall, we see that direct attacks pose\na much harder problem than influencer attacks. As shown, Nettack outperforms\nboth. Last, we also analyzed how the structure of the target, i.e. As seen, high\ndegree nodes are slightly harder to attack: they have both, higher\nclassification accuracy in the clean graph and in the attacked graph. Smaller is better. 7 shows the result for a direct attack. As seen, even if only\n10% of the graph is observed, we can still significantly attack it. 7 CONCLUSION\nWe presented the first work on adversarial attacks to (attributed)\ngraphs, specifically focusing on the task of node classification via\ngraph convolutional networks. Based on our extensive experiments we can conclude\nthat even the challenging poisoning attack is successful possible\nwith our approach. 2005. 2014. IEEE TKDE\n(2018). 2009. 2014. 2000. Automating the construction of internet portals with machine learning. 2871\u20132877. [24] Ahmed Mohamed Mohamed El-Sayed. 1. 3. In IEEE European Symposium on Security and Privacy. 2014. 2485\u20132491. 2008. 2014. 2013.",
        "label": 3
    },
    {
        "Abstract": "Real-time Personalization using Embeddings for Search\nRanking at Airbnb\nMihajlo Grbovic\nAirbnb, Inc.\nSan Francisco, California, USA\nmihajlo.grbovic@airbnb.comHaibin Cheng\nAirbnb, Inc.\nSan Francisco, California, USA\nhaibin.cheng@airbnb.com\nABSTRACT\nSearch Ranking and Recommendations are fundamental problems\nof crucial interest to major Internet companies, including web\nsearch engines, content publishing websites and marketplaces. https://doi.org/10.1145/3219819.3219885\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Compared to previously published work on embeddings for per-\nsonalization on the Web, novel contributions of this paper are:\n\u2022Real-time Personalization - Most of the previous work\non personalization and item recommendations using embed-\ndings [ 8,11] is deployed to production by forming tables of\nuser-item and item-item recommendations offline, and then\nreading from them at the time of recommendation. 3.1 Listing Embeddings\nLet us assume we are given a set SofSclick sessions obtained\nfrom Nusers, where each session s=(l1, . More formally, the objective of the model is to learn listing\nrepresentations using the skip-gram model [ 17] by maximizing\nthe objective function Lover the entire set Sof search sessions,\ndefined as follows\nL=\u00d5\ns\u2208S\u00d5\nli\u2208s\u0000\u00d5\n\u2212m\u2265j\u2264m,i,0logP(li+j|li)\u0001, (1)\nProbability P(li+j|li)of observing a listing li+jfrom the contextual\nneighborhood of clicked listing liis defined using the soft-max\nP(li+j|li)=exp(v\u22a4\nliv\u2032\nli+j)\n\u00cd|V|\nl=1exp(v\u22a4\nliv\u2032\nl), (2)\nwhere vlandv\u2032\nlare the input and output vector representations of\nlisting l, hyperparameter mis defined as a length of the relevant\nforward looking and backward looking context (neighborhood) for\na clicked listing, and Vis a vocabulary defined as a set of unique\nlistings ids in the data set. We can break down the\nclick sessions setSinto 1) booked sessions, i.e. Note that each session consists of bookings by same\nuser_id, however for a single user_idtheir user_types can change\nover time, similarly to how listin\u0434_types for the same listing can\nchange over time as they receive more bookings. ones with less space, lower price and small number of reviews\nhave low cosine similarity. For conciseness we will not list all of them. ACM, 1819\u20131828. ACM, 1933\u20131942.",
        "Methodology": "How-\never, despite sharing some common characteristics a one-size-fits-\nall solution does not exist in this space. Given a large difference in\ncontent that needs to be ranked, personalized and recommended,\neach marketplace has a somewhat unique challenge. We conducted rigorous offline test-\ning of the embedding models, followed by successful online tests\nbefore fully deploying them into production. In Proceedings of The 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining,\nLondon, United Kingdom, August 19\u201323, 2018 (KDD \u201918), 10 pages. Copyrights for components of this work owned by others than ACM\nmust be honored. Request permissions from permissions@acm.org. KDD \u201918, August 19\u201323, 2018, London, United Kingdom\n\u00a92018 Association for Computing Machinery. clicks and time spent on news articles that are be-\ning searched), others aim at maximizing conversions (e.g. are appealing to the guest and,\nat the same time, are a good match in terms of host preferences for\ntrip duration and lead days. Furthermore, we need to detect listings\nthat would likely reject the guest due to bad reviews, pets, length\nof stay, group size or any other factor, and rank these listings lower. Specifically, we\nformulate the problem as pairwise regression with positive utilities\nfor bookings and negative utilities for rejections, which we optimize\nusing a modified version of Lambda Rank [ 4] model that jointly\noptimizes ranking for both sides of the marketplace. click on more than one listing and contact more than one host\nduring their search session, we can use these in-session signals, i.e. At the same time\nwe can use the negative signal, e.g. To be able to calculate similarities between listings\nthat guest interacted with and candidate listings that need to be\nranked we propose to use listing embeddings, low-dimensional\nvector representations learned from search sessions. We leverage\nthese similarities to create personalization features for our Search\nRanking Model and to power our Similar Listing Recommendations,\nthe two platforms that drive 99% of bookings at Airbnb. In addition to Real-time Personalization using immediate user\nactions, such as clicks, that can be used as proxy signal for short-\nterm user interest, we introduce another type of embeddings trained\non bookings to be able to capture user\u2019s long-term interest. Due\nto the nature of travel business, where users travel 1-2 times per\nyear on average, bookings are a sparse signal, with a long tail of\nusers with a single booking. To tackle this we propose to train\nembeddings at a level of user type, instead of a particular user id,\nwhere type is determined using many-to-one rule-based mapping\nthat leverages known user attributes. At the same time we learn\nlisting type embeddings in the same vector space as user type\nembeddings. This enables us to calculate similarities between user\ntype embedding of the user who is conducting a search and listing\ntype embeddings of candidate listings that need to be ranked. We imple-\nmented a solution where embeddings of items that user most\nrecently interacted with are combined in an online manner\nto calculate similarities to items that need to be ranked. We\nadapted the embedding training algorithm to take this into\naccount when doing negative sampling, which lead to cap-\nturing better within-market listings similarities. When learning listing embeddings\nwe treat the booked listing as global context that is always\nbeing predicted as the window moves over the session. \u2022User Type Embeddings - Previous work on training user\nembeddings to capture their long-term interest [ 6,27] train\na separate embedding for each user. When target signal is\nsparse, there is not enough data to train a good embedding\nrepresentation for each user. Not to mention that storing\nembeddings for each user to perform online calculations\nwould require lot of memory. For that reason we propose\nto train embeddings at a level of user type, where groups of\nusers with same type will have the same embedding. \u2022Rejections as Explicit Negatives - To reduce recommen-\ndations that result in rejections we encode host preference\nsignal in user and listing type embeddings by treating host\nrejections as explicit negatives during training.For short-term interest personalization we trained listing embed-\ndings using more than 800 million search clicks sessions, resulting\nin high quality listing representations. For long-term interest personalization we trained user type and\nlisting type embeddings using sequences of booked listings by 50\nmillion users. Both user and listing type embeddings were learned\nin the same vector space, such that we can calculate similarities\nbetween user type and listing types of listings that need to be\nranked. low-dimensional repre-\nsentations of words, through the use of neural networks [ 25,27]. The networks are trained by directly taking into account the word\norder and their co-occurrence, based on the assumption that words\nfrequently appearing together in the sentences also share more\nstatistical dependence. With the development of highly scalable\ncontinuous bag-of-words (CBOW) and skip-gram (SG) language\nmodels for word representation learning [ 17], the embedding mod-\nels have been shown to obtain state-of-the-art performance on\nmany traditional language tasks after training on large text data. Alternative approach, specifically useful\nfor cold-start recommendations, is to still to use text embeddings\n(e.g. title and description)\nto compute their embeddings [ 5,14,19,28]. Embedding approaches have had a major impact in both academia\nand industry circles. 3 METHODOLOGY\nIn the following we introduce the proposed methodology for the\ntask of listing recommendations and listing ranking in search at\nAirbnb. A new session is started whenever there is a time\ngap of more than 30 minutes between two consecutive user clicks. Given this data set, the aim is to learn a d-dimensional real-valued\nrepresentation vli\u2208Rdof each unique listing li, such that similar\nlistings lie nearby in the embedding space. From (1)and(2)we see that the proposed\napproach models temporal context of listing click sequences, where\nlistings with similar contexts (i.e., with similar neighboring listings\nin search sessions) will have similar representations. Time required to compute gradient \u2207Lof the objective function\nin(1)is proportional to the vocabulary size |V|, which for large\nvocabularies, e.g. As\nan alternative we used negative sampling approach proposed in [ 17],\nwhich significantly reduces computational complexity. Negative\nsampling can be formulated as follows. The\noptimization is done via stochastic gradient ascent. click sessions that end\nwith user booking a listing to stay at, and 2) exploratory sessions,\ni.e. click sessions that do not end with booking, i.e. Both are useful from the standpoint of capturing\ncontextual similarity, however booked sessions can be used to adapt\nthe optimization such that at each step we predict not only the\nneighboring clicked listings but the eventually booked listing as\nwell. This adaptation can be achieved by adding booked listing as\nglobal context, such that it will always be predicted no matter if\nit is within the context window or not. Consequently, for booked\nsessions the embedding update rule becomes\nargmax\n\u03b8\u00d5\n(l,c)\u2208Dplog1\n1+e\u2212v\u2032cvl+\u00d5\n(l,c)\u2208Dnlog1\n1+ev\u2032cvl+log1\n1+e\u2212v\u2032\nlbvl,\n(4)\nwhere vlbis the embedding of the booked listing lb. At each step the embedding of the central listing vlis being\nupdated such that it predicts the embeddings of the context listings\nvcfromDpand the booked listing vlb. As a consequence, there is a high\nprobability thatDpcontains listings from the same market. On\nthe other hand, due to random sampling of negatives, it is very\nlikely thatDncontains mostly listings that are not from the same\nmarkets as listings in Dp. To address this\nissue we propose to add a set of random negatives Dmn, sampled\nfrom the market of the central listing l,\nargmax\n\u03b8\u00d5\n(l,c)\u2208Dplog1\n1+e\u2212v\u2032cvl+\u00d5\n(l,c)\u2208Dnlog1\n1+ev\u2032cvl\n+log1\n1+e\u2212v\u2032\nlbvl+\u00d5\n(l,mn)\u2208Dmnlog1\n1+ev\u2032mnvl. (5)\nwhere parameters \u03b8to be learned are vlandvc,l,c\u2208V. Every day new listings are\ncreated by hosts and made available on Airbnb. At that point these\nlistings do not have an embedding because they were not present\nin the click sessions Straining data. To create embeddings for new\nlistings we propose to utilize existing embeddings of other listings. Upon listing creation the host is required to provide information\nabout the listing, such as location, price, listing type, etc. Next, we calculate the mean vector using 3embeddings\nof identified listings to form the new listing embedding. To\nevaluate if these characteristics are captured by embeddings we can\nexamine k-nearest neighbors of unique architecture listings in the\nlisting embedding space. As such, they are suitable for short-term, in-\nsession, personalization where the aim is to show to the user listings\nthat are similar to the ones they clicked during the immanent search\nsession. However, in addition to in-session personalization, based on\nsignals that just happened within the same session, it would be\nuseful to personalize search based on signals from user\u2019s longer-\nterm history. While some cross-market similarities are captured in listing\nembeddings trained using clicks, a more principal way of learn-\ning such cross-market similarities would be to learn from ses-\nsions constructed of listings that a particular user booked over\ntime. Specifically, let us assume we are given a set Sbof book-\ning sessions obtained from Nusers, where each booking session\nsb=(lb1, . Attempting to learn embeddings vlidfor\neach listin\u0434_idusing this type of data would be challenging in\nmany ways:\n\u2022First, booking sessions data Sbis much smaller than click\nsessions dataSbecause bookings are less frequent events. \u2022Third, to learn a meaningful embedding for any entity from\ncontextual information at least 5\u221210occurrences of that\nentity are needed in the data, and there are many listin\u0434_ids\non the platform that were booked less than 5\u221210times. To address these very common marketplace problems in practice,\nwe propose to learn embeddings at a level of listin\u0434_typeinstead of\nlistin\u0434_id. Given meta-data available for a certain listin\u0434_idsuch as\nlocation, price, listing type, capacity, number of beds, etc., we use aTable 3: Mappings of listing meta data to listing type buckets\nBuckets 1 2 3 4 5 6 7 8\nCountry US CA GB FR MX AU ES ... Buck-\nets are determined in a data-driven manner to maximize for cover-\nage in each listin\u0434_typebucket. The mapping from listin\u0434_idto a\nlistin\u0434_typeis a many-to-one mapping, meaning that many listings\nwill map into the same listin\u0434_type. To account for user ever-changing preferences over time we\npropose to learn user_typeembeddings in the same vector space\naslistin\u0434_typeembeddings. The user_typeis determined using a\nsimilar procedure we applied to listings, i.e. by leveraging meta-\ndata about user and their previous bookings, defined in Table 4. When gen-\nerating booking sessions for training embeddings we calculate the\nuser_typeup to the latest booking. For users who made their first\nbooking user_typeis calculated based on the first 5rows from Table\n4 because at the time of booking we had no prior information about\npast bookings. This is convenient, because learned embeddings for\nuser_types which are based on first 5rows can be used for cold-\nstart personalization for logged-out users and new users with no\npast bookings. Training Procedure. To learn user_typeandlistin\u0434_typeem-\nbeddings in the same vector space we incorporate the user_type\nApplied Data Science Track Paper\nKDD 2018, August 19-23, 2018, London, United Kingdom\n315Figure 5: Listing Type and User Type Skip-gram model\ninto the booking sessions. Specifically, we form a set Sbconsist-\ning of Nbbooking sessions from Nusers, where each session\nsb=(utype1ltype1, . Similarly, if the central item is a listin\u0434_type(lt) we optimize the\nfollowing objective\nargmax\n\u03b8\u00d5\n(lt,c)\u2208Dbooklog1\n1+e\u2212v\u2032cvlt+\u00d5\n(lt,c)\u2208Dne\u0434log1\n1+ev\u2032cvlt. Explicit Negatives for Rejections. Unlike clicks that only re-\nflect guest-side preferences, bookings reflect host-side preferences\nas well, as there exists an explicit feedback from the host, in form\nof accepting guest\u2019s request to book or rejecting guest\u2019s request\nto book. Host rejections can be utilized during training to encode the host\npreference signal in the embedding space in addition to the guest\npreference signal. We formulate the use of the rejections as explicit negatives in\nthe following manner. The\nnew optimization objective can then be formulated as\nargmax\n\u03b8\u00d5\n(ut,c)\u2208Dbooklog1\n1+exp\u2212v\u2032cvut+\u00d5\n(ut,c)\u2208Dne\u0434log1\n1+expv\u2032cvut\n+\u00d5\n(ut,lt)\u2208Drejectlog1\n1+expv\u2032\nltvut. (9)\nin case of updating the central item which is a listin\u0434_type(lt). Given learned embeddings for all user_types andlistin\u0434_types,\nwe can recommend to the user the most relevant listings based\non the cosine similarities between user\u2019s current user_typeembed-\nding and listin\u0434_typeembeddings of candidate listings. This\nwas followed by splitting one large ordered list of listing ids into\nmultiple ones based on 30minute inactivity rule. Next, we removed\naccidental and short clicks, i.e. clicks for which user stayed on the\nlisting page for less than 30 seconds, and kept only sessions con-\nsisting of 2or more clicks. Setting up Daily Training . We learn listing embeddings for\n4.5million Airbnb listings and our training data practicalities and\nparameters were tuned using offline evaluation techniques pre-\nsented below. Our training data is updated daily in a sliding window\nmanner over multiple months, by processing the latest day search\nsessions and adding them to the dataset and discarding the old-\nest day search sessions from the dataset. We train embeddings for\neach listin\u0434_id, where we initialize vectors randomly before train-\ning (same random seed is used every time). The day-to-day vector differences do not cause\ndiscrepancies in our models because in our applications we use the\ncosine similarity as the primary signal and not the actual vectors\nthemselves. Dimensionality of listing embeddings was set to d=32, as\nwe found that to be a good trade-off between offline performance\nand memory needed to store vectors in RAM memory of search\nmachines for purposes of real-time similarity calculations. Context\nwindow size was set to m=5, and we performed 10iterations over\nthe training data. To implement the congregated search change to\nthe algorithm we modified the original word2vec c code1. Training\nused MapReduce, where 300mappers read data and a single reducer\ntrains the model in a multi-threaded manner. End-to-end daily data\ngeneration and training pipeline is implemented using Airflow2,\nwhich is Airbnb\u2019s open-sourced scheduling platform. 4.2 Offline Evaluation of Listing Embeddings\nTo be able to make quick decisions regarding different ideas on\noptimization function, training data construction, hyperparameters,\netc, we needed a way to quickly compare different embeddings. One way to evaluate trained embeddings is to test how good\nthey are in recommending listings that user would book, based on\nthe most recent user click. More specifically, let us assume we are\ngiven the most recently clicked listing and listing candidates that\nneed to be ranked, which contain the listing that user eventually\nbooked. By calculating cosine similarities between embeddings of\nclicked listing and candidate listings we can rank the candidates\nand observe the rank position of the booked listing. Rankings of booked listing are averaged\nfor each click leading to the booking, going as far back as 17 clicks\nbefore the booking to the Last click before the booking. Lower val-\nues mean higher ranking. It can also be observed\nthat re-ranking listings based on embedding similarity would be\nuseful, especially in early stages of the search funnel. The same type of graphs were used to make\ndecisions regarding hyperparameters, data construction, etc. At the time of our test, the existing algorithm\nforSimilar Listings carousel was calling the main Search Ranking\nmodel for the same location as the given listing followed by filtering\non availability, price range and listing type of the given listing. The K listings with the highest similarity\nwere retrieved as similar listings. The calculations were performed\nonline and happen in parallel using our sharded architecture, where\nparts of embeddings are stored on each of the search machines. In light of these results we deployed the\nembedding-based Similar Listings to production. After\nthat 1 week wait the set Dsis also shortened to keep only search\nresults up to the last result user clicked on Kc\u2264K. Every time we train a new ranking model we use\nthe most recent 30 days of data. Feature vector xifor the i-th listing result consists of listing fea-\ntures ,user features ,query features andcross-features . Next, we formulate the problem as pairwise regression with\nsearch labels as utilities and use data Dto train a Gradient Boosting\nDecision Trees (GBDT) model, using package4that was modified\nto support Lambda Rank. When evaluating different models offline,\nwe use NDCG, a standard ranking metric, on hold-out set of search\nsessions, i.e. 80%ofDfor training and 20%for testing. The signals needed to calculate feature vectors xi\nfor each listing returned by search query qperformed by user uare\nall calculated in an online manner and scoring happens in parallel\nusing our sharded architecture. Given all the scores, the listings are\nshown to the user in a descending order of predicted utility. The first step in adding embed-\nding features to our Search Ranking Model was to load the 4.5\nmillion embeddings into our search backend such that they can be\naccessed in real-time for feature calculation and model scoring. Next, we introduced several user short-term history sets, that\nhold user actions from last 2 weeks, which are updated in real-time\nas new user actions happen. The logic was implemented using\nusing Kafka5. Specifically, for each user_id we collect and maintain\n(regularly update) the following sets of listing ids:\n(1)Hc: clicked listing_ids - listings that user clicked on in last\n2 weeks. We further split each of the short-term history sets H\u2217into\nsubsets that contain listings from the same market. In the following we describe how EmbClickSim feature is com-\nputed using Hc. To compute EmbClickSim for candidate listing liwe need to\ncompute cosine similarity between its listing embedding vliand\nembeddings of listings in Hc. We do so by first computing Hc\nmarket-level centroid embeddings. To illustrate, let us assume Hc\ncontains 5 listings from NY and 3 listings from LA. This would entail\ncomputing two market-level centroid embeddings, one for NY and\none for LA, by averaging embeddings of listing ids from each of\nthe markets. In addition to similarity to all user clicks, we added a feature that\nmeasures similarity to the latest long click, EmbLastLongClickSim. For a candidate listing liit is calculated by finding the cosine simi-\nlarity between its embedding vliand the embedding of the latest\nlong clicked listing llast from Hlc,\nEmbLastLon \u0434ClickSim(li,Hlc)=cos(vli,vllast). We follow\nsimilar procedure to introduce features based on user type and\nlisting type embeddings. We trained embeddings for 500Kuser\ntypes and 500Klisting types using 50 million user booking sessions. Embeddings were d=32dimensional and were trained using a\nsliding window of m=5over booking sessions. The user type and\nlisting type embeddings were loaded to search machines memory,\nsuch that we can compute the type similarities online. To compute the UserTypeListingTypeSim feature for candidate\nlisting liwe simply look-up its current listing type ltas well as\ncurrent user type utof the user who is conducting the search and\ncalculate cosine similarity between their embeddings,\nUserTypeListin\u0434TypeSim(ut,lt)=cos(vut,vlt). Top\nranking features are similarity to listings user clicked on (EmbClick-\nSim: ranked 5th overall) and similarity to listings user skipped\n(EmbSkipSim: ranked 8th overall). Five embedding features ranked\namong the top 20 features. To evaluate if the model learned to use the features as we in-\ntended, we plot the partial dependency plots for 3 embedding fea-\ntures: EmbClickSim, EmbSkipSim andUserTypeListTypeSim. These\nplots show what would happen to listing\u2019s ranking score if we fix\nvalues of all but a single feature (the one we are examining). On\nthe left subgraph it can be seen that large values of EmbClickSim,\nwhich convey that listing is similar to the listings user recently click\non, lead to a higher model score. The middle subgraph shows that\nlarge values of EmbSkipSim, which indicate that listing is similar to\nthe listings user skipped, lead to a lower model score. First, we compared two\nsearch ranking models trained on the same data with and without\nembedding features. The method learns low-dimensional representa-\ntions of home listings and users based on contextual co-occurrence\nin user click and booking sessions. To better leverage available\nsearch contexts, we incorporate concepts such as global context\nand explicit negative signals into the training procedure. We eval-\nuated the proposed method in Similar Listing Recommendations\nand Search Ranking. ACKNOWLEDGEMENTS\nWe would like to thank the entire Airbnb Search Ranking Team for\ntheir contributions to the project, especially Qing Zhang and Lynn\nYang. REFERENCES\n[1] Kamelia Aryafar, Devin Guillory, and Liangjie Hong. An Ensemble-based\nApproach to Click-Through Rate Prediction for Promoted Listings at Etsy. [2]Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al .1999. [3]Oren Barkan and Noam Koenigstein. [4]Christopher J Burges, Robert Ragno, and Quoc V Le. Learning to rank with\nnonsmooth cost functions. In Advances in NIPS 2007. Joint Text Em-\nbedding for Personalized Content-based Recommendation. In arXiv preprint\narXiv:1706.01084. Hidden conditional random fields with distributed user embeddings\nfor ad targeting. [8]Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, Ri-\ncardo Baeza-Yates, Andrew Feng, Erik Ordentlich, Lee Yang, and Gavin Owens. Scalable semantic matching of queries to ads in sponsored search advertis-\ning. ACM, 375\u2013384. [9]Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, and\nNarayan Bhamidipati. ACM, 383\u2013392. [10] Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati,\nand Ananth Nagarajan. Gender and interest targeting for sponsored post\nadvertising at tumblr. [11] Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati,\nJaikit Savla, Varun Bhagwan, and Doug Sharp. [12] Aditya Grover and Jure Leskovec. ACM, 855\u2013864. [13] Krishnaram Kenthapadi, Benjamin Le, and Ganesh Venkataraman. ACM, 346\u2013347. Metadata embeddings for user and item cold-start recommen-\ndations. InSlideshare: https://www.slideshare.net/SessionsEvents/dr-steve-liu-chief-scientist-\ntinder-at-mlconf-sf-2017. [17] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. [18] Thomas Nedelec, Elena Smirnova, and Flavian Vasile. arXiv preprint\narXiv:1706.07625 (2017). [21] Vladan Radosavljevic, Mihajlo Grbovic, Nemanja Djuric, Narayan Bhamidipati,\nDaneo Zhang, Jack Wang, Jiankai Dang, Haiying Huang, Ananth Nagarajan,\nand Peiji Chen. Smartphone app categorization for interest targeting\nin advertising marketplace. [23] Thomas Schmitt, Fran\u00e7ois Gonard, Philippe Caillou, and Mich\u00e8le Sebag. [24] Yukihiro Tagami, Hayato Kobayashi, Shingo Ono, and Akira Tajima. Mod-\neling User Activities on the Web using Paragraph Vector. [25] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Nonlinear latent factorization\nby embedding multiple user interests. ACM, 65\u201368. [28] Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, and Jason\nWeston. Ranking relevance in yahoo search.",
        "Results and Findings": "The embedding models were\nspecifically tailored for Airbnb marketplace, and are able to cap-\nture guest\u2019s short-term and long-term interests, delivering effective\nhome listing recommendations. 2018. ACM ISBN 978-1-4503-5552-0/18/08. . . The large amounts of collected data\nopen up possibilities for using Machine Learning to personalize\nsearch results for a particular user based on previous searches and\nrecommend similar content to recently consumed one. The objective of any search algorithm can vary depending on the\nplatform at hand. purchases\nof goods or services that are being searched over), and in the case\nof two sided marketplaces we often need to optimize the search\nresults for both sides of the marketplace, i.e. In the case of Airbnb, there is a clear need to optimize search\nresults for both hosts and guests, meaning that given an input query\nwith location and trip dates we need to rank high listings whose\nlocation, price, style, reviews, etc. Since guests typically conduct multiple searches before booking,\ni.e. We used extensive offline\nand online evaluation on real search traffic which showed that\nadding embedding features to the ranking model resulted in sig-\nnificant booking gain. In addition to the search ranking algorithm,\nlisting embeddings were successfully tested and launched for simi-\nlar listing recommendations where they outperformed the existing\nalgorithm click-through rate (CTR) by 20%. Researchers from the Web Search, E-commerce and Mar-\nketplace domains have quickly realized that just like one can train\nword embeddings by treating a sequence of words in a sentence\nas context, same can be done for training embeddings of user ac-\ntions, e.g. Recent industry conference publications and\nApplied Data Science Track Paper\nKDD 2018, August 19-23, 2018, London, United Kingdom\n312talks show that they have been successfully deployed in various per-\nsonalization, recommendation and ranking engines of major Web\ncompanies, such as Yahoo [ 8,11,29], Etsy [ 1], Criteo [ 18], Linkedin\n[15, 23], Tinder [16], Tumblr [10], Instacart [22], Facebook [28]. We describe two distinct approaches, i.e. . . We generate a set Dpof\npositive pairs(l,c)of clicked listings land their contexts c(i.e.,\nclicks on other listings by the same user that happened before and\nafter click on listing lwithin a window of length m), and a setDn\nofnegative pairs(l,c)of clicked listings and nrandomly sampled\nlistings from the entire vocabulary V. The optimization objective\nthen becomes\nargmax\n\u03b8\u00d5\n(l,c)\u2208D plog1\n1+e\u2212v\u2032cvl+\u00d5\n(l,c)\u2208D nlog1\n1+ev\u2032cvl,(3)\nFigure 1: Skip-gram model for Listing Embeddings\nwhere parameters \u03b8to be learned are vlandvc,l,c\u2208V . As the window slides some\nlistings fall in and out of the context set, while the booked listing\nalways remains within it as global context (dotted line). At each step, for a given central listing l,\nthe positive context mostly consist of listings from the same market\nasl, while the negative context mostly consists of listings that are\nnot from the same market as l. We found that this imbalance leads\nto learning sub-optimal within-market similarities. We use\nthe provided meta-data about the listing to find 3geographically\nclosest listings (within a 10miles radius) that have embeddings,\nare of same listing type as the new listing (e.g. $20\u2212$25\nper night). To evaluate what character-\nistics of listings were captured by embeddings we examine the\nd=32dimensional embeddings trained using (5)on800million\nclick sessions. First, by performing k-means clustering on learned\nembeddings we evaluate if geographical similarity is encoded. Fig-\nure 2, which shows resulting 100clusters in California, confirms\nthat listings from similar locations are clustered together. From those tables\nit can observed that cosine similarities between listings of same\ntype and price ranges are much higher compared to similarities\nbetween listings of different types and price ranges. Applied Data Science Track Paper\nKDD 2018, August 19-23, 2018, London, United Kingdom\n314Table 1: Cosine similarities between different Listing Types\nRoom Type Entire Home Private Room Shared Room\nEntire Home 0.895 0.875 0.848\nPrivate Room 0.901 0.865\nShared Room 0.896\nTable 2: Cosine similarities between different Price Ranges\nPrice Range <$30 $30-$60 $60-$90 $90-$120 $120+\n<$30 0.916 0.887 0.882 0.871 0.854\n$30-$60 0.906 0.889 0.876 0.865\n$60-$90 0.902 0.883 0.880\n$90-$120 0.898 0.890\n$120+ 0.909\n3.2 User-type & Listing-type Embeddings\nListing embeddings described in Section 3.1. that were trained using\nclick sessions are very good at finding similarities between listings\nof the same market. . . \u2022Second, many users booked only a single listing in the past\nand we cannot learn from a session of length 1. Listing Type Ent Priv Share\n$ per Night <40 40-55 56-69 70-83 84-100 101-129 130-189 190+\n$ per Guest <21 21-27 28-34 35-42 43-52 53-75 76+\nNum Reviews 0 1 2-5 6-10 11-35 35+\nListing 5 Star % 0-40 41-60 61-90 90+\nCapacity 1 2 3 4 5 6+\nNum Beds 1 2 3 4+\nNum Bedrooms 0 1 2 3 4+\nNum Bathroom 0 1 2 3+\nNew Guest Acc % <60 61-90 >91\nTable 4: Mappings of user meta data to user type buckets\nBuckets 1 2 3 4 5 6 7 8\nMarket SF NYC LA HK PHL AUS LV ... Language en es fr jp ru ko de ...\nDevice Type Mac Msft Andr Ipad Tablet Iphone ...\nFull Profile Yes No\nProfile Photo Yes No\nNum Bookings 0 1 2-7 8+\n$ per Night <40 40-55 56-69 70-83 84-100 101-129 130-189 190+\n$ per Guest <21 21-27 28-34 35-42 43-52 53-75 76+\nCapacity <2 2-2.6 2.7-3 3.1-4 4.1-6 6.1+\nNum Reviews <1 1-3.5 3.6-10 > 10\nListing 5 Star % 0-40 41-60 61-90 90+\nGuest 5 Star % 0-40 41-60 61-90 90+\nrule-based mapping defined in Table 3 to determine its listin\u0434_type. For example, an Entire Home listing from U Sthat has a 2 person\ncapacity, 1 bed, 1 bedroom & 1 bathroom, with Average Price Per\nNight of $60.8, Average Price Per Night Per Guest of $29.3, 5re-\nviews, all 5stars, and 100% New Guest Accept Rate would map into\nlistin\u0434_type=U S_lt1_pn3_p\u04343_r3_5s4_c2_b1_bd2_bt2_nu3. For\nexample, for a user from San Francisco with MacBook laptop, Eng-\nlish language settings, full profile with user photo, 83 .4%average\nGuest 5 star rating from hosts, who has made 3bookings in the past,\nwhere the average statistics of booked listings were $52.52 Price\nPer Night, $31.85 Price Per Night Per Guest, 2.33Capacity, 8.24\nReviews and 76 .1%Listing 5 star rating, the resulting user_typeis\nSF_l\u04341_dt1_f p1_pp1_nb1_ppn 2_pp\u04343_c2_nr3_l5s3_\u04345s3. . . As depicted in Figure 5b (on\nthe right), we specifically focus on the cases when host rejections\n(labeled with a minus sign) were followed by a successful booking\n(labeled with a plus sign) of another listing by the same user. It can be observed that\nlisting types that best match these user preferences, i.e. entire home,\nlots of good reviews, large and above average price, have high co-\nsine similarity, while the ones that do not match user preferences,\ni.e. We then show Online Ex-\nperiment Results of using Listing Embeddings for Similar Listing\nRecommendations on the Listing Page. In light of offline evaluation results\nwe oversampled booked sessions by 5x in the training data, which\nresulted in the best performing listing embeddings. We found that we get\nbetter offline performance if we re-train listing embeddings from\nscratch every day, instead of incrementally continuing training on\nexisting vectors. 1https://code.google.com/p/word2vec\n2http://airbnb.io/projects/airflow\nFigure 6: Offline evaluation of Listing Embeddings\nFor purposes of evaluation we use a large number of such search,\nclick and booking events, where rankings were already assigned\nby our Search Ranking model. In Figure 6 we show results of offline\nevaluation in which we compared several versions of d=32em-\nbeddings with regards to how they rank the booked listing based\non clicks that precede it. Embedding versions that we compared\nwere 1) d32: trained using (3), 2)d32 book : trained with bookings\nas global context (4)and 3) d32 book + neg : trained with bookings\nas global context and explicit negatives from same market (5). We conducted an A/B test where we compared the existing sim-\nilar listings algorithm to an embedding-based solution, in which\nsimilar listings were produced by finding the k-nearest neighbors in\nlisting embedding space. Given learned listing embeddings, similar\nlistings for a given listing lwere found by calculating cosine similar-\nity between its vector vland vectors vjof all listings from the same\nmarket that are available for the same set of dates (if check-in and\ncheck-out dates are set). 3https://www.airbnb.com/rooms/433392\nApplied Data Science Track Paper\nKDD 2018, August 19-23, 2018, London, United Kingdom\n317The A/B test showed that embedding-based solution lead to a\n21%increase in Similar Listing carousel CTR (23% in cases when\nlisting page had entered dates and 20%in cases of dateless pages)\nand4.9%increase in guests who find the listing they end up booking\nin Similar Listing carousel. To assign the label to a particular listing from the search\nresult we wait for 1 week after search happened to observe the final\noutcome, which can be yi=1if listing was booked, yi=0.25if\nlisting host was contacted by the guest but booking did not happen,\ny=\u22120.4if listing host rejected the guest, yi=0.01is listing was\nclicked and yi=0if listing was just viewed but not clicked. As expected, long-term feature UserType-\nListingTypeSim which used all past user bookings ranked better\nthan short-term feature EmbBookSim which takes into account only\nbookings from last 2 weeks. Online Experiment Results Summary . We conducted both\noffline and online experiments (A/B test). In Table 8 we summarize the results in terms\nof DCU (Discounted Cumulative Utility) per each utility (impres-\nsion, click, rejection and booking) and overall NDCU (Normalized\nDiscounted Cumulative Utility). It can be observed that adding em-\nbedding features resulted in 2.27%lift in NDCU, where booking\nDCU increased by 2.58%, meaning that booked listings were ranked\nhigher in the hold-out set, without any hit on rejections (DCU -0.4\nwas flat), meaning that rejected listings did not rank any higher\nthan in the model without embedding features. Applied Data Science Track Paper\nKDD 2018, August 19-23, 2018, London, United Kingdom\n319Table 8: Offline Experiment Results\nMetrics Percentage Lift\nDCU -0.4 (rejections) +0.31%\nDCU 0.01 (clicks) +1.48%\nDCU 0.25 (contacts) +1.95%\nDCU 1 (bookings) +2.58%\nNDCU +2.27%\nObservations from Table 8, plus the fact that embedding features\nranked high in GBDT feature importances (Table 7) and the find-\ning that features behavior matches what we intuitively expected\n(Figure 7) was enough to make a decision to proceed to an online\nexperiment. In the online experiment we saw a statistically sig-\nnificant booking gain and embedding features were launched to\nproduction. Several months later we conducted a back test in which\nwe attempted to remove the embedding features, and it resulted in\nnegative bookings, which was another indicator that the real-time\nembedding features are effective. After successful test on live search traffic both\nembedding applications were deployed to production. The summary of this\npaper was published in Airbnb\u2019s Medium Blog6. 2016. In\narXiv preprint arXiv:1711.01377. Modern information\nretrieval. Vol. ACM press New York. 2016. Item2vec: neural item embedding\nfor collaborative filtering. IEEE, 1\u20136. [5]Ting Chen, Liangjie Hong, Yue Shi, and Yizhou Sun. 2017. 2015. Hierarchical neural language models for joint representation\nof streaming documents and their content. 2016. In SIGIR 2016. 2015. In SIGIR 2015. 2015. 2015. 2017. [14] Maciej Kula. 2015. arXiv preprint arXiv:1507.08439 (2015). [15] Benjamin Le. 2017. [16] Steve Liu. 2017. 2017. [19] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. [20] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. ACM, 701\u2013710. 2016. [22] Sharath Rao. 2017. 2017. In IEEE International Conference on Tools with Artificial Intelligence. 2015. ACM, 125\u2013126. [26] Dongjing Wang, Shuiguang Deng, Xin Zhang, and Guandong Xu. 2016. [27] Jason Weston, Ron J Weiss, and Hector Yee. 2017. arXiv preprint arXiv:1709.03856 . [29] Dawei Yin, Yuening Hu, Jiliang Tang, Tim Daly, Mianwei Zhou, Hua Ouyang,\nJianhui Chen, Changsung Kang, Hongbo Deng, Chikashi Nobata, et al .2016. In Proceedings of the 22nd ACM SIGKDD.",
        "Conclusion": "CCS CONCEPTS\n\u2022Information systems \u2192Content ranking; Web log analysis;\nPersonalization; Query representation; Document representation;\nKEYWORDS\nSearch Ranking; User Modeling; Personalization\nACM Reference format:\nMihajlo Grbovic and Haibin Cheng. sellers and buyers. Furthermore, it has been\nshown that items which user interacted with can be leveraged to\ndirectly lean user embeddings in the same feature space as item\nembeddings, such that direct user-item recommendations can be\nmade [ 6,10,11,24,27]. Finally, similar ex-\ntensions of embedding approaches have been proposed for Social\nNetwork analysis, where random walks on graphs can be used to\nlearn embeddings of nodes in graph structure [12, 20]. Therefore, we\ncan conclude that those two listing characteristics are well encoded\nin the learned embeddings as well. \u2022Finally, long time intervals may pass between two consecu-\ntive bookings by the user, and in that time user preferences,\nsuch as price point, may change, e.g. Finally, we give background\non our Search Ranking Model and describe how Listing Embeddings\nand Listing Type & User Type Embeddings were used to implement\nfeatures for Real-time Personalization in Search. Finally, the sessions were anonymized\nby dropping the user id column. As mentioned before, click ses-\nsions consist of exploratory sessions &.booked sessions (sequence of\nclicks that end with booking). Finally, we\ncan conclude that d32 book + neg outperforms the other two\nembedding versions. Finally, to form\ndataD=\u00d0N\ns=1Dswe only keep Dssets which contain at least one\nbooking label. Finally, once the model is trained it is used for online scoring of\nlistings in search. Finally, we define the embedding features which utilize the de-\nfined sets and the listing embeddings to produce a score for each\ncandidate listing. Finally, EmbClickSim is calculated as maximum out of\n5https://kafka.apache.org\nApplied Data Science Track Paper\nKDD 2018, August 19-23, 2018, London, United Kingdom\n318Figure 7: Partial Dependency Plots for EmbClickSim, EmbSkipSim and UserTypeListTypeSim\ntwo similarities between listing embedding vliandHcmarket-level\ncentroid embeddings. As expected, it can be observed that\nfeatures based on user clicks and skips have the highest coverage. Finally, we trained a new GBDT Search Ranking model with\nembedding features added. Finally, the\nright subgraph shows that large values of UserTypeListingTypeSim,\nwhich indicate that user type is similar to listing type, lead to a\nhigher model score as expected. 5 CONCLUSION\nWe proposed a novel method for real-time personalization in Search\nRanking at Airbnb. 463. 2011. 2014. 2013. 3111\u20133119. 2014. 2010. 2013. StarSpace: Embed All The Things!",
        "label": 3
    },
    {
        "Abstract": "XiaoIce Band:\nA Melody and Arrangement Generation\nFramework for Pop Music\nHongyuan Zhu1,2\u2217, Qi Liu1\u2020, Nicholas Jing Yuan2\u2020, Chuan Qin1, Jiawei Li2,3\u2217,\nKun Zhang1, Guang Zhou2, Furu Wei2, Yuanchun Xu2, Enhong Chen1\n1University of Science and Technology of China ,2AI and Research Microsoft\n3Soochow University\nABSTRACT\nWiththedevelopmentofknowledgeofmusiccompositionandthe\nrecent increase in demand, an increasing number of companies\nand research institutes have begun to study the automatic gener-\nation of music. Copyrights for components of this work owned by others than\nACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orre-\npublish,topostonserversortoredistributetolists,requirespriorspecificpermission\nand/or a fee. Different from single track music, Chu et al. Althoughextensiveresearchhasbeencarriedoutonmusicgen-\neration, no single study exists considering the specificity of music. \u000fEmpty. [17] DanielJohnson.2015.Composingmusicwithrecurrentneuralnetworks.(2015). arXiv\npreprint arXiv:1705.08142 (2017).",
        "Methodology": "We pro-\npose an end-to-end melody and arrangement generation frame-\nwork, called XiaoIce Band, which generates a melody track with\nseveral accompany tracks played by several types of instruments. Specifically, we devise a Chord based Rhythm and Melody Cross-\nGeneration Model (CRMCG) to generate melody with chord pro-\ngressions. Then, we propose a Multi-Instrument Co-Arrangement\nModel (MICA) using multi-task learning for multi-track music ar-\nrangement. Request permissions from permissions@acm.org. How to generate mu-\nsic automatically has become a hot topic in recent years. Many\ncompanies and research institutes have done interesting works in\nthis area. [8] proposed a statistical model for\ntheproblemofmusicgeneration.Theyemployedasamplingmethod\nto generate music from extant music pieces. These\nmethods could generate novel music, but require manual inspec-\ntion of the features. Recently, Google Magenta1[3] created piano\nmusicwithDeepRecurrentNeuralNetwork[12](DRNN)bylearn-\ning MIDI (a digital score format) data. However, this method can\nonly deal with single track music. To be specific, we propose a Chord based Rhythm and\nMelody Cross-Generation Model (CRMCG) to generate melody con-\nditioned on the given chord progression for single track music. The contributions of this paper are summarized as follows. \u000fWe propose an end-to-end multi-track song generation sys-\ntem, including both the melody and arrangement. \u000fBased on the knowledge of music, we propose to utilize\nchord progression to guide melody procession and rhythm\npatterntolearnthestructureofasong.Then,weuserhythm\nand melody cross-generation method for song generation. Methods GMt M Cp Ar Sa\nMarkov\nmusic [31]p p\nMusic unit\nselection [2]p\nMagenta [\n3]p p\nSong fr\nom PI [6]p p p p\nDeepBach\n[13]p p p\nGANMidi [\n32]p p\nSampling music\nsequences [25]p p\nXiaoIce Band\n(this paper)p p p p p p\n2 RELA\nTED WORK\nThe related work can be grouped into two categories, i.e., music\ngeneration and multi-task learning. However, traditional methods require\nmassive manpower and domain knowledge. [17] combined one recurrent neural\nnetwork and one nonrecurrent neural network to represent the\npossibility of more than one note at the same time. Contrary to models based on RNNs, Sabathe et al. [24, 32] adopted GANs [11] to generate music,\nwhich treated random noises as inputs to generate melodies from\nscratch. [6] used hi-\nerarchical Recurrent Neural Network to generate both the melody\nas well as accompanying effects such as chords and drums. Specially, chord progres-\nsion usually guides the melody procession and the rhythm pattern\ndecides whether the song is suitable for singing. [34] proposed to improve generalization perfor-\nmance by leveraging the domain-specific information of the train-\ning data in related tasks. Chord pro-\ngressions are frequently used in songs and a song often sounds\nharmonious and melodic if it follows certain chords patterns. However,previousworksdidn\u2019tconsiderthestructureofthesong. We target at generating the suitable rhythm Ri=\nfri1;ri2; :::;rilrgand melody Mi=fmi1;mi2; :::;milmg. We will intro-\nduce the second and third part in detail. Notations Description\nM the melo\ndy sequence of pop music\nR the rhythm sequence of pop music\nC the chord progression of pop music\npi thei-th period of pop music\nmi j thej-th note in i-th period of pop music\nri j thej-th note duration in i-th period of pop music\nci thei-th chord of chord progression\nlm;lr;lc the length of melody/rhythm/chord progression sequence respectively\n\u0016hm\ni;j;\u0016hr\ni;j;\u0016hc\ni;jthej-th hidden state in i-th period of melody/rhythm/chord progression\nsequence respectively\nhi\nt;kthei-th task hidden state in period t at step k\n4.2 Chor\nd based Rhythm and Melody\nCross-Generation Model\nMelodyismadeupofaseriesofnotesandthecorrespondingdura-\ntion. Besides, note-level gener-\nation methods have more randomness on the pause, which causes\nthemusichardtosing.Thus,wepropose CRMCG tosolvetheprob-\nlem and generate a suitable rhythm for singing. Given a chord progression C=fc1;c2; :::;cNg, we aim at gen-\nerating the corresponding periods fp1;p2; :::;pNg. In order to better understand the chord progression and model\ntheinteractionandrelationofthesechords,weutilizeGatedRecur-\nrent Units (GRU) [4] to process the low-dimension representation\nof chords. They can be formulated as follows:\n\u0016C=EcC;Ec2RVc\u0003d;\n\u0016hc\ni;0=GRU( \u0016ci);i= 1 ;2; :::;lc;(1)\nhere, Ecis the embedding matrix for chord and hidden states \u0016ci\nencode each chord and sequence context around it. Then we can\nusethesehiddenstatestohelpgeneraterhythmandmelody.Tobe\nspecific, our generation processing can be divided into two parts:\nrhythm generation and melody generation. It is critical that the generated rhythm is in\nharmony with the existing part of music. Thus, in this part, we\n... \n... ... \n... ... ... ... \n... ... ... \n... \n... \n... ... ... ... ... ... ... ... ... ... ... ... To be specific,\nwe firstly multiply previous rhythm Rt\u00001and melody Mt\u00001with\nembedding matrix ErandEm. Af-\nter getting these representations, we utilize two different GRUs to\nencode these inputs:\n\u0016hm\nt\u00001 ;i=GRU( f\u0016mt\u00001 ;ig);i= 1 ;2; :::lm;\n\u0016hr\nt\u00001 ;i=GRU( f\u0016rt\u00001 ;ig);i= 1 ;2; :::;lr:(3)\nThenweseparatelyconcatenatethelasthiddenstatesofrhythm\nencoder and melody encoder, and make a linear transformation. Thus we get the\nrhythm for the t-th period and turn to generate the melody. Aftergeneratingthecurrentrhythm,wecan\nutilize this information to generate melody. Like rhythm genera-\ntion, we first concat previous melody Mt\u00001, currently generated\nrhythm Rtand corresponding chords ct. Second, we make a linear\ntransformation in the concatenation, which can be formulated as\nfollows:\nsm\n0=\u0434(W[\u0016hm\nt\u00001 ;lm;\u0016hr\nt;lr;\u0016hc\nt] +b);W2Rb\u0003b:(5)\nThen we get the initial hidden state of melody decoder. Based on the char-\nacteristic of the model, we can update the parameters alternately\nResearch Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2840 ...\n...\n...\n...... ...... Differently, we update all\nthe parameters by melody loss Lmin melody section. Differentfrom conventional mul-\ntiple sequences learning, the generated sequences in OSMGare\nclosely related. When generating one of the sequences, we should\ntake into account its harmony, rhythm matching, and instrument\ncharacteristic with other sequences. Encouraged by this evidence, we aim to model\nthe information flow between different tracks during music gen-\neration and propose the Multi-Instrument Co-Arrangement Model\n(MICA)based on CRMCG. Given a melody, we focus on generating more tracks to accom-\npany melody with different instruments. To this end, we designed two cooperate\ncells between the hidden layer of decoder to tackle this issue. The\ndetails of these two cells are in the following parts. Since our model shares\neachtrackinformationateachdecodingstep,itcanobtaintheover-\nall information about the music and generate music in harmony. Motivatedby[9],weoptimizethesumma-\ntion of several conditional probability terms conditioned on repre-\nsentation generated from the same encoder. Npis the size\nof parallel training corpus of p-th sequence pair. In generation part, we arrange for melody\ngeneratedby CRMCG.Wewilldiscussthispartindetails.Withthe\nhelpof CRMCG,wegetamelodysequence Mi={mi1,mi2,\u2026,milmg,\nand the next step is to generate other instrument tracks to accom-\npany it. Similarly, we utilize GRU to process the sequence and get\ntheinitialstate sm\n0ofmulti-sequencesdecoder.Theycanbeformu-\nlated as follows:\n\u0016M=EmM;Em2RVm\u0003d;\nsm\n0=GRU( \u0016mi;lm);(10)\nthe outputs of multi-sequences decoder correspond other instru-\nment tracks, considering both melody and other accompanying\ntracks. Specifically, each MIDI file contains var-\nious categories of audio tracks, such as melody, drum, bass and\nstring. Toguaranteethereliabilityoftheexperimentalresults,wemade\nsomepreprocessingonthedatasetasfollows.Firstly,weconverted\nall MIDI files to C major or A minor to keep all the music in the\nsametune.ThenwesettheBPM(BeatsPerMinute)to60forallthe\nmusic, which ensures that all notes correspond to an integer beat. In our model, the number of recurrent hidden units are set\nto 256 for each GRU layer in encoder and decoder. The model is updated with the Stochastic\nGradient Descent [ 1] algorithm where batch size set is 64, and the\nfinal model is selected according to the cross entropy loss on the\nvalidation set. That is, we only use the\nmelody track extracted from the original MIDI music to train the\nmodels and evaluate the aesthetic quality of the melody track gen-\neration result. 5.3.1 Baseline Methods. In addition to the proposed CRMCG model, we evaluate two\nvariants of the model to validate the importance of chord progres-\nsion and cross-training methods on melody generation:\n\u000fCRMCG (full). Proposed model, which generates melody\nand rhythm crosswise with chords information. \u000fCRMCG (w/o chord progression). BasedonCRMCG(full),\nthe chords information is removed. \u000fCRMCG (w/o cross-training). BasedonCRMCG(full),we\ntrainmelodyandrhythmpatternsrespectivelybasedon Lm\nandLrduring the training processing. Methods Rhythm MelodyIntegrity Singability Av\nerage\nMagenta (RNN)\n[3] 3.1875 2.8125 2.8000 2.6000 2.8500\nGANMidi (\nGAN) [11] 1.7125 1.7625 1.3500 1.4250 1.5625\nCRMCG (full) 3.7125 3.8125 3.7125 3.9000 3.7844\nCRMCG (\nw/o chord progression) 3.7000 3.5875 3.4375 3.8000 3.6312\nCRMCG (\nw/o cross-training) 3.6375 3.5500 3.3500 3.6250 3.5406\n\u000fRhythm. Does\nthemusicsoundsfluentandpausesuitably? Volunteers rated\nevery generated music with a score from 1 to 5 based on above\nevaluation metrics. 5.3.3 Chord Progression Analysis. We de-\nfine Chord Accuracy to evaluate whether chords of generated\nmelodies match the input chord sequence:\nChord Accurac y=P\u2211\ni=1e(yi;eyi)/P;\ne(yi;eyi)={1;i fyi=eyi\n0;i fyi,eyi;\nwhere Pis the number of the periods, yiis the i-th chord of gener-\natedmelodydetectedthrough[16],and eyiisthe i-thcorresponding\nchord in given chord progression. It is important to provide spaces to allow listeners\nto absorb each musical phrase before the next one starts. To createTable 5: Human evaluation of arrangement generation. Methods Ov\nerallDrum Bass String Guitar\nHRNN[6] 3.2500 2.9875 3.0875 2.8000 2.8625\nMICA (\nw/ att) 3.6625 3.0750 2.8000 3.2125 3.0000\nMICA (\nw/ mlp) 3.8125 3.1000 3.4625 3.3125 3.3500\nsatisfying music,\nit is necessary to keep a good dynamic balance\nbetween musical activity and rest. Therefore, we evaluate the per-\nformance of rests in our generated music by contrasting the differ-\nences between distributions of the length of the music segments\nin generated music and original ones. 5.4.1 Baseline Methods. Tovalidatetheperformanceofourtwo\nMICA models, a relevant model HRNN [6] is selected as baseline\nmethod. Specifically, we set the comparison methods as follows:\n\u000fHRNN. In particular, it uses\na low-level structure to generate melody and higher-level\nstructures to produce the tracks of different instruments. \u000fMICA w/ Attention Cell. Theproposedmodel,whichuses\nAttentionCelltoshareinformationbetweendifferenttracks. \u000fMICA w/ MLP Cell. Theproposedmodel,whichusesMLP\nCell to share information between different tracks. Furthermore, our MLP\nCell based MICA model performs better than Attention Cell based\nMICAmodel,anditseemsthatourMLPCellmechanismcanbetter\nutilize the information among the multiple tracks. Here we consider that two tracks\nare harmonious if they have similar chord progression [ 14]. we use chord similarity to represent harmony among multi-tracks. To observer how our model per-\nforms at multi-track music arrangement, we generate each track\nwhilefixingmelodytrackassourcemelodysequence.Hereweval-\nidate the performance based on four metrics as follows:\n\u000fNote accuracy. Note accuracy is the fraction of matched\ngenerated notes and source notes over the total amount of\nsource notes in a piece of music, that is\nNotes Accurac y=N\u2211\ni=1e(yi;eyi)/N;where yi,eyidenote the i-th source note and generated note,\nrespectively. Levenshteindistanceiscalculated\nby counting the minimum number of single-character edits\n(insertions, deletions or substitutions) required to change\none sequence into the other. Here we\ncalculatetheLevenshteinsimilaritybyLevenshteindistance,\nanditcanevaluatethesimilarityofgeneratedmusicalnotes\nsequences and original ones. \u000fNotes distribution MSE. NotesdistributionMSEisusedto\nanalyze the notes distribution between generated and orig-\ninal ones, which can be formulated by:\nNotes distribution MSE =\u2211M\ni=1\u2211N\nj=1(yi\nN\u0000eyi\nN)2\nMN;\nwher\neM,Ndenote the number of pieces of music and note\ncategories respectively. 5 tracks 4 (w/o G) 4 (w/o S) 4 (w/o B) 0.00.10.20.30.4Harmony mean score HRNN\nMICA w/ Attention Cell\nMICA w/ MLP Cell\nFigure\n10: The harmony analysis of arrangement (G: Guitar,\nS: String, B: Bass). For melody generation, we devised a Chord based\nRhythm and Melody Cross-Generation Model (CRMCG), which uti-\nlizeschordprogressiontoguidethemelodyprocession,andrhythm\npattern to learn the structure of song crosswise. For arrangement\ngeneration,motivatedbymulti-tasklearning,weproposeda Multi-\nInstrument Co-Arrangement Model (MICA) for multi-track music\narrangement, which used other task states at every step in the de-\ncoder layer to improve the whole generation performance and en-\nsure the harmony of multi-track music. 7 ACKNOWLEDGMENTS\nThis research was partially supported by grants from the National\nNaturalScienceFoundationofChina(No.s61672483and61727809). 2014299) and the MOE-\nMicrosoft Key Laboratory of USTC. Large-scale machine learning with stochastic gradient de-\nscent. [2] Mason Bretan, Gil Weinberg, and Larry Heck. A Unit Selection Method-\nology for Music Generation Using Deep Neural Networks. arXiv preprint\narXiv:1612.03789 (2016). [4] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Ben-\ngio. Predictive tabla mod-\nelling using variable-length markov and hidden markov models. [6] Hang Chu, Raquel Urtasun, and Sanja Fidler. arXiv preprint arXiv:1611.03477\n(2016). ACM, 160\u2013167. Multi-\nTask Learning for Multiple Language Translation.. [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. [12] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. [13] Ga\u00ebtan Hadjeres and Fran\u00e7ois Pachet. DeepBach: a Steerable Model for\nBach chorales generation. Detecting harmonic\nchange in musical audio. [15] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task model: Growing a neural network for multiple NLP\ntasks. arXiv preprint\narXiv:1705.07115 (2017). Recurrent neural network\nfor text classification with multi-task learning. Learning multiple tasks with deep\nrelationship networks. [23] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. C-RNN-GAN: Continuous recurrent neural networks with\nadversarial training. Sampling variations of sequences for structured music generation. arXiv preprint arXiv:1801.01957\n(2018). [31] Andries Van Der Merwe and Walter Schulze. arXiv preprint\narXiv:1707.08114 (2017).",
        "Results and Findings": "To this end,\nwe present a focused study on pop music generation, in which we\ntake both chord and rhythm influence of melody generation and\nthe harmony of music arrangement into consideration. For instance, Conklin et al. Besides, a\n\u2020Corresp\nonding authors\n\u2217This work was accomplished when the first and fifth authors working as interns in\nMicrosoft supervised by the third author\n1https://magenta.tensorflow.org/\nResearch Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2837 \npop\nsong has several fixed rhythm patterns, which make\nthesongmorestructuralandpausesuitably.However,exist-\ningstudies[17,19]usuallygeneratemusicnote-by-noteand\nwithoutconsideringtherhythmpattern.Ontheotherhand,\nthough several works [13, 25] utilize chord for music gener-\nation, they only use single chord as a feature of input and\nwithout considering the progression of chords when gener-\nating melody. \u000fDifferenttracksandinstrumentshavetheirowncharacteris-\ntics,whiletheyshouldbeinharmonywitheachother.Afew\nexisting works tackled the generation of multi-track music\n[6],butnoneofthemconsideredtheharmonybetweenmul-\ntiple tracks. Extensive experiments on real-world dataset\ndemonstrateourmodel\u2019ssuperiorityoverbaselinesonsingle-track\nand multi-track music generation. \u000fBy massive experiments provided, our system shows better\nperformancecomparedwithothermodelsaswellashuman\nevaluations. Among them, Johnson et al. 0 10 20 30 40 \nTrack numbers 0100020003000Number (a)\nTracks distribution\n0 10000 20000 30000\nNumbers Clarinet Voice FluteLeadBox Guitar Bass StringDrum Piano (b)\nTop 10 instruments\nFigure 3: Tracks and instruments analysis of pop song. We analyze the MIDI files, and the detailed statistics\nare shown in Figure 3(a), which indicates that the multi-track mu-\nsic widely exists in pop songs. It\u2019s a fundamental part of pop music. Then, we get the representations of\n\u0016Rt\u00001,\u0016Mt\u00001as follows:\n\u0016Rt\u00001=ErRt\u00001;Er2RVr\u0003d;\n\u0016Mt\u00001=EmMt\u00001;Em2RVm\u0003d;(2)\nwhere, VmandVrare the vocabulary size of notes and beats. The outputs of GRU are the probability\nofgeneratedrhythmofthecurrentperiod.Theycanbeformalized\nas follows:\nsr\n0=\u0434(W[\u0016hm\nt\u00001 ;lm;\u0016hr\nt\u00001 ;lr] +b);W2Rb\u0003b;\nsr\ni=GRU( yr\ni\u00001;sr\ni\u00001);i>0;\nyr\ni=sof tmax (sr\ni);(4)\nhere \u0434is the Relu activation function and sr\niis the hidden state of\nGRU for generating the i-th beat in t-th period. The\nmodifications are as follows:\nri\nt;k=\u03c3(Wi\nrxi\nt;k+Ui\nrhi\nt;k\u00001+Ai\nrai\nt;k+bi\nr);\nzi\nt;k=\u03c3(Wi\nzxi\nt;k+Ui\nzhi\nt;k\u00001+Ai\nzai\nt;k+bi\nz);\nghi\nt;k=\u03c3(Wixi\nt;k+Ui[\nri\nt;k\u0001hi\nt;k\u00001]\n+Aiai\nt;k+bi);\nhi\nt;k= (1 \u0000zi\nt;k)\u0001hi\nt;k\u00001+zi\nt;k\u0001ghi\nt;k;(8)\nby combining attention mechanism and GRU cell, our model can\ngenerate every track for one instrument with the consideration of\nother instruments. Research Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2841 L(\u03b8) =a\nr\u0434max\n\u03b8(\u2211\nTk(1\nNpNp\u2211\nilo\u0434p(YTk\nijXTk\ni;\u03b8)));\nwher\ne\u03b8={\n\u03b8src;\u03b8tr\u0434Tk;Tk= 1 ;2; :::;Tm}\n, and mis the number\nof tasks. Statistics Values\n# of\npopular songs 14,077\n# of\nall tracks 164,234\n# of\ndrum tracks 18,466\n# of\nbass tracks 16,316\n# of\nstring tracks 23,906\n# of\nguitar tracks 28,200\n# of\npiano tracks 18,172\n# of\nother instruments tracks 59,174\nTime of\nall tracks (hours) 10,128\n5.2 T\nraining Details\nWerandomlyselect9,855instancesfromthedatasetasthetraining\ndata, another 2,815 for tuning the parameters, and the last 1,407 as\ntest data to validate the performance as well as more generated\nmusic. \u000fGANMidi (GAN). 5.3.2 Overall Performance. We invited eight volunteers, who are experts in music appreci-\nation, to evaluate the results of various methods. Accord-\ning to the results, we realize that our CRMCG model outperforms\nallthebaselineswithasignificantmarginonallthemetrics,which\ndemonstrate the effectiveness of our CRMCG model on Melody\nGeneration.Especially,CRMCG(full)performsbetterthanCRMCG\n(w/ochord),whichverifiesthatthechordinformationcanenhance\nthe quality of melody. In addition, we also find that cross-training\ncanimprovethequalityof6.9%onaverage,whichproveseffective-\nness of our cross-training algorithm on melody generation. At the same time, we find that the RNN based baseline outper-\nforms the GAN based model which uses convolutional neural net-\nworks to generate melody. Here we further analyze the\nperformance of chord progression in our CRMCG model. Specially, the average\nChord Accuracy of our generated melody is 82.25%. From the result, we\nrealize that as the chord accuracy increases, the quality of melody\ngeneration improves significantly, which also confirms the impor-\ntance of using the chord information on Melody Generation. Figure 8 shows the distribu-\ntions of the minimum, maximum and average length of the mu-\nsic segments of the generated music and original ones. We realize\nourgeneratedmusic havesimilar distributionsonmusic segments\nlengths with original ones, which verifies that our CRMCG model\ncan generate the appropriate rests in pieces of music. 5.4.2 Overall Performance. Ac-\ncording to the results, we realize that our MICA model performs\nbetter than current method HRNN both on single-track and multi-\ntrack, which means MICA has significant improvement on Multi-\ntrack Music Generation task. Specially, we find that multi-track\nhashigherscorethansingletrackscore,whichindicatesthatmulti-\ntrack music sounds better than single-track music and confirms\nthe importance of the arrangement. Meanwhile, we observe that\nthe drum tracks has the worst performance compared to other\nsingle-track, which is because the drum track only plays an acces-\nsorial role in a piece of multi-track music. Thus,\nResearch Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2843 0.40.60.81.0\nChord accuracy 0.10.20.3Percentage (a)\n0.40.60.81.0\nChord accuracy 3.53.63.73.8Rhythm score (b)\n0.40.60.81.0\nChord accuracy 3.03.23.43.63.8Melody score (c)\n0.40.60.81.0\nChord accuracy 3.23.43.63.8Integrity score (d)\n0.40.60.81.0\nChord accuracy 3.23.43.63.8Singability score (e)\nFigur\ne 7: Chord progression analysis compared with human study. AsshowninFigure10,werealizethatourMLPCellbasedMICA\nmodel achieves the best performance, with an improvement by\nup to 24.4% compared to HRNN. Specially, we find that less tracks\nmusic harmony is higher than more tracks music. It\u2019s bad for generation results to be empty while a\nrealresulthasnotes.Weuseittoevaluategenerationresults\nand a lower score indicates better performance. According to the results,\ngenerally, our MLP Cell based MICA model achieves best perfor-\nmance across all metrics. And,\nasshowninFigure9(b),ourMLPCellbasedMICAmodelcouldim-\nprove the quality of 6.9% on average compared with HRNN. Mean-\nwhile, from Figure 9(c), we observe that our MLP Cell based MICA\nmodel has the most stable effect on Notes distribution MSE, which\nproves our model can do a better job in learning instrument char-\nacteristics. By massive experiments\nprovided, our system showed better performance compared with\nothermodelsinhumanevaluationandwehavecompletedtheTur-\ning test and achieved good results. In Proceedings of COMPSTAT\u20192010 . Springer, 177\u2013186. 2016. [3] PietroCasellaandAnaPaiva.2001. Springer, 224\u2013232. arXiv preprint arXiv:1409.1259 (2014). Journal of New\nMusic Research 40, 2 (2011), 105\u2013118. 2016. [7] Ronan Collobert and Jason Weston. [8] Darrell Conklin. [9] Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. 2015. 1723\u20131732. [10] RossGirshick.2015.Fastr-cnn.In Proceedings of the IEEE international conference\non computer vision. 2672\u20132680. IEEE, 6645\u20136649. 2016. arXiv preprint arXiv:1612.01010 (2016). [14] Christopher Harte, Mark Sandler, and Martin Gasser. 2006. ACM, 21\u201326. 2016. arXiv preprint arXiv:1611.01587 (2016). [16] Nanzhu Jiang, Peter Grosche, Verena Konz, and Meinard M\u00fcller. [18] Alex Kendall, Yarin Gal, and Roberto Cipolla. 2017. [19] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013). [20] Vladimir I Levenshtein. Binary codes capable of correcting deletions, in-\nsertions, and reversals. 707\u2013710. [21] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. arXiv preprint arXiv:1605.05101\n(2016). [22] Mingsheng Long and Jianmin Wang. 2015. arXiv preprint arXiv:1506.02117 (2015). 2016. [24] Olof Mogren. 2016. arXiv preprint arXiv:1611.09904 (2016). In Proceed-\nings of the 18th International Society for Music Information Retrieval Conference\n(ISMIR\u20192017), Suzhou, China. 167\u2013173. [26] Fran\u00e7ois Pachet and Pierre Roy. [27] SebastianRuder,JoachimBingel,IsabelleAugenstein,andAndersS\u00f8gaard.2017. 2017. IEEE, 3467\u20133474. [29] Paul Schmeling. Berklee Press. [30] Heung-Yeung Shum, Xiaodong He, and Di Li. 2018. IEEE MultiMedia 18, 3 (2011), 78\u201385. [32] Li-ChiaYang,Szu-YuChou,andYi-HsuanYang.2017. [33] Xiaofan Zhang, Feng Zhou, Yuanqing Lin, and Shaoting Zhang. 2016. Embed-\nding label structures for fine-grained feature representation. [34] YuZhangandQiangYang.2017. Research Track Paper\nKDD 2018, August 19\u201223, 2018, London, United Kingdom\n2846",
        "Conclusion": "Finally, we conduct extensive experiments on a real-\nworld dataset, where the results demonstrate the effectiveness of\nXiaoIce Band. \u000fAcompletesongtypicallyhasmulti-trackarrangement2con-\nsidering chord, beats and rhythm patterns, etc, with accom-\npanying background music played with other instruments,\nsuch as drum, bass, string and guitar. Tothisend,inthispaper,weproposetheXiaoIceBand3,anend-\nto-end melody and arrangement generation framework for song\ngeneration. Lastly,\nharmony plays a significant role in multi-track music, but it has\nnot been addressed very well in previous studies. To sum up, we compare XiaoIce Band with several related mod-\nels and show the results in Table 1. 3.3 Instrument Characteristic\nThelastcharacteristicofthesongisthearrangement,whichmeans\ncombingotherinstrumentswiththemelodyformakingthewhole\nmusic more contagious. To this\nend, we propose CRMCG for single track music, as well as MICA\nfor multi-track music to tackle this issue. Finally,\nweutilizeGRUtoprocesstheresultandgeneratethecurrentmelody\nfor the whole generation as follows:\nsm\ni=GRU( ym\ni\u00001;sm\ni\u00001);i>0;\nym\ni=sof tmax (sm\ni):(6)\nLoss Function. Since the generating process can be divided into\ntwoparts,wedesigntwolossfunctionsforeachpart.Thelossfunc-\ntions are both softmax cross-entropy functions. 4.3.3 MLP Cell. 4.3.4 Loss Function. Finally,wemergedevery2barsintoaperiod.Somebasicstatistics\nof the pruned dataset are summarized in Table 3.Table 3: Data Set Description. Thus, we validate the perfor-\nmance of models based on human study. \u000fIntegrity. For this result,\nwe think more tracks music have higher harmony requirements. \u000fLevenshtein similarity. Specially, from Figure 9(a), it can be con-\ncluded that the drum task has the greatest note accuracy, which\nconfirms that drum is easier to learn than other instruments. At last, the Figure 9(d) illustrates the robustness of our\nMLP Cell based MICA model, which can maintain a high level of\ngeneration result. 6 CONCLUSIONS\nIn this paper, we proposed a melody and arrangement generation\nframeworkbasedonmusicknowledge,calledXiaoIceBand,which\ngenerated a melody with several instruments accompanying si-\nmultaneously. 2010. 2014. 2011. 2008. 2003. 1440\u20131448. 2014. 2013. 2011. 2013. 1966. 10. 3994\u20134003. 2011. 2011. 2011. 1114\u20131123.",
        "label": 3
    },
    {
        "Abstract": "Not All Tokens Are What You Need for Pretraining\nZhenghao Lin\u22c6\u03c7\u03d5Zhibin Gou\u22c6\u03c0\u03d5Yeyun Gong\u22c4\u03d5Xiao Liu\u03d5Yelong Shen\u03d5\nRuochen Xu\u03d5Chen Lin\u22c4\u03c7\u03c1Yujiu Yang\u22c4\u03c0Jian Jiao\u03d5Nan Duan\u03d5Weizhu Chen\u03d5\n\u03c7Xiamen University\u03c0Tsinghua University\u03c1Shanghai AI Laboratory\n\u03d5Microsoft\nhttps://aka.ms/rho\nAbstract\nPrevious language model pre-training methods have uniformly applied a next-token\nprediction loss to all training tokens. 1 Introduction\nScaling up model parameters and dataset size has consistently elevated the next-token prediction\naccuracy in large language models, yielding significant advancements in artificial intelligence [Kaplan\net al., 2020, Brown et al., 2020, OpenAI, 2023, Team et al., 2023]. Removing such tokens\nmight alter the text\u2019s meaning, while overly strict filtering could exclude useful data [Welbl et al.,\n2021, Muennighoff et al., 2024] and lead to biases [Dodge et al., 2021, Longpre et al., 2023]. Figure 3(a) reveals a striking pattern: tokens\nfall into four categories based on their loss trajectory\u2014persistent high loss (H \u2192H), increasing\nloss (L\u2192H), decreasing loss (H \u2192L), and consistent low loss (L \u2192L). 2.2 Selective Language Modeling\nOverview Inspired by the practice of reference model in document-level filtering, we propose a\nsimple pipeline of token-level data selection, termed \u201cSelective Language Modeling (SLM)\u201d. Toks\u2217Train\nToksGSM8K MATH\u2020SV AMP ASDiv MA WPS TAB MQAMMLU\nSTEMSAT\u2021A VG\n1-2B Base Models\nTinyllama 1.1B - - - 2.9 3.2 11.0 18.1 20.4 12.5 14.6 16.1 21.9 13.4\nPhi-1.5 1.3B - - - 32.4 4.2 43.4 53.1 66.2 24.4 14.3 21.8 18.8 31.0\nQwen1.5 1.8B - - - 36.1 6.8 48.5 63.6 79.0 29.2 25.1 31.3 40.6 40.0\nGemma 2.0B - - - 18.8 11.4 38.0 56.6 72.5 36.9 26.8 34.4 50.0 38.4\nDeepSeekLLM 1.3B OWM 14B 150B 11.5 8.9 - - - - - 29.6 31.3 -\nDeepSeekMath 1.3B - 120B 150B 23.8 13.6 - - - - - 33.1 56.3 -\nContinual Pretraining on Tinyllama-1B\nTinyllama-CT 1.1B OWM 14B 15B 6.4 2.4 21.7 36.7 47.7 17.9 13.9 23.0 25.0 21.6\nRHO-1-Math 1.1B OWM 14B 9B 29.8 14.0 49.2 61.4 79.8 25.8 30.4 24.7 28.1 38.1\n\u2206 -40% +23.4 +11.6 +27.5 +24.7 +32.1 +7.9 +16.5 +1.7 +3.1 +16.5\nRHO-1-Math 1.1B OWM 14B 30B 36.2 15.6 52.1 67.0 83.9 29.0 32.5 23.3 28.1 40.9\n\u22657B Base Models\nLLaMA-2 7B - - 14.0 3.6 39.5 51.7 63.5 30.9 12.4 32.7 34.4 31.4\nMistral 7B - - 41.2 11.6 64.7 68.5 87.5 52.9 33.0 49.5 59.4 52.0\nMinerva 8B - 39B 164B 16.2 14.1 - - - - - 35.6 - -\nMinerva 62B - 39B 109B 52.4 27.6 - - - - - 53.9 - -\nMinerva 540B - 39B 26B 58.8 33.6 - - - - - 63.9 - -\nLLemma 7B PPile 55B 200B 38.8 17.2 56.1 69.1 82.4 48.7 41.0 45.4 59.4 50.9\nLLemma 34B PPile 55B 50B 54.2 23.0 67.9 75.7 90.1 57.0 49.8 54.7 68.8 60.1\nIntern-Math 7B - 31B 125B 41.8 14.4 61.6 66.8 83.7 50.0 57.3 24.8 37.5 48.7\nIntern-Math 20B - 31B 125B 65.4 30.0 75.7 79.3 94.0 50.9 38.5 53.1 71.9 62.1\nDeepSeekMath 7B - 120B 500B 64.1 34.2 74.0 83.9 92.4 63.4 62.4 56.4 84.4 68.4\nContinual Pretraining on Mistral-7B\nMistral-CT 7B OWM 14B 15B 42.9 22.2 68.6 71.0 86.1 45.1 47.7 52.6 65.6 55.8\nRHO-1-Math 7B OWM 14B 10.5B 66.9 31.0 77.8 79.0 93.9 49.9 58.7 54.6 84.4 66.2\n\u2206 -30% +24.0 +8.8 +9.2 +8.0 +7.8 +4.8 +11.0 +2.0 +18.8 +10.4\nBy default, we use L\u2206as the score function S. This ensures that the loss is applied only to the tokens\nthat are deemed most beneficial for the language model to learn from. In this scenario, we assume the absence of downstream task-related data, a common situation in\nreal-world applications. arXiv\npreprint arXiv:1911.00359 , 2019. Camels in a changing climate: Enhancing lm adaptation\nwith tulu 2. arXiv preprint arXiv:2311.10702 , 2023. arXiv\npreprint arXiv:2310.10631 , 2023. CoRR ,\nabs/1907.11692, 2019. arXiv\npreprint arXiv:2306.11644 , 2023. a\ncomprehensive study of automatic data selection in instruction tuning. arXiv\npreprint arXiv:1708.00489 , 2017. arXiv\npreprint arXiv:1811.00225 , 2018. URL https://arxiv.org/abs/2110.14168 . B.2 Data Selection\nData selection for fine-tuning has been extensively studied, focusing on improving quality [Li et al.,\n2023c], diversity [Liu et al., 2024], and distribution matching [Li et al., 2023d, Xia et al., 2024, Ni\net al., 2024, Kang et al., 2024]. Sample-level selection has been extensively studied in previous research [Sener and Savarese, 2017,\nKillamsetty et al., 2021], particularly through online batch selection [Loshchilov and Hutter, 2015,\nSchaul et al., 2015, Chang et al., 2017, Katharopoulos and Fleuret, 2018, Jiang et al., 2019]. We leave this for future works. 24NeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope? Answer: [Yes]\nJustification: In the abstract and \u00a71, we clearly demonstrate the contribution and scope of\nthis paper. Guidelines:\n\u2022The answer NA means that the abstract and introduction do not include the claims\nmade in the paper. \u2022The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. 3.Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof? Guidelines:\n\u2022 The answer NA means that the paper does not include theoretical results. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Guidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Answer: [NA]\nJustification: The purpose of this paper is to improve the training process of large language\nmodels, without any negative societal impacts. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster. Answer: [NA]\nJustification: The paper does not release new assets. Guidelines:\n\u2022 The answer NA means that the paper does not release new assets. Answer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects. 15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained? Answer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects. Pay later. The guy said yeah and I asked for a\nliter of it. It was $45 but a liter will last me a while and\nmaking it myself would\u2019ve been vastly more expensive I imagine. \\n \\n ### Student Probability/PDE Seminar: Large Deviation Principle\nfor random graphs II \\n \\n Seminar | December1 | 2:10-3:30 p.m. | 891Evans Hall \\n \\n Fraydoun Rezakhanlou, UC\nBerkeley \\n \\n Department of Mathematics \\n \\n ### BLC Fellows Forum \\n \\n Presentation | December1 | 3-5\np.m. | Dwinelle Hall, B-4 (Classroom side) \\n \\n FAll 2017 BLC Fellows, UC Berkeley \\n \\n Berkeley Language\nCenter \\n \\n Teaching French Listening Comprehension and Cultural Awareness through Regional Variation \\n\nElyse Ritchey, GSR, French \\n At the university level, French language instruction in the US traditionally includes\na course on phonetics and pronunciation. I have no plans to check that the form is fully condensed, e.g. \\n \\n $\\mathbb{Q} = \\frac{\\mathbb{Z} \\times \\mathbb{Z}\\backslash\\{0\\}}{\\sim}$ \\n\n\\n Let\u2019s now take a look at what members of $$\\mathbb{Q}$$ look like, say for the equation $$2x = 3$$. I think the length $$L$$ of the\nrod stays the same (I\u2019m here assuming that the rod is infinitely stiff), but that it would now \u201dappear shorter\u201d to the\ndistant observer - i.e. Division of Mechanical Sciences >\nMechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question\n#8de97 \\n \\n Dec 10, 2016 \\n \\n That is not an identity. \\n \\n This allows us to continue \\n \\n $= \\frac{{\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2\nx}{\\frac{1}{\\sin} \u02c6 2 x}$ \\n \\n $= {\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2 x \\cdot {\\sin}\u02c6{2} \\frac{x}{1}$ \\n \\n $= {\\cos}\u02c6{2}\nx$ \\n \\n Which is not identically $\\cos x$. \\n \\n (${\\cos}\u02c6{2} x = \\cos x$ only when $\\cos x = 1$ or $0$) \\n \\n Dec\n10, 2016 \\n \\n No. \\n \\n The same way, $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2$ can be written\nas $\\frac{1}{\\csc} \u02c6 2 x - \\frac{{\\csc}\u02c6{2} x}{{\\csc}\u02c6{2} x}$. Division of Mechanical Sciences >\nMechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question\n#8de97 \\n \\n Dec 10, 2016 \\n \\n That is not an identity. \\n \\n This allows us to continue \\n \\n $= \\frac{{\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2\nx}{\\frac{1}{\\sin} \u02c6 2 x}$ \\n \\n $= {\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2 x \\cdot {\\sin}\u02c6{2} \\frac{x}{1}$ \\n \\n $= {\\cos}\u02c6{2}\nx$ \\n \\n Which is not identically $\\cos x$. \\n \\n (${\\cos}\u02c6{2} x = \\cos x$ only when $\\cos x = 1$ or $0$) \\n \\n Dec\n10, 2016 \\n \\n No. \\n \\n The same way, $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2$ can be written\nas $\\frac{1}{\\csc} \u02c6 2 x - \\frac{{\\csc}\u02c6{2} x}{{\\csc}\u02c6{2} x}$. Division of Mechanical Sciences >\nMechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question\n#8de97 \\n \\n Dec 10, 2016 \\n \\n That is not an identity. \\n \\n This allows us to continue \\n \\n $= \\frac{{\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2\nx}{\\frac{1}{\\sin} \u02c6 2 x}$ \\n \\n $= {\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2 x \\cdot {\\sin}\u02c6{2} \\frac{x}{1}$ \\n \\n $= {\\cos}\u02c6{2}\nx$ \\n \\n Which is not identically $\\cos x$. \\n \\n (${\\cos}\u02c6{2} x = \\cos x$ only when $\\cos x = 1$ or $0$) \\n \\n Dec\n10, 2016 \\n \\n No. \\n \\n The same way, $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2$ can be written\nas $\\frac{1}{\\csc} \u02c6 2 x - \\frac{{\\csc}\u02c6{2} x}{{\\csc}\u02c6{2} x}$. Division of Mechanical Sciences >\nMechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question\n#8de97 \\n \\n Dec 10, 2016 \\n \\n That is not an identity. \\n \\n This allows us to continue \\n \\n $= \\frac{{\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2\nx}{\\frac{1}{\\sin} \u02c6 2 x}$ \\n \\n $= {\\cos}\u02c6{2} \\frac{x}{\\sin} \u02c6 2 x \\cdot {\\sin}\u02c6{2} \\frac{x}{1}$ \\n \\n $= {\\cos}\u02c6{2}\nx$ \\n \\n Which is not identically $\\cos x$. \\n \\n (${\\cos}\u02c6{2} x = \\cos x$ only when $\\cos x = 1$ or $0$) \\n \\n Dec\n10, 2016 \\n \\n No. \\n \\n The same way, $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2$ can be written\nas $\\frac{1}{\\csc} \u02c6 2 x - \\frac{{\\csc}\u02c6{2} x}{{\\csc}\u02c6{2} x}$.",
        "Methodology": "Challenging this norm, we posit that \u201cNot all\ntokens in a corpus are equally important for language model training\u201d . Unlike traditional LMs that learn to predict\nevery next token in a corpus, RHO-1employs Selective Language Modeling (SLM),\nwhich selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training\nthe language model with a focused loss on tokens with higher scores. When\ncontinual pretraining on 15B OpenWebMath corpus, RHO-1yields an absolute\nimprovement in few-shot accuracy of up to 30% in 9 math tasks. RHO-1is trained\nwith our proposed Selective Language Modeling (SLM), while baselines are trained using causal\nlanguage modeling. \u22c6Equal contribution. \ud835\udc65!\ud835\udc65\"\ud835\udc65#\ud835\udc65$\ud835\udc65%\ud835\udc65&\ud835\udc65'EOSUndesired TokensDesired Tokens\nCausal Language Modeling\u2718Remove loss \u2713Keep loss \ud835\udc65!\ud835\udc65\"\ud835\udc65$\ud835\udc65%\ud835\udc65#\ud835\udc65&\ud835\udc65'\ud835\udc65(Selective Language Modeling\ud835\udc65!\ud835\udc65\"\ud835\udc65$\ud835\udc65%\ud835\udc65#\ud835\udc65&\ud835\udc65'\ud835\udc65(\n\u2718\u2718\u2718\u2713\u2713\u2713\u2713\nNoisy Pretraining Corpus\n\ud835\udc65!\ud835\udc65\"\ud835\udc65#\ud835\udc65$\ud835\udc65%\ud835\udc65&\ud835\udc65'EOS\u2713Figure 2: Upper: Even an extensively filtered pretraining corpus contains token-level noise. Right: Our proposed Selective\nLanguage Modeling (SLM) selectively applies loss on those useful and clean tokens. As a result, the practice of data filtering has become crucial,\nusing various heuristics and classifiers [Brown et al., 2020, Wenzek et al., 2019] to select training\ndocuments. These techniques significantly improve data quality and boost model performance. Applying the same loss to all tokens can lead to inefficient\ncomputation on non-essential tokens, potentially restricting LLMs from achieving more advanced\nlevels of intelligence. To explore how language models learn at the token level, we initially examined training dynamics,\nparticularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the\nmodel\u2019s token perplexity at different checkpoints and categorized tokens into different types. These tokens can lead to numerous ineffective gradient updates. Based on these analyses, we introduce RHO-1models trained with a novel Selective Language\nModeling (SLM) objective3. As shown in Figure 2 (Right), this approach inputs the full sequence\ninto the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted\nin Figure 4: First, SLM trains a reference language model on high-quality corpora. This model\nestablishes utility metrics to score tokens according to the desired distribution, naturally filtering\nout unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a\ncorpus using its loss (\u00a72.2). Finally, we train a language model only on those tokens that exhibit a\nhigh excess loss between the reference and the training model, selectively learning the tokens that\nbest benefit downstream applications (\u00a72.2). 20 5 10 15\nTrained T okens(B)01234Loss(a) Loss for different token types\nHH (11%)\nLH (12%)\nHL (26%)\nLL (51%)\n0 5 10 15\nTrained T okens(B)0.00.10.20.3Loss(b) Example L L tokens\nLL T oken 1\nLL T oken 2\nLL T oken 3\n0 5 10 15\nTrained T okens(B)1.52.02.53.03.5Loss(c) Example H H tokens\nHH T oken 1\nHH T oken 2\nHH T oken 3\nFigure 3: The loss of four categories of tokens during pretraining. on benchmarks for models trained with the selected tokens. Remarkably, RHO-1-7B matches the state-of-the-art performance of DeepSeekMath-7B\nusing only 15B tokens, compared to the 500B tokens required by DeepSeekMath. Notably, RHO-1-1B is the\nfirst 1B LM to exceed 40% accuracy, nearing the early GPT-4\u2019s CoT performance of 42.5%. In \u00a73.4, we demonstrate that in settings without high-quality reference data, we can use SLM\nfor self-referencing, leading to an average improvement of up to 3.3% in downstream tasks. We continue pre-training Tinyllama-1B with 15B tokens from OpenWebMath, saving\ncheckpoints after every 1B tokens. We then evaluate token-level loss at these intervals using the\nvalidation set of approximately 320,000 tokens. Additionally, 12% of tokens experience an\nunexpected loss increase (L \u2192H) during training. Our second observation is that a significant number of token losses exhibit persistent fluctuations,\nand resist convergence. Consequently, we learn that the loss associated with each token during training does not decrease\nsmoothly like the overall loss; instead, there is a complex training dynamic among different tokens. If we can select the appropriate tokens for the model to focus on during training, we may be able to\nstabilize the trajectory of the model\u2019s training and enhance its data efficiency. Our\nmethod comprises three steps, as depicted in Figure 4. We begin by training a reference model on a\ncurated, high-quality dataset. This model then assesses the loss of each token within the pretraining\n3Step 1Train a reference model on high-quality text.Reference ModelHigh-quality CorpusPretraining CorpusStep 2Calculate each token\u2019s ppl in the pretraining corpus.Language Model\nStep 3Train an LLM with loss focused on high-score tokens.Figure 4: The pipeline of Selective Language Modeling (SLM). SLM optimizes language model\nperformance by concentrating on valuable, clean tokens during pre-training. It involves three steps:\n(Step 1) Initially, train a reference model on high-quality data. (Step 2) Then, score each token\u2019s\nloss in a corpus using the reference model. The intuition is that tokens with high excess\nloss are more learnable and better aligned with the desired distribution, naturally excluding tokens\nthat are either irrelevant or of low quality. Below, we provide a detailed description of each step. We train a reference model (RM) using standard cross-entropy loss on the curated data. The resulting RM is then used to assess the token loss within a larger pretraining corpus. We compute\nthe reference loss ( LRM) of a token xibased on the probability that the RM assigns to this token. The\ncalculation is formalized as follows:\nLRM(xi) =\u2212logP(xi|x<i) (1)\nBy evaluating LRMfor each token, we establish the reference loss for selective pretraining, allowing\nus to focus on the most influential tokens in language modeling. The excess loss ( L\u2206) for a token xiis\ndefined as the difference between the current training model loss ( L\u03b8) and the reference loss:\nL\u2206(xi) =L\u03b8(xi)\u2212 L RM(xi) (3)\nWe introduce a token selection ratio k%, which determines the proportion of tokens to be included\nbased on their excess loss. All models are tested with few-shot\nprompting. In practice, token selection can\nbe implemented by ranking the tokens in a batch according to their excess loss and using only the top\nk%of tokens for training. This process eliminates the loss for undesired tokens without incurring\nadditional costs during pretraining, making our approach both efficient and easily integrated. 3 Experiments\nWe continually pretrained models in both mathematical and general domain and designed ablation\nand analysis experiments to understand the effectiveness of SLM. This dataset is a blend of synthetic data from GPT [Yu et al.,\n2024, Huang et al., 2024] and manually curated data [Yue et al., 2024, Ni et al., 2024]. We trained the reference models for 3\nepochs. The maximum learning rate was set at 5e-5 for 1B models and 1e-5 for 7B models, applying\na cosine decay schedule. We set the maximum sequence lengths to 2048 for 1B models and 4096 for\n7B models, packing multiple samples into these lengths for model input. In all main experiments, we\ninitialized the continual pretraining model and the reference model with the same base model. \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717\nPrevious Models\nGPT4-0314 - \u2717 - 92.0 42.5 93.1 91.3 97.6 67.1 64.7 78.3\nGPT4-0314 (PAL) - \u2713 - 94.2 51.8 94.8 92.6 97.7 95.9 77.6 86.4\nMAmmoTH 70B \u2713 MI-260k 76.9 41.8 82.4 - - - - -\nToRA 7B \u2713 ToRA-69k 68.8 40.1 68.2 73.9 88.8 42.4 54.6 62.4\nToRA 70B \u2713 ToRA-69k 84.3 49.7 82.7 86.8 93.8 74.0 67.2 76.9\nDeepSeekMath 7B \u2713 ToRA-69k 79.8 52.0 80.1 87.1 93.8 85.8 63.1 77.4\nOur Pretrained Models\nTinyLlama-CT 1B \u2713 ToRA-69k 51.4 38.4 53.4 66.7 81.7 20.5 42.8 50.7\nRHO-1-Math 1B \u2713 ToRA-69k 59.4 40.6 60.7 74.2 88.6 26.7 48.1 56.9\n\u2206 +8.0 +2.2 +7.3 +7.5 +6.9 +6.2 +5.3 +6.2\nMistral-CT 7B \u2713 ToRA-69k 77.5 48.4 76.9 83.8 93.4 67.5 60.4 72.6\nRHO-1-Math 7B \u2713 ToRA-69k 81.3 51.8 80.8 85.5 94.5 70.1 63.1 75.3\n\u2206 +3.8 +3.4 +3.9 +1.7 +1.1 +2.6 +2.7 +2.7\nPretraining Corpus For mathematical reasoning, we utilize the OpenWebMath (OWM) dataset\n[Paster et al., 2023], which comprises approximately 14B tokens sourced from math-related web\npages in the Common Crawl. For the 1.1B model, we conducted our training on 32 \u00d7H100 80G GPUs. This\nconfiguration allowed us to train approximately 15 billion tokens in around 3.5 hours and 50 billion\ntokens in about 12 hours. In the case of the 7B model, training the same 15 billion tokens took\napproximately 18 hours under similar hardware conditions. The batch size is uniformly set to 1M tokens for both domains. Regarding the token selection ratio, we use 60% for the Tinyllama-1.1B model and 70% for the\nMistral-7B model. Baseline Setting We use models that have been continually pretrained (Tinyllama-CT and Mistral-\nCT) through regular causal language modeling as baselines. For fine-tuning results, we also compare with previous best models MAmmoTH[Yue\net al., 2024] and ToRA[Gou et al., 2024]. We adopt the lm-eval-harness4[Gao\net al., 2023] for general tasks, and develop math evaluation suite5for math tasks. We use vllm\n(v0.3.2) [Kwon et al., 2023] to speed up inference. We continual pretraining Tinyllama-1B on 80G general\ntokens. Tinyllama-CT is etrained with CLM, while R HO-1 is trained with our proposed SLM. Specifically, we initially\ntrained the reference model on the OpenWebMath (OWM) corpus, a subset of Proof-Pile-2 (PPile). We evaluated OWM and PPile using the trained reference model and selected tokens for training. We hypothesize that the key factor is not scoring the desired distribution\nbut filtering out noisy tokens. Therefore, we employed two different scoring functions based on the\nreference model loss, LRM, and the information entropy of the next token, HRM, which measures the\nuncertainty of the next token. We use OpenWebMath (OWM) to train the reference model. The experimental results, as shown in Table 3, indicate that using only the OWM-trained reference\nmodel can effectively guide the model in pre-training on the same corpus, improving average\ndownstream performance by +2.4%. Furthermore, training the SLM on the PPile, despite only using the\nOWM subset to train the reference model, still achieved a 1.8% improvement with 30% fewer tokens\nused. For more details, please refer to Appendix H.\n3.5 Ablation Study and Analysis\nSelected Token Loss Aligns Better with Downstream Performance We utilized the reference\nmodel to filter tokens and assess their impact on validation and downstream losses after training. Therefore, we expect that selecting tokens for pretraining is more efficient. Thus, reducing loss across all tokens is not imperative for improved model performance. Refer to Appendix F for further details. What Tokens are Selected with SLM? We aim to analyze the tokens selected by the SLM method\nin pretraining to further explore its working mechanism. To this end, we visualize the token selection\nprocess during the training of RHO-1using the OpenWebMath. In \u00a7G.1, we have highlighted in blue\nthe tokens that were retained during actual pretraining. The x-axis represents the average loss on selected tokens / unselected tokens at corresponding\ncheckpoint (2B, 5B, 8B, 11B, and 14B). 2 5 8 11 14\nT okens(B)2.352.452.552.65PPLPPL of T okens Selected by Different CKPT\nSelected T okens at 2B\nSelected T okens at 5B\nSelected T okens at 8B\nSelected T okens at 11B\nSelected T okens at 14B\nFigure 8: The PPL of tokens selected by differ-\nent checkpoint. We test the PPL of the tokens\nselected at 2B, 5B, 8B, 11B, and 14B. We train\n1B LM with SLM objective on 5B tokens. by the SLM method are closely related to mathematics, effectively training the model on the parts of\nthe original corpus that are pertinent to mathematical content. Furthermore, we investigated the differences in token filtering across various checkpoints during the\ntraining process and tested the perplexity of these tokens on different checkpoints. This may suggest that\nthe model first optimizes tokens with a larger learnable space, thereby increasing learning efficiency. Moreover, we noticed a sample-wise \u201cdouble descent\u201d [Nakkiran et al., 2021] on the loss of selected\ntokens, where the select token\u2019s perplexity initially increases before decreases. This might be an\neffect of selecting tokens based on excess loss, targeting those most in need at each checkpoint. Generally, the selecting ratio is defined by heuristic rules, similar to the approach previously employed\nin the training of Masked Language Models (MLMs) [Devlin et al., 2019, Liu et al., 2019]. Acknowledgments\nZhenghao Lin and Chen Lin were supported by National Key R&D Program of China (No. arXiv preprint\narXiv:2001.08361 , 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty\nAnderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo\nPyysalo, Thomas Wolf, and Colin A Raffel. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages\n1286\u20131305, 2021. A pretrainer\u2019s guide to training data: Measuring the effects of data\nage, domain coverage, quality, & toxicity. Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q Tran, Dani Yogatama, and Donald Metzler. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of\nthe European Chapter of the Association for Computational Linguistics , pages 2985\u20133000, Dubrovnik,\nCroatia, May 2023. Metamath: Bootstrap your own mathematical questions for large language models. Mammoth:\nBuilding math generalist models through hybrid instruction tuning. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang,\nDavid Wadden, Noah A Smith, Iz Beltagy, et al. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent\nSifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. URL https://github.com/deepseek-ai/DeepSeek-LLM . Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu\nLiu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. arXiv preprint\narXiv:2308.12950 , 2023. 11Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. Efficient memory management for large language model serving with\npagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles , 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. arXiv preprint\narXiv:2009.03393 , 2020. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,\nMojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and\nNicholas Carlini. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas\nMuennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto,\nand William Yang Wang. Doremi: Optimizing data mixtures speeds up language model pretraining. URL https://openreview.net/forum?id=KIPJKST4gw . Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and\nJing Xiao. One shot learning as instruction data prospector for large language models. Less: Selecting influential\ndata for targeted instruction tuning. URL https://openreview.net/\nforum?id=QmYNBVukex . URL https:\n//openreview.net/forum?id=C61sk5LsK6 . Active learning for convolutional neural networks: A core-set approach. Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection for efficient\nand robust semi-supervised learning. Online batch selection for faster training of neural networks. arXiv preprint\narXiv:1511.06343 , 2015. arXiv preprint\narXiv:1511.05952 , 2015. Accelerating deep learning by focusing\non the biggest losers. Carpe diem, seize the samples uncertain\" at\nthe moment\" for adaptive batch selection. Prioritized training on\npoints that are learnable, worth learning, and not yet learnt. URL https://proceedings.mlr.press/v162/mindermann22a.html . Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt Kusner. URL https://openreview.net/forum?id=thbXgJ8gNK . Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure\nLeskovec, and Matei Zaharia. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE , 86(11):2278\u20132324, 1998. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the\n2013 conference on empirical methods in natural language processing , pages 1631\u20131642, 2013. Train no evil: Selective masking\nfor task-guided pre-training. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6966\u20136974,\nOnline, November 2020. Using selective masking as a\nbridge between pre-training and fine-tuning. Revisiting token\ndropping strategy in efficient BERT pretraining. Token\ndropping for efficient BERT pretraining. URL https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/\nsolidgoldmagikarp-plus-prompt-generation . Fishing for magikarp: Automatically detecting under-trained tokens in large\nlanguage models. Probing across time: What\ndoes roberta know and when? Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke\nZettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large\nlanguage models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, et al. Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer,\nand Christopher Olah. Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage,\nZac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Charles AE Goodhart and CAE Goodhart. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. In Proceedings of the 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies , pages 1152\u20131157, San\nDiego, California, June 2016. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and\nAshwin Kalyan. net/forum?id=DHyHRBwJUTN . Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha\nChowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. arXiv preprint\narXiv:1803.05457 , 2018. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages\n7432\u20137439, 2020. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 16Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, et al. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer,\nand Mike Lewis. URL\nhttps://openreview.net/forum?id=jiDsk12qcz . Specializing smaller language models towards\nmulti-step reasoning. 20\nC Limitations and Future Work 20\nD Analysis and Visualization of Tokens in Pretraining 21\nD.1 More Details of Four Categories Tokens . Zhibin Gou presented a preliminary proposal, introduced the method of using excess loss for\nreweighting tokens, compiled high-quality corpora, trained reference models, set up the fine-tuning\nand evaluation pipelines, designed the experimental analysis, and significantly contributed to the\nwriting. Yeyun Gong proposed the initial project and co-led the project with Weizhu Chen, they\noffered extensive advice and guidance on experiments and writing, and oversaw team collaboration\nand resource management. Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao,\nand Nan Duan offered research mentorship, coordinated the project, and contributed to the writing. B Related Works\nB.1 Pretraining Data Optimization\nThe objective of optimizing pre-training corpora is to maximize the performance and efficiency\nof language model training by improving the quality and scale of the pretrain data mixture. This\nincludes data collecting through crawling [Raffel et al., 2020] or synthesis [Polu and Sutskever, 2020,\nGunasekar et al., 2023], de-duplication [Lee et al., 2021, Kandpal et al., 2022, Tirumala et al., 2023],\nfiltering and selection [Xie et al., 2024a, Albalak et al., 2024], as well as data composition [Xie et al.,\n2024b] and curriculum [Chen et al., 2024, Ma et al., 2024]. These\napproaches have been applied to various classification tasks [Song et al., 2020, Mindermann et al.,\n2022] and language modeling [Fan and Jaggi, 2023]. However, Kaddour et al. [2024] utilize n-gram\nmodels or datamodels with a target dataset to estimate importance weights. [2024b] optimize the worst-case excess loss [Oren et al., 2019] relative to a reference model to\ndetermine domain weights. Although excess loss is mathematically identical to RHO-LOSS, SLM differs in three important ways:\n1) The focus is distinct. Motivated by the training dynamics of token loss, the core idea of SLM is to\nselect useful tokens for pre-training. Its score functions are highly flexible and not limited to excess\nloss (see Appendix H for other functions). In contrast, RHO-LOSS aims to mathematically derive a\nreducible holdout loss to minimize generalization loss. SLM trains a reference model on high-quality data to reflect the desired\ndata distribution, whereas RHO-LOSS trains a small model on a random holdout set. 3) The selection\nscale and granularity vary. RHO-LOSS selects sample-level data on a small scale (typically 1K\u20131M\nsamples) for task-specific fine-tuning tasks such as MNIST [LeCun et al., 1998] and SST-2 [Socher\n19et al., 2013]. In contrast, SLM conducts fine-grained token-level selection on large-scale language\nmodel pre-training, involving up to 80B tokens. Token-level training strategies have also been explored, especially for the pre-training of BERT-like\nmodels using Masked Language Modeling (MLM) [Devlin et al., 2018]. Specifically, \u201cselective\nmasking\u201d involves masking important tokens in the input to focus on learning tokens that are more\nrelevant to downstream tasks [Gu et al., 2020, Lad et al., 2022], whereas \u201ctoken dropping\u201d aims to\nreduce training costs by omitting less important tokens [Zhong et al., 2023a, Hou et al., 2022]. [Li\net al., 2023e] assesses the quality of each token based on the skewness of its predicted distribution and\ntruncates the noisy tokens during training. Additionally, some research has approached the analysis\nand detection of under-trained tokens from a tokenization perspective [Rumbelow and Watkins, 2023,\nLand and Bartolo, 2024]. To our knowledge, we are the first to explore token-level data selection for\nlarge language model training, aimed at enhancing data quality and information density at the most\nfundamental granularity. This research includes studying internal representations [Saphra\nand Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021],\nand the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022], who posit that tokens with little change in\nperplexity are \u201calready learned\u201d. Recognizing this, we propose a method of selective\nlanguage modeling that targets the influential tokens, optimizing the learning process. B.4 Scaling Laws\nScaling laws guide us in discovering the impact of factors such as parameter count, data size, and\ncompute on language model performance and behavior. Most scaling laws on model performance study cross-entory loss on all\ntraining tokens, while we focus on the tokens loss of desired distributions. Although no adverse effects, like biases, have\nbeen observed from the increased loss yet, a general pretraining loss on text and code may prevent\noverfitting [Goodhart and Goodhart, 1984], as suggested by Ouyang et al. Scalability Due to budget constraints, we have only verified the effectiveness of our method on\nsmaller models (<=7B parameters) and smaller datasets (<100B tokens). Smaller models benefit\nsignificantly from removing the loss of irrelevant tokens and focusing on important ones. However,\nit\u2019s possible that very large models trained on extensive corpora may naturally develop this inductive\nbias to compress useful data ( i.e.,compressing everything), although it may sounds inefficient for\nnow. Therefore, future works should study whether this selective language modeling technique can\nscale to very large models and data [Kaplan et al., 2020]. This could be a base model trained with a small amount of high-quality data, or a performant\nopen-source model. In fact, since we only need input logprobs or perplexity from reference model,\n20/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni0000000b/uni00000025/uni0000000c/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni0000002f/uni00000052/uni00000056/uni00000056/uni0000000b/uni00000044/uni0000000c/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000049/uni00000052/uni00000055/uni00000003/uni00000047/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000057/uni0000005c/uni00000053/uni00000048/uni00000056\n/uni0000002b/uni0000002f/uni00000003/uni0000000b/uni00000015/uni0000001b/uni00000008/uni0000000c\n/uni0000002b/uni0000002b/uni00000003/uni0000000b/uni0000001b/uni00000008/uni0000000c\n/uni0000002f/uni0000002f/uni00000003/uni0000000b/uni00000018/uni00000014/uni00000008/uni0000000c\n/uni0000002f/uni0000002b/uni00000003/uni0000000b/uni00000014/uni00000016/uni00000008/uni0000000c\n/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni0000000b/uni00000025/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni0000002f/uni00000052/uni00000056/uni00000056/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000028/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni0000002f /uni0000002f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056\n/uni0000002f/uni0000002f/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000014\n/uni0000002f/uni0000002f/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000015\n/uni0000002f/uni0000002f/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000016\n/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni0000000b/uni00000025/uni0000000c/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000017/uni00000011/uni00000018/uni0000002f/uni00000052/uni00000056/uni00000056/uni0000000b/uni00000046/uni0000000c/uni00000003/uni00000028/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni0000002b /uni0000002b/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056\n/uni0000002b/uni0000002b/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000014\n/uni0000002b/uni0000002b/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000015\n/uni0000002b/uni0000002b/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000016\nFigure 10: The loss of four categories of tokens during Mistral-7B pretraining on OpenWebMath. we could even utilize more powerful proprietary model APIs. We can input tokens and use the log\nprobabilities of the input returned by the API as reference scores. How to improve upon SLM? Expanding the use of SLM SLM may be extended to supervised fine-tuning to address the noise\nand distribution mismatches in many SFT datasets. D Analysis and Visualization of Tokens in Pretraining\nD.1 More Details of Four Categories Tokens\nWe categorize tokens into four categories: H \u2192H, L\u2192H, H\u2192L, L\u2192L. We then used\nlinear fitting and took the difference in loss between the first and last points as evidence of whether\nthe loss decreased during the training process. Specifically, suppose we have a sequence of token\u2019s loss (l0, l1, ..., l n). Our goal is to minimize the\nsum of the squares of the differences between each data point and its linear predictive value:\nf(a, b) =minimizenX\ni=0(li\u2212(axi+b))2, (6)\nwhere x0= 0is the initial checkpoint and xn=nis the final checkpoint. Substituting these into\nthe fitted equation, we can obtain the Loss values at the start and end after fitting: Lstart=band\nLend=an+b. The change in loss can then be expressed as: \u2206L=Lend\u2212 L start. Meanwhile, we\nrepresent the average Loss of the last checkpoint as Lmean. Next, we can classify the tokens based on \u2206Land the Lmean. We categorize tokens with \u2206L<\u22120.2\nas H\u2192L (loss decreases from high to low) category tokens, and tokens with \u2206L>0.2as L\u2192H (loss\nincreases from low to high) category tokens. If \u22120.2\u2264\u2206L \u22640.2andln\u2264 L mean, then tokens are\nclassified as L \u2192L (loss remains low); if ln>Lmean, they are classified as H \u2192H (loss remains high). 21D.2 Non-Converging Tokens in Pretrainig\nIn \u00a72.1, we mentioned that during the training process, only a minority of tokens belong to the H \u2192L\ncategory. Among the remaining categories of H \u2192H and L\u2192L tokens, there are tokens that exhibit\nsignificant fluctuations during training. Furthermore, there are instances where H \u2192L tokens are\nnot effectively learned. Therefore, in our analysis, we specifically select those tokens from these\ncategories that demonstrate considerable variability and distinct loss. We visualize these tokens\nthat exhibit abnormal behavior during the training process. The latter may not necessarily be\ndisastrous for training; in fact, it could represent a normal occurrence. However, if we can effectively\nmitigate the losses caused by the former, it might lead to more stable and efficient model training. Our\nbenchmarks include GSM8k [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], GSM-Hard [Gao\net al., 2022], SV AMP [Patel et al., 2021], ASDIV [Miao et al., 2020], MAWPS [Koncel-Kedziorski\net al., 2016], TabMWP (TAB) [Lu et al., 2023], MathQA (MQA) [Amini et al., 2019], MMLU-STEM\n[Hendrycks et al., 2020], and SAT [Azerbayev et al., 2023]. F Relate the Selected Tokens\u2019 Loss to Downstream Task Performance\nIn this section, we declare the details about correlating the loss of selected tokens with the performance\nof downstream tasks. Concurrent study has explored similar methods to study the impact of scaling\nlaws with the performance of models in downstream tasks [Gadre et al., 2024]. Our analysis\nhere differs in that it aims to elucidate the relationship between the decrease/increase in loss for\nselected/unselected tokens and the model\u2019s performance on downstream tasks. We use the average accuracy of MATH and GSM8K as the standard for measuring downstream tasks\nperformance of model. If the loss of selected tokens Lsis used for fitting,\nthena >0. Conversely, if the loss of unselected tokens Lusis used for fitting, then a <0. Therefore,\nwe believe that training the model on selected tokens can effectively improve its performance on\ndownstream tasks, while unselected tokens may have a detrimental effect on the model\u2019s performance\nin downstream tasks. Score\nFunctionSelect\nRatioGSM8K MATH SV AMP ASDiv MA WPS MQA A VG\n- 100% 6.3 2.6 21.7 36.7 47.7 13.9 21.5\nLRM(xi)90% 7.4 4.4 23.4 38.7 51.9 14.4 23.4\n80% 6.4 4.6 23.1 39.7 52.0 14.3 23.4\n70% 6.7 4.6 23.3 40.0 54.5 14.3 23.9\n60% 7.0 4.6 22.2 38.5 52.2 13.7 23.0\n50% 5.7 4.2 20.7 36.7 46.7 10.3 20.7\nHRM(xi)90% 6.7 3.0 23.7 40.3 52.3 13.1 23.2\n80% 6.8 3.6 22.5 40.6 52.9 13.6 23.3\n70% 7.0 4.8 23.0 39.3 50.5 13.5 23.0\n60% 6.5 4.8 26.5 37.3 49.7 15.6 23.4\n50% 4.7 5.8 20.9 33.8 42.5 11.1 19.8\nHRM(xi)\u222a L RM(xi)50%\u222a70%(80%) 6.4 3.6 22.7 38.4 52.6 15.3 23.2\n70%\u222a60%(77%) 6.3 4.6 24.4 39.6 51.4 16.3 23.8\n70%\u222a50%(75%) 6.9 5.6 23.2 39.9 52.9 12.6 23.5\n60%\u222a60%(70%) 6.7 5.2 24.7 39.2 50.6 14.6 23.5\n60%\u222a50%(68%) 7.1 5.8 21.7 37.3 49.6 15.3 22.8\n60%\u222a40%(65%) 7.3 6.0 23.6 36.9 48.6 13.1 22.6\nHRM(xi)\u2229 L RM(xi)80%\u222990%(76%) 6.0 4.4 23.7 38.5 51.2 13.3 22.8\n75%\u222975%(72%) 7.8 5.2 24.2 39.4 54.9 14.7 24.4\n70%\u222990%(68%) 6.8 4.6 22.2 40.3 53.0 14.8 23.6\n80%\u222980%(67%) 8.2 6.4 21.2 39.1 53.4 15.0 23.9\n70%\u222970%(60%) 7.1 5.0 23.5 41.2 53.8 18.0 24.8\nG Examples of Tokens Selected by SLM\nG.1 Token Selected Examples\nIn Figure 13, we present several examples of tokens selected by the SLM method, with content\nmarked in blue indicating the tokens actually chosen during the pretraining process. G.2 Dynamic Token Selected\nIn Figure 14, we display the dynamic changes in token selection tendencies throughout the SLM\ntraining process. We chose four checkpoints during the training process (0%, 33%, 66%, and 100%)\nto analyze the current tendencies in token selection. Reference loss score function is to directly use the loss\nof the reference model as the basis for selecting tokens. The higher the token\u2019s loss of the reference\nmodel, the lower the expectation that the token will be selected. The score LRM(xi)can be directly\nobtained by referring to Equation 1. Information entropy score function is to select the corresponding\ntoken based on the information entropy of the reference model in each token. Model Train Toks GSM8K MATH SV AMP ASDiv MA WPS TAB MQAMMLU\nSTEMSAT A VG\nLlama-2-7B-CT 15B 28.4 13.6 50.3 62.8 79.5 37.6 34.1 41.6 43.5 43.5\nLlama-2-7B-CT w/ 1B RM 10.5B 29.8 16.0 55.5 63.7 80.4 37.9 34.3 38.2 43.8 44.4\nwhere tkrepresents the i-th token in the vocabulary, and Vrepresents the size of the vocabulary. The\nintuition of this strategy is that the higher the information entropy, the higher the uncertainty of the\ntoken in the context. Therefore, we consider that if the language model is still uncertain for certain\ntokens after pretraining, we do not expect that the language model will learn it during pretraining. A No or\nNA answer to this question will not be perceived well by the reviewers. \u2022It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper. 2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]\nJustification: In Appendix C, we have thoroughly discussed the limitations of our article,\nhoping to guide more future work. Guidelines:\n\u2022The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be. \u2022The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. \u2022The authors should reflect on the factors that influence the performance of the approach. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon. \u2022The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size. \u2022If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness. \u2022While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations. 25Answer: [Yes]\nJustification: In \u00a72.1 and \u00a72.2, we elaborated on the motivation and theoretical derivation of\nour method, with a complete proof process in place. \u2022All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced. \u2022All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition. \u2022Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material. Answer: [Yes]\nJustification: We have provided detailed descriptions of the experimental setup in \u00a73.1 and\nmethods in \u00a72.2 to ensure that our experiment can be reproduced. \u2022If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not. \u2022Depending on the contribution, reproducibility can be accomplished in various ways. \u2022While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. (c)If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset). In the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results. 5.Open access to data and code\n26Question: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial? Answer: [No]\nJustification: This may be temporary, and we are working hard to promote the process of\nopen source. \u2022Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy ) for more details. \u2022While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark). See the NeurIPS code and data submission guidelines ( https:\n//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. \u2022The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why. \u2022At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable). \u2022The full details can be provided either with the code, in appendix, or as supplemental\nmaterial. Answer: [No]\nJustification: Due to the high cost of pre-training and the significant results obtained across\nvarious settings, we do not repeat the same experiments. \u2022The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.) \u2022 The assumptions made should be given (e.g., Normally distributed errors). \u2022It should be clear whether the error bar is the standard deviation or the standard error\nof the mean. \u2022It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified. \u2022For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. Answer: [Yes]\nJustification: In \u00a73.1, we have provided sufficient information on the computer resources\nneeded to reproduce the experiments. \u2022The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage. \u2022The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute. \u2022The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper). Answer: [Yes]\nJustification: We guarantee that the research conducted in the paper complies with NeurIPS\nCode of Ethics in all aspects. \u2022The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction). 10.Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed? Guidelines:\n28\u2022 The answer NA means that there is no societal impact of the work performed. \u2022If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact. \u2022The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. \u2022The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology. 11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)? Answer: [NA]\nJustification: The paper poses no such risks. Guidelines:\n\u2022 The answer NA means that the paper poses no such risks. \u2022Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images. 12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected? Answer: [Yes]\nJustification: The creators or original owners of the assets used in the paper, such as code,\ndata, and models, have been appropriately recognized, and the licenses and terms of use\nhave been clearly mentioned and properly respected. Guidelines:\n\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. 29\u2022For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided. \u2022If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. \u2022For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided. \u2022If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators. 13.New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets? This includes details about training, license,\nlimitations, etc. \u2022The paper should discuss whether and how consent was obtained from people whose\nasset is used. \u2022At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file. 14.Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)? \u2022According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector. 30\u2022Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. \u2022We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution. \u2022For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review. If they have a child 2 years from now, how many children will they\nhave in total? \\n Veritas Prep GMAT Instructor \\n Joined: 16 Oct 2010 \\n Posts: 4566 \\n Location: Pune, India \\n\nFollowers: 1029 \\n \\n Kudos [? If they have a child 2 years from\nnow, how many children will they have in total? \\n \\n 1st child, 2nd child, 3rd child, ....... nth child (to be born after 2 yrs) \\n \\n\nWhat is the difference between first and last terms (children)? J yrs \\n \\n What is the number of terms (children)? (T + 2)/J + 1\n\\n (Number of terms of an AP is n = (Last term - First term)/Common Difference + 1. ) \\n _________________\n\\n \\n Karishma \\n Veritas Prep | GMAT Instructor \\n My Blog \\n \\n Save $100 on Veritas Prep GMAT Courses\nAnd Admissions Consulting Enroll now. No problems whatsoever, he just said \u201dbe careful with it\u201d. Among them, blue represents tokens\nof categorie H \u2192L, green indicates tokens of categorie L \u2192L, yellow signifies tokens of categorie\nH\u2192H, and red denotes tokens of categorie L \u2192H. functions,\nfunctorial, etc.). Otherwise the search is exact. The and-operator & is default and can be\nomitted. Cheb yshev | Tschebyscheff The or-operator | allows to search for Cheb yshev or Tschebyscheff. \u201dPartial diff* eq*\u201d ! eliminates all results containing the word elliptic. dt: b & au: Hilbert The document\ntype is set to books; alternatively: j for journal articles, a for book articles. py: 2000-2015 cc: (94A | 11T) Number\nranges are accepted. la: chinese Find documents in a given language. It does not reduce any further\n17-bitters, though. The idea is to create an array of the student\u2019s primitive factors, so say 3(x+1)(x+2)\u02c62 gives (3,x+1,x+2). \u2022Because\nwe may want factoring over Z, checking the gcd of coefficients within each factor. \u2022Pass each of these things to\nSAGE and ask if the nonconstant factors are reducible over Z or Q. \\n \\n Suppose, the body reaches\nthe point P after time ( t ) . \\n \\n Horizontal motion has no acceleration. The trajectory of the projectile is a parabola. \\n \\n ### Time of Maximum height \\n \\n As the body is projected it\ngoes up. Vertical component of velocity ( u \\sin \\theta ) gradually diminishes and becomes zero at the maximum\nheight of flight. \\n \\n Let, ( t_m ) is the time to reach at maximum\nheight ( h_m ) of flight. \\n \\n Let, the body reaches at point B on ground after time ( T_f ) of projection. Multiplying these equations\ntogether, we get $$adcf = bcde$$. We can cancel $$d$$ and $$c$$ from both sides to get $$af = be$$. \\n \\n Notice that the length is independent of the signature of the metric, so whether you work with\nthe (-+++) or (+\u2014) metric is purely conventional and will not change the physics. it would no longer occupy the entire space between $$r_1$$ and $$r_2$$. It is equal to ${\\sin}\u02c6{2} x - 1$. \\n \\n #### Explanation: \\n \\n If we have $\\frac{1 - x}{x}$, we\ncan write it as $\\frac{1}{x} - \\frac{x}{x}$. \\n \\n This is equal to ${\\sin}\u02c6{2} x - 1$. It is equal to ${\\sin}\u02c6{2} x - 1$. \\n \\n #### Explanation: \\n \\n If we have $\\frac{1 - x}{x}$, we\ncan write it as $\\frac{1}{x} - \\frac{x}{x}$. \\n \\n This is equal to ${\\sin}\u02c6{2} x - 1$. It is equal to ${\\sin}\u02c6{2} x - 1$. \\n \\n #### Explanation: \\n \\n If we have $\\frac{1 - x}{x}$, we\ncan write it as $\\frac{1}{x} - \\frac{x}{x}$. \\n \\n This is equal to ${\\sin}\u02c6{2} x - 1$. It is equal to ${\\sin}\u02c6{2} x - 1$. \\n \\n #### Explanation: \\n \\n If we have $\\frac{1 - x}{x}$, we\ncan write it as $\\frac{1}{x} - \\frac{x}{x}$. \\n \\n This is equal to ${\\sin}\u02c6{2} x - 1$. SMS scnews\nitem created by Hannah Bryant at Wed 25 May 2022 1227 \\n Type: Seminar \\n Distribution: World \\n Expiry: 31\nMay 2022 \\n Calendar1: 31 May 2022 1500-1600 \\n CalLoc1: Quad S224 & via Zoom \\n CalTitle1: SMRI \u2019What is\n...a virtual knot?\u2019 Hans Boden (McMaster University) \\n Auth: hannahb@w1d4n6z2.staff.sydney.edu.au (hbry8683)\n19Figure 14: An example of dynamic token selection changes during the training process , which\nillustrated with five different score levels represented by deep blue, light blue, black, light orange,\nand dark orange. The bluer the color indicates a higher tendency for the token to be selected, while\nthe more orange the color suggests a lower tendency for the token to be selected.",
        "Results and Findings": "Our initial\nanalysis examines token-level training dynamics of language model, revealing\ndistinct loss patterns for different tokens. After fine-tuning,\nRHO-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH\ndataset, respectively \u2014 matching DeepSeekMath with only 3% of the pretraining\ntokens. Furthermore, when continual pretraining on 80B general tokens, RHO-1\nachieves 6.8% average enhancement across 15 diverse tasks, increasing both data\nefficiency and performance of the language model pre-training. 0 5 10 15\nT okens (B)5101520Math Acc (%)\n16.3% better\n10x fasterAvg Few-shot Acc on 1B LMs\nDeepSeekMath-1B (150B T okens)\nRho-1-1B\nBaseline\n0 5 10 15\nT okens (B)20253035404550\n16.4% better\n5x fasterAvg Few-shot Acc on 7B LMs\nDeepSeekMath-7B (500B T okens)\nRho-1-7B\nBaseline\nFigure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%,\nachieving the baseline performance 5-10x faster. Work done during their internships at Microsoft\nResearch Asia. B:zhenghaolin@stu.xmu.edu.cn ;zebgou@gmail.com\n\u22c4Correspondence authors. Furthermore, research indicates that the distribution of web data does not inherently align with the\nideal distribution for downstream applications [Tay et al., 2022, Wettig et al., 2023]. Our\nfindings reveal that significant loss reduction is limited to a select group of tokens. We show through comprehensive experiments that SLM significantly enhances token efficiency during\ntraining and improves performance on downstream tasks. Furthermore, our findings indicate that SLM\neffectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores\n3\u201cRho\u201d denotes selective modeling of tokens with higher information \u201cdensity ( \u03c1)\u201d. Upon fine-tuning,\nRHO-1-1B and 7B achieve 40.6% and 51.8% on MATH, respectively. Our analysis uncovers that a mere 26% of tokens show a notable loss reduction\n(H\u2192L), while the majority (51%) remain in the L \u2192L category, indicating they have already been\nlearned. The cross-entropy loss for the selected tokens is computed as follows:\nLSLM(\u03b8) =\u22121\nN\u2217k%NX\ni=1Ik%(xi)\u00b7logP(xi|x<i;\u03b8) (4)\nHere, N\u2217k%defines the number of tokens that fall within the top k%of excess loss. Moreover, we compare RHO-1with\nwell-known and top-performing baselines, including Gemma [Team et al., 2024], Qwen1.5 [Bai et al.,\n2023], Phi-1.5 [Li et al., 2023b], DeepSeekLLM [DeepSeek-AI, 2024], DeepSeekMath [Shao et al.,\n2024], CodeLlama [Roziere et al., 2023], Mistral [Jiang et al., 2023], Minerva [Lewkowycz et al.,\n2022], Tinyllama [Zhang et al., 2024], LLemma [Azerbayev et al., 2023], and InternLM2-Math [Ying\net al., 2024]. Further details on our evaluation can be found in\nAppendix E.\n3.2 Math Pre-training Results\nFew-shot CoT Reasoning Results We evalute base models prompting with few-shot chain-of-\nthought (CoT) [Wei et al., 2022a] examples following previous works [Lewkowycz et al., 2022,\nAzerbayev et al., 2023, Shao et al., 2024]. As results shown in Table 1, in comparison to continue\npretraining directly, RHO-1-Math has achieved the average few-shot accuracy improvement of\n4https://github.com/EleutherAI/lm-evaluation-harness\n5https://github.com/ZubinGou/math-evaluation-harness\n616.5% on 1B models and 10.4% on 7B models. Furthermore, after training for multiple epochs on\nOpenWebMath, we find that RHO-1could further increase the average few-shot accuracy to 40.9%. Compared to DeepSeekMath-7B, which pretrained on 500 billion math-related tokens, RHO-1-7B\npretrained on only 15 billion tokens (selecting 10.5 billion tokens) achieved comparable results,\ndemonstrating the efficiency of our approach. Tool-Integrated Reasoning Results We fine-tune RHO-1and baseline models on 69k ToRA corpus\n[Gou et al., 2024], consisting of 16k GPT-4-generated trajectories in a tool-integrated reasoning\nformat, and 53k answer-augmented samples using LLaMA. As presented in Table 2, RHO-1-1B\nandRHO-1-7B achieved a state-of-the-art 40.6% and 51.8% on MATH dataset, respectively. MMLU BBH MATH GSM8kMBPP(p@1) MBPP(p@10) HumEval(p@1) HumEval(p@10)010203040Metrics (%)+11.3\n+3.9\n+5.0+28.2\n+6.5+7.8\n+6.9+10.6Performance of General Pretrained Base Model\nAGIEval ARC-C ARC-E BoolQ PIQAHellaSwag WinoGrandeOBQA TydiQA20304050607080Metrics (%) +1.1+5.0+8.6+11.3+0.9\n+1.4 +0.2\n+3.4\n+8.9Tinyllama\nTinyllama-CT\nRho-1-1B\nFigure 5: General pretraining results. 3.3 General Pre-training Results\nWe confirm the efficacy of the SLM in general pretraining by continual training Tinyllama-1.1B\non 80 billion tokens. The results depicted in Figure 5 indicate that although Tinyllama has already\nundergone extensive training on the majority of these tokens, the application of SLM yields an\naverage enhancement of 6.8% across 15 benchmarks compared to direct continual pretraining. Details are provided in Appendix H.\n7Table 3: Self-Reference results. ToksTrain\nToksGSM8K MATH SV AMP ASDiv MA WPS MQA A VG\nTinyllama-CT (RM) - OWM 14B 15B 6.3 2.6 21.7 36.7 47.7 13.9 21.5\nTinyllama-SLM LRM OWM 14B 10.5B 6.7 4.6 23.3 40.0 54.5 14.3 23.9\nTinyllama-SLM HRM OWM 14B 10.5B 7.0 4.8 23.0 39.3 50.5 13.5 23.0\nTinyllama-SLM LRM\u2229 H RMOWM 14B 9B 7.1 5.0 23.5 41.2 53.8 18.0 24.8\nTinyllama-CT - PPile 55B 52B 8.0 6.6 23.8 41.0 54.7 14.2 24.7\nTinyllama-SLM LRM\u2229 H RMPPile 55B 36B 8.6 8.4 24.4 43.6 57.9 16.1 26.5\n0 1 2 3 4\nT okens (B)0.981.021.061.10Loss(a) Selected T oken Loss\nBaseline\nRho-1\n0 1 2 3 4\nT okens (B)0.850.900.951.001.051.10Loss(b) Downstream T oken Loss\nBaseline\nRho-1\n0 1 2 3 4\nT okens (B)2.52.93.33.74.1Loss(c) Unselected T oken Loss\nBaseline\nRho-1\nFigure 6: The dynamics of pretraining loss and downstream loss. We tested the\nabove results through the process of pretraining with a total of 4 billion tokens. Additionally, we considered training on the intersection of\ntokens selected by the two scoring functions and found better performance, with a 40% reduction in\ntokens and +3.3% performance. The RHO-1showed greater loss reduction on selected tokens than regular pretraining. Cross-referencing figures (a), (b), and (c) reveals that selected-token pretraining substantially lowers\ndownstream loss, while traditional pretraining\u2019s effect on downstream loss is less pronounced despite\ninitial loss reductions. In Figure 7, we demonstrate that the loss of selected tokens correlates with downstream task perfor-\nmance, following a power law similar to recent findings [Gadre et al., 2024]. Our analysis shows\nthat tokens selected by SLM positively impact performance, while those not selected have a negative\nimpact. We observe that the majority of tokens chosen\n80.86 0.89 0.92 0.95 0.98\nLoss5.07.510.012.515.017.520.022.5Accuracy(%)(a) Accuracy vs. Selected T okens' Loss\nSelected T okens at 2B\nSelected T okens at 5B\nSelected T okens at 8B\nSelected T okens at 11B\nSelected T okens at 14B\n3.50 3.54 3.58 3.62 3.66\nLoss5.07.510.012.515.017.520.022.5Accuracy(%)(b) Accuracy vs. Unselected T okens' Loss\nUnselected T okens at 2B\nUnselected T okens at 5B\nUnselected T okens at 8B\nUnselected T okens at 11B\nUnselected T okens at 14BFigure 7: The relationship between the selected tokens / unselected tokens loss in SLM and\ndownstream task performance. 40 50 60 70 80 90 100\nT oken Select Ratio (%)05101520Accuracy (%)\nAccuracy vs. Effect of Token Select Ratio We investigate the impact of token selecting ratios of the SLM. We conducted the detailed analysis of the loss of\ntokens during the pretraining process and found that not all tokens are equal during pretraining. Our\n9experiments and analysis in the fields of mathematics and general have demonstrated the effectiveness\nof the SLM method, emphasizing the importance of token level in the LLM pretraining process. 2022ZD0160501), the Natural Science Foundation of China (No.62372390,62432011). Zhibin\nGou and Yujiu Yang were supported by the Shenzhen Science and Technology Program\n(JCYJ20220818101001004) and the \u201cGraph Neural Network Project\u201d of Ping An Technology (Shen-\nzhen) Co., Ltd.\nReferences\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. OpenAI. Gpt-4 technical report, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, et al. arXiv preprint arXiv:2312.11805 , 2023. Jesse Dodge, Maarten Sap, Ana Marasovi \u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason\nWei, Kevin Robinson, David Mimno, et al. arXiv preprint arXiv:2305.13169 , 2023. arXiv preprint arXiv:2207.10551 , 2022. Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. doi: 10.18653/v1/2023.eacl-main.217. Eyke H\u00fcllermeier and Willem Waegeman. Machine learning , 110(3):457\u2013506, 2021. Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. 10Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. arXiv preprint arXiv:2403.02333 ,\n2024. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. arXiv preprint arXiv:2305.20050 , 2023. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of\nhigh-quality mathematical web text, 2023. https://www.cerebras.net/blog/\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama , 2023. URL\nhttps://huggingface.co/datasets/cerebras/SlimPajama-627B . CoRR , abs/2305.06161, 2023a. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Mistral 7b. arXiv\npreprint arXiv:2310.06825 , 2023. arXiv preprint arXiv:2403.08295 , 2024. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023. DeepSeek-AI. arXiv preprint\narXiv:2401.02954 , 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya\nGuo. arXiv preprint\narXiv:2402.03300 , 2024. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang,\nJia Deng, Stella Biderman, and Sean Welleck. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong,\nKuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332 , 2024. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. URL https://zenodo.org/records/10256836 . Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-\nzalez, Hao Zhang, and Ion Stoica. In NIPS , volume 35, pages\n24824\u201324837, 2022a. Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao,\nJean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev,\nAlexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard\nHeckel, Niklas Muennighoff, and Ludwig Schmidt. Language models scale reliably with over-training and on\ndownstream tasks. Preprint , 2024. Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. In NAACL-HLT (1) , pages 4171\u20134186. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Journal of\nmachine learning research , 21(140):1\u201367, 2020. Stanislas Polu and Ilya Sutskever. Textbooks are all you need. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 ,\n2021. Nikhil Kandpal, Eric Wallace, and Colin Raffel. In International Conference on Machine Learning , pages 10697\u201310707. Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. In NIPS , volume 36, 2023. 12Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le,\nTengyu Ma, and Adams Wei Yu. Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher R\u00e9. Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. arXiv preprint arXiv:2308.12032 , 2023c. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang\nLiu, Fei Huang, et al. arXiv\npreprint arXiv:2312.10302 , 2023d. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. arXiv preprint arXiv:2402.04333 , 2024. Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar\nSahu, and Ruoxi Jia. Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang,\nBaigui Sun, Xuansong Xie, and Yang You. Ozan Sener and Silvio Savarese. Ilya Loshchilov and Frank Hutter. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Angelos Katharopoulos and Fran\u00e7ois Fleuret. In International conference on machine learning , pages 2525\u20132534. Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri\nJoshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. arXiv preprint arXiv:1910.00762 , 2019. 13Hwanjun Song, Minseok Kim, Sundong Kim, and Jae-Gil Lee. S\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu,\nBenedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal. PMLR,\n17\u201323 Jul 2022. Simin Fan and Martin Jaggi. arXiv preprint\narXiv:2310.15389 , 2023. In Thirty-seventh Conference on Neural\nInformation Processing Systems , 2023. arXiv preprint\narXiv:1906.11829 , 2019. Logan Engstrom, Axel Feldmann, and Aleksander Madry. arXiv preprint arXiv:2401.12926 , 2024. Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and Percy Liang. arXiv preprint arXiv:1909.02060 , 2019. Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher\nPotts. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. arXiv preprint arXiv:1810.04805 , 2018. Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun. doi: 10.18653/v1/2020.emnlp-main.566. Tanish Lad, Himanshu Maheshwari, Shreyas Kottukkal, and Radhika Mamidi. arXiv preprint arXiv:2211.13815 , 2022. Qihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu, Min Zhang, Bo Du, and Dacheng Tao. doi: 10.18653/v1/2023.acl-long.579. Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and Denny Zhou. doi:\n10.18653/v1/2022.acl-long.262. Tianjian Li, Haoran Xu, Philipp Koehn, Daniel Khashabi, and Kenton Murray. arXiv preprint arXiv:2310.00840 , 2023e. Jessica Rumbelow and Matthew Watkins. arXiv preprint arXiv:2405.05417 , 2024. 14Naomi Saphra and Adam Lopez. Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend. arXiv preprint arXiv:2109.06096 , 2021. Leo Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A Smith. arXiv preprint arXiv:2104.07885 , 2021. arXiv preprint arXiv:2201.02177 , 2022. arXiv preprint\narXiv:2212.09803 , 2022. Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. arXiv preprint\narXiv:2102.01293 , 2021. arXiv preprint arXiv:2203.15556 , 2022. arXiv preprint\narXiv:2206.07682 , 2022b. arXiv preprint arXiv:2402.04177 ,\n2024. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. arXiv preprint arXiv:2202.07646 , 2022. Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu\nPurohit, and Edward Raff. arXiv preprint arXiv:2205.10487 , 2022. Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. Springer,\n1984. In NIPS , 2021. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\nNeubig. arXiv preprint arXiv:2211.10435 , 2022. 15Arkil Patel, Satwik Bhattamishra, and Navin Goyal. doi: 10.18653/v1/2021.naacl-main.168. doi: 10.18653/v1/\n2020.acl-main.92. URL https://aclanthology.org/2020.acl-main.92 . doi: 10.18653/v1/N16-1136. URL\nhttps://aclanthology.org/N16-1136 . URL https://openreview. Mathqa:\nTowards interpretable math word problem solving with operation-based formalisms. arXiv preprint\narXiv:1905.13319 , 2019. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. arXiv preprint arXiv:2009.03300 , 2020. arXiv preprint arXiv:2210.09261 , 2022. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\nand Nan Duan. arXiv preprint\narXiv:2304.06364 , 2023b. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. arXiv preprint arXiv:1905.10044 ,\n2019. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. arXiv preprint arXiv:1905.07830 , 2019. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. arXiv preprint arXiv:1809.02789 , 2018. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang,\nYang Li, et al. arXiv preprint\narXiv:2108.07732 , 2021. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu,\nYK Li, et al. arXiv preprint arXiv:2401.14196 , 2024. In ACL (1) , pages\n12286\u201312312. Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. In International Conference on Machine Learning , pages 10421\u201310430. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nG.2 Dynamic Token Selected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Nevertheless, strict filtering like blocklist [Raffel et al.,\n2020] and Safety API filtering [Welbl et al., 2021], have been found to hurt evaluation loss or induce\nbias [Dodge et al., 2021]. [2023] find that batch\nselection is not computationally efficient. [2024a] and Engstrom et al. Our\nfindings, however, diverge from those of Xia et al. We identify a spectrum of token patterns, including \u201ceasy tokens\u201d\nand \u201chard tokens\u201d that resist convergence. [2022] and Azerbayev\net al. During the training process,\nwe collected the loss of each token after training on each 1 billion tokens training data. Within a segment of normal text, there may also be fluctuations in the usage\nof common conjunctions, word suffixes, and punctuation marks. On HumanEval [Zheng et al., 2023] and TydiQA [Clark et al.,\n2020], we follow the evaluation pipeline of open-instrcut [Ivison et al., 2023] and report Pass@1 and\nPass@10 for HumanEval and F1 for TydiQA. For MBPP [Austin et al., 2021] benchmark, we follow\nthe evaluation pipeline of DeepSeek-Coder [Guo et al., 2024], and report Pass@1 and Pass@10. Based on the trend of data points in Figure 7, we propose the relationship\nbetween the average accuracy of downstream tasks and selected/unselected tokens\u2019 loss,\nAcc(L) = log( a\u2217 L+c) (7)\nThe parameters aandcare fitted from the data. 22Table 4: Full Self-Reference results on Tinyllama-1.1B. In Table 4, we provide more SLM results, including different select ratios and combinations of two\nscore functions, for the convenience of the readers to refer to. Results presented in Table 5 indicate that, despite the considerable\ngap between the small and large models [Li et al., 2023f], employing the small reference model to\ntoken selection can still yield benefits to the pre-training of the larger model. \u2022The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings. \u2022The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). In general, empirical results often\ndepend on implicit assumptions, which should be articulated. 4.Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)? \u2022If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed. \u2022The instructions should contain the exact command and environment needed to run to\nreproduce the results. \u2022Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted. \u2022The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them. \u2022The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper. \u2022If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text. 9.Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? \u2022Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters. \u2022We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort. \u2022The authors should state which version of the asset is used and, if possible, include a\nURL. Their licensing guide can help determine the\nlicense of a dataset. \u2022Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. \u2022Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper. If you obtained IRB approval, you\nshould clearly state this in the paper. ]: 890 [0], given: 235 \\n \\n Re:\nMr. and Mrs Wiley, VIC[#permalink] 13 Feb 2010, 01:03 \\n Ans A \\n \\n their first child was born after J years... \\n\n\\n thus 1 child \u2014> j years \\n \\n => thus after another J years his age = J \\n \\n thus his age is J \u2013> after 2J years and\n2j after 3j years \\n \\n his present age is T which is after T years. \\n \\n thus total time after 2years will be T+2 \\n\nsince after every J year they have a child after T+2 they will have \\frac{(T+2)}{J} + 1 ( +1 is for the oldest) \\n\n\\n thus A \\n _________________ \\n \\n Fight for your dreams :For all those who fear from Verbal- lets give it a\nfight \\n \\n Money Saved is the Money Earned \\n \\n Jo Bole So Nihaal , Sat Shri Akaal \\n \\n Gmat test review :\n\\n 670-to-710-a-long-journey-without-destination-still-happy-141642.html \\n \\n Intern \\n Joined: 06 Apr 2012 \\n\nPosts: 28 \\n Followers: 0 \\n \\n Kudos [? Their oldest child is now T years old. Please share your take. Their oldest child is now T years old. \\n \\n I\u2019ve stated several times on various threads,\nthat SO<sub>3</sub> produces a practically incondensable acid mist when led to water and, BTW, at 700 \u00b0C the\ndecomposition rate of SO<sub>3</sub> is \u02dc87% . . . 16Figure 11: Sample text containing four categories of tokens. \u201dQuasi*\nmap*\u201d py: 1989 The resulting documents have publication year 1989. so: Eur* J* Mat* Soc* cc: 14 Search for\npublications in a particular source with a Mathematics Subject Classification code (cc) in 14. Terms can be grouped within (parentheses). ISO 639-1 language codes can also be used. Code: Select all \\n \\n x = 64, y = 86, rule = B3/S23 \\n 13bo$3bobo6bo$4b2o6b3o$4bo$54bo$54bobo$13b2o39b\n2o $12b2o44b2o$3o11bo43b \\n o3b2o$2bo49bo 6bo2bo$bo50b 2o6b obo$51bob o7bo$7bo49bo$7\nb3o47b3o$10bo5b2o \\n 42bo$9b2o4b2o 42b 2o$17bo7$13bo$3b obo6bo$4b 2o6b\n3o$4bo$54bo$54b obo$13b 2o 39b2o$12b2o44b2o$3o11bo43bo3b2o$2bo49bo6bo2bo$bo50b 2o6b\nobo$51bobo7bo$7bo49bo$7b3o47b3o$10bo5b2o42bo$9bo5b2o42bo$9b2o6bo41b2o7$13bo$3bobo6bo$4b\n2o6b3o$4bo$54bo$54bobo$13b2o39b2o$12b2o44b2o$3o11bo43bo3b2o$2bo49bo\n6bo2bo$bo50b2o6bobo$51bobo7bo$7bo49bo$7b3o47b3o$10bo5b2o42bo$7b3o5b2o\n40b3o$7bo9bo39bo7$13bo$3b obo6bo$4b 2o6b 3o$4bo$54bo$54b obo$13b 2o39b 2o$ \\n 12b 2o44b\n2o$3o11bo43bo3b 2o$2bo49bo6bo2bo$bo50b 2o6b obo$51b obo7bo$7bo49bo$7b 3o47b 3o$10bo5b 2o42bo$7b\n2obo4b 2o40b 2obo$7b obo7bo39b obo! These things at\nleast we learned how to do at the Vancouver code camp. Thus, using kinematic equation, horizontal\ndistance covered will be \u2013 \\n \\n x = u \\cos \\theta t \\n \\n Or, \\quad t = ( \\frac { x }{ u \\cos \\theta } ) \\n \\n Vertical\nmotion has constant acceleration ( g ) . Thus, distance covered will be \u2013 \\n \\n y = ( u \\sin \\theta ) t - \\left ( \\frac\n{1}{2} \\right ) g t\u02c62 \\n \\n = ( u \\sin \\theta ) \\left ( \\frac {x}{u \\cos \\theta} \\right ) - \\left ( \\frac {1}{2} \\right ) g \\left (\n\\frac {x}{u \\cos \\theta} \\right )\u02c62 \\n \\n = \\left ( \\tan \\theta \\right ) x - \\left ( \\frac {g}{2 u\u02c62 \\cos\u02c62 \\theta} \\right ) x\u02c62\n\\n \\n In this equation, ( \\theta, \\ u \\ \\text {and} \\ g ) are constants. Term \\left [ \\left ( \\frac {g}{2 u\u02c62 \\cos\u02c62 \\theta} \\right ) \\right ] is also a constant, let it is\n( q ) \\n \\n So, \\quad y = p x - q x\u02c62 \\n \\n Therefore, ( y \\propto x\u02c62 ) , which is a required condition of a parabola. Then \u2013 \\n \\n Net vertical\ndisplacement covered during the time of flight is zero. Using kinematic equation of motion, we get \u2013 \\n \\n 0 = ( u\n\\sin \\theta ) T_f - \\left ( \\frac {1}{2} \\right ) g \\ ( T_f )\u02c62 \\n \\n Or, \\quad T_f = \\left ( \\frac {2 u \\sin \\theta}{g} \\right\n) = 2 \\left ( \\frac {u \\sin \\theta}{g} \\right ) \\n \\n = 2 t_m \\n \\n Thus, \\quad \\text {Total time of flight} = \\text {Time\nof ascent} + \\text {Time of descent} \\n \\n = 2 \\times \\text {Time of maximum height.} \\n \\n From kinematic equation of motion, we have \u2013 \\n\n\\n v\u02c62 = u\u02c62 + 2 a s \\n \\n Therefore, \\quad 0\u02c62 - ( u \\sin \\theta )\u02c62 = 2 ( - g ) h_m \\n \\n Or, \\quad h_m = \\left ( \\frac\n{u\u02c62 \\sin\u02c62 \\theta}{2 g} \\right )\nWe identify two equations having the same solution with the equivalence relation: \\n \\n $(a,b) \\sim (c,d) \\mbox{ if\nand only if } ad = bc$ \\n \\n To show that this is an equivalence relation: \\n \\n 1. Hence it is reflexive. The equation of motion $$ds\u02c62 =0$$ then takes the form $$g_{tt} \\left(\\frac{dt}{d\\lambda}\\right)\u02c62\n+ g_{rr} \\left(\\frac{dr}{d\\lambda}\\right)\u02c62 = 0,$$ which we can rewrite as $$\\left(\\frac{dt}{dr}\\right)\u02c62 = -\n\\frac{g_{rr}}{g_{tt}}.$$ \\n \\n The length of the rod is then $$L = c \\int_{r_1}\u02c6{r_2} \\frac{dt}{dr} \\text{ d}r = c\n\\int_{r_1}\u02c6{r_2} \\sqrt{-\\frac{g_{rr}}{g_{tt}}} \\text{ d}r,$$ where I have taken the positive square root because\n$$r_2 > r_1$$. \\n \\n For the Schwarzschild\nmetric, we obtain explicitly $$L = r_2 - r_1 + r_s \\ln\\left(\\frac{r_2 - r_s}{r_1 - r_s}\\right) > r_2 - r_1.$$ \\n \\n Now\nwhat happens if you magically, instantaneously increase the mass of the black hole? 34After Training 0% Checkpoint\nItem Type: Journal Article Copyright of this article belongs to Elsevier. \\n \\n So, we can write \\n \\n $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2 x = \\frac{1 - \\left({\\cot}\u02c6{2} x\n+ 1\\right)}{\\csc} \u02c6 2 x$ \\n \\n $= {\\cot}\u02c6{2} \\frac{x}{\\csc} \u02c6 2 x$ \\n \\n Recall also that $\\cot x = \\cos \\frac{x}{\\sin}\nx$ and $\\csc x = \\frac{1}{\\sin} x$. \\n \\n So, we can write \\n \\n $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2 x = \\frac{1 - \\left({\\cot}\u02c6{2} x\n+ 1\\right)}{\\csc} \u02c6 2 x$ \\n \\n $= {\\cot}\u02c6{2} \\frac{x}{\\csc} \u02c6 2 x$ \\n \\n Recall also that $\\cot x = \\cos \\frac{x}{\\sin}\nx$ and $\\csc x = \\frac{1}{\\sin} x$. \\n \\n So, we can write \\n \\n $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2 x = \\frac{1 - \\left({\\cot}\u02c6{2} x\n+ 1\\right)}{\\csc} \u02c6 2 x$ \\n \\n $= {\\cot}\u02c6{2} \\frac{x}{\\csc} \u02c6 2 x$ \\n \\n Recall also that $\\cot x = \\cos \\frac{x}{\\sin}\nx$ and $\\csc x = \\frac{1}{\\sin} x$. \\n \\n So, we can write \\n \\n $\\frac{1 - {\\csc}\u02c6{2} x}{\\csc} \u02c6 2 x = \\frac{1 - \\left({\\cot}\u02c6{2} x\n+ 1\\right)}{\\csc} \u02c6 2 x$ \\n \\n $= {\\cot}\u02c6{2} \\frac{x}{\\csc} \u02c6 2 x$ \\n \\n Recall also that $\\cot x = \\cos \\frac{x}{\\sin}\nx$ and $\\csc x = \\frac{1}{\\sin} x$. 35",
        "Conclusion": "\u00a73.3\nconfirms the efficacy of SLM in general continual pretraining: Training Tinyllama-1B on 80B tokens\nwith SLM improves 6.8% on average across 15 benchmarks, with gains over 10% in code and math\ntasks. (Step 3) Finally, selectively train the language model on\ntokens that have higher scores. corpus. In the final phase, we train the language model selectively, focusing on tokens with high\nexcess loss between the training and reference model. 4 Conclusion\nIn this paper, we propose using Selective Language Modeling(SLM) to train RHO-1, which select\nmore suitable tokens for current pretraining stage. Advances in neural information processing systems , 33:1877\u20131901, 2020. In\nFindings of the Association for Computational Linguistics: EMNLP 2021 , pages 2447\u20132469, 2021. URL\nhttps://aclanthology.org/2023.eacl-main.217 . InICLR , 2024. In ICLR , 2024. Teknium. Advances in Neural Information Processing Systems , 35:3843\u20133857, 2022. In ICLR , 2024. PMLR, 2022. Skill-it! In ICLR , 2024. Together Computer. PMLR, 2018. URL https://aclanthology.org/2023.acl-long.579 . URL https://aclanthology.org/2022.acl-long.262 . LessWrong, 2023. Advances in neural information processing systems , 35:27730\u201327744, 2022. URL\nhttps://aclanthology.org/2021.naacl-main.168 . PMLR, 2023. Xie et al. [2023]. necessary to understand the\nresults? \\n \\n (A) \\frac{T+2}{J} + 1 \\n \\n (B) JT + 1 \\n \\n (C) \\frac{J}{T} + \\frac{1}{T} \\n \\n (D) TJ - 1 \\n\n\\n (E) \\frac{T+J}{J} \\n \\n [Reveal] Spoiler: OA: \\n (A) \\n \\n Source: Manhattan Guide \\n \\n Bunuel - would really\nappreciate you providing your bit on solving the original problem above algebraically. Thank you. \\n \\n (A) \\frac{T+2}{J} + 1 \\n \\n (B) JT + 1 \\n \\n (C) \\frac{J}{T}\n+ \\frac{1}{T} \\n \\n (D) TJ - 1 \\n \\n (E) \\frac{T+J}{J} \\n \\n [Reveal] Spoiler: OA: \\n (A) \\n \\n Source: Manhattan\nGuide \\n \\n Think of it as an Arithmetic Progression where every subsequent term (child) has a difference of J yrs\nfrom the previous term (child). Success! anyways... The 16-bitter thus goes down to 9 gliders. The end goal is to count the following forms as correct,\npossibly controlled by flags: n \\{}prod (factor)\u02c6power, where each factor is irreducible in Z[X], n in Z r \\{}prod\n(factor)\u02c6power, where each factor is irreducible and monic in Q[X], r in Q I suppose on the last one the monic\nrequirement could be dropped with a flag. Thus, \\n \\n 1. After that, body starts moving downwards. \\n \\n Therefore, from kinematic equation, we have \u2013 \\n \\n 0 = u \\sin \\theta - g t_m \\n \\n Or,\n\\quad t_m = \\left ( \\frac {u \\sin \\theta}{g} \\right ) \\n \\n ### Time of Flight \\n \\n Total time taken by the projectile\nbetween the instant it is projected and till it reaches at a point in the horizontal plane of its projection is called Time\nof flight. \\n 2. Hence it is symmetric. \\n 3. Hence\n$$(a,b) \\sim (e,f)$$. \\n \\n Hence, we have successfully formed the set of rational numbers when we factor out\nthe equivalence classes!",
        "label": 4
    },
    {
        "Abstract": "Stochastic Taylor Derivative Estimator: Efficient\namortization for arbitrary differential operators\nZekun Shi\nNational University of Singapore\nSea AI Lab\nshizk@sea.com,Zheyuan Hu\nNational University of Singapore\ne0792494@u.nus.edu,Min Lin\nSea AI Lab\nlinmin@sea.com,\nKenji Kawaguchi\nNational University of Singapore\nkenji@nus.edu.sg\nAbstract\nOptimizing neural networks with loss that contain high-dimensional and high-order\ndifferential operators is expensive to evaluate with back-propagation due to O(dk)\nscaling of the derivative tensor size and the O(2k\u22121L)scaling in the computation\ngraph, where dis the dimension of the domain, Lis the number of ops in the\nforward computation graph, and kis the derivative order. 1 Introduction\nIn many problems, especially in Physics-informed machine learning [ 19,32], one needs to solve\noptimization problems where the loss contains differential operators:\narg min\n\u03b8f(x, u\u03b8(x),D\u03b1(1)u\u03b8(x), . 3 Preliminaries and discussions\n3.1 First-order auto-differentiation (AD)\nAD is a technique for evaluating the gradient of composition of known analytical functions commonly\ncalled primitives. Backward mode AD Each primitives Fiis linearized as the adjoint of the Fr\u00e9chet derivative \u2202\u22a4Fi\ninstead, which computes the vector-Jacobian-product (VJP): \u2202\u22a4Fi(a)(v\u22a4) =v\u22a4\u2202F\n\u2202x\f\f\nawhere v\u22a4is\nthe cotangent. The tuple Jg(t) := ( g(t), g\u2032(t))\ncan be thought of as the first-order expansion of gwhich lives in the tangent bundle of F. Treating\nFas the smooth map between manifolds, we can define the pushforward dFwhich pushes the first\norder expansion of g(i.e.Jg(t)) forward to the first order expansion of F\u25e6g(i.e.JF\u25e6g(t)):\ndF(Jg(t)) =JF\u25e6g(t) =\u0012\n[F\u25e6g](t),d\ndt[F\u25e6g](t)\u0013\n= (F(a), \u2202F(a)(v)). dkdefines a high-order AD: we can compute dkFof arbitrary composition F\nfrom the kth-order pushforward of the primitives dkFi, since dkis an homomorphism of the group\n({Fi},\u25e6):\ndk[F2\u25e6F1](Jk\ng(t)) =Jk\nF2\u25e6F1\u25e6g(t) = dkF2(Jk\nF1\u25e6g(t)) = [dkF2\u25e6dkF1](Jk\ng(t)). The action of the\nderivative D\u03b1=\u2202|\u03b1|\n\u2202x\u03b11\n1,...,\u2202x\u03b1d\ndon function ucan be identified with the derivative tensor slice D|\u03b1|\nu(a)\u03b1. , Dknu)\nwhere fis arbitrary function. v(vk)\ndk] =Dk\nu(a)d1,...,d kE[v(v1)\nd1. (20)\nAll the derivative terms can be found in the pushforward of the following jet:\nJ= d13u(x,v(1), . Furthermore, we did not consider variance reduction techniques that could be\napplied, which can be explored in future works. This shows the connection between the fields of AD and randomized numerical\nlinear algebra and indicates that further works in the intersection of these two fields might bring\nsignificant progress in large-scale scientific modeling with neural networks. A general construction for abstract interpretation of higher-order automatic differentiation. Lax pairs informed neural networks solving integrable systems,\nJanuary 2024. arXiv:2401.04982 [nlin]. Theadjoint (transpose) of the Fr\u00e9chet derivative of f:Rn\u2192Rm, denoted as \u2202\u22a4f, is thus defined\nas\n\u2202\u22a4f:Rn\u2192L(Rm,Rn), \u2202\u22a4f(a)(v) =v\u22a4Jf(a),v\u2208Rm(32)\nwhere v\u22a4is the cotangent which lives in the dual space of the codomain. D Taylor mode AD\nD.1 High-order Fr\u00e9chet Derivatives\nThekth order Fr\u00e9chet derivative of a function f:Rn\u2192Rmat a point ais the multi-linear map with\nkarguments around point athat best approximates f. For example, when k= 2, we have\n\u22022f:Rn\u2192L(Rn\u00d7Rn,Rm), \u22022f(a)(v,v\u2032) =v\u22a4Hf(a)v\u2032=X\nj,kHf(a)i,j,kvjv\u2032\nk (33)\nwhere Hf(a)\u2208Rm\u00d7n\u00d7ndenote the Hessian of fat point a, andv,v\u2032\u2208Rn. (38)\nwhere the outermost summation is taken over all partitions of the derivative order k. Here a partition\nofkis defined as a tuple (p1, . 17ERemoving the mixed partial derivatives term from second order semilinear\nparabolic PDE\n1\n2tr\u0000\n\u03c3(x, t)\u03c3(x, t)\u22a4(Hess xu)(x, t)\u0001\n=1\n2tr\u0000\n\u03c3(x, t)\u22a4(Hess xu)(x, t)\u03c3(x, t)\u0001\n=1\n2dX\ni=0\u0002\n\u03c3(x, t)\u22a4(Hess xu)(x, t)\u03c3(x, t)\u0003\ni,i\n=1\n2dX\ni=0e\u22a4\ni\u03c3(x, t)\u22a4(Hess xu)(x, t)\u03c3(x, t)ei\n=1\n2dX\ni=0\u22022u((x, t), \u03c3(x, t)ei,0\u22a4)[3]. For example, the pushforward of the\nfifth-order tangent is\n10\u00b7D3\nf(a)l,m1,m2,m3v(3)\nm1v(1)\nm2v(1)\nm3+D5\nf(a)l,m1,m2,m3,m4,m5v(1)\nm1v(1)\nm2v(1)\nm3v(1)\nm4v(1)\nm5, (49)\nif all input tangents are set to zero except for v(1)andv(3). G Further memory reduction via weight sharing in the first layer\nWhen dealing with high-dimensional data, the parameters of the model\u2019s first layer in a conventional\nfully connected network would grow proportionally with the input dimension, resulting in a significant\nincrease in memory requirements and forming a memory bottleneck due to massive model parameters. I.1.1 Further details on ablation study\nThe gain by using JAX instead of PyTorch Since the original SDGD was implemented in PyTorch,\nwe implemented the stacked backward mode without parallelization in SDGD dimensions in JAX\nfor fair comparison (dubbed as \u201cStacked Backward mode SDGD in JAX\u201d in Table 3). (62)\nwith initial condition g(x) = arctan(max ixi),\n\u2022 Sine-Gordon equation\nLu(x, t) =\u22072u(x, t) + sin( u(x, t)). I.4.1 High-order low-dimensional PDEs\nAlternative way to compute the terms in 2D Korteweg-de Vries (KdV) equation The terms in\nthe 2D KdV equation\nuty+uxxxy+ 3(uyux)x\u2212uxx+ 2uyy= 0. (67)\nwhich can be expanded as\nutx+ 6uxux+ 6uuxx+uxxxx+ 3\u03c32uyy= 0. Let\nJ(1):=d7u(x,ex,0,0,0,0,0,0)\nJ(2):=d7u(x,ex,0,0,et,0,0,0)\nJ(3):=d2u(x,et,0). J.1 Review of HTE\nHTE provides a random estimation of the trace of a matrix A\u2208Rd\u00d7das follows:\ntr(A) =Ev\u223cp(v)\u0002\nvTAv\u0003\n,v\u2208Rd(83)\nwhere p(v)isisotropic , i.e.Ev\u223cp(v)[vvT] =I. \u201cNone\u201d\nin the \u201cgPINN method\u201d column indicates that no gPINN loss was used. We can now write the computation of HTE as follows\n\u02dc\u22072p,Nu\u03b8=d\nNNX\nj=1\u22022u\u03b8(x)(vj,0),vj\u223cp(v). 31NeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope? Guidelines:\n\u2022The answer NA means that the abstract and introduction do not include the claims\nmade in the paper. \u2022The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. Answer: [Yes]\nJustification: We discussed the limitation of our work in section 6. 3.Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof? Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Guidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Answer: [NA]\nJustification: Our work is not tied to particular applications, and there are no obvious paths\nthat lead to potential harm. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster. Answer: [NA]\nJustification: Our work is foundational and not tied to particular applications. Answer: [NA]\nJustification: Our work does not use existing assets. Answer: [NA]\nJustification: Our work does not release new assets. Guidelines:\n\u2022 The answer NA means that the paper does not release new assets. Answer: [NA]\nJustification: Our paper did not involve crowdsourcing and human subjects. Guidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects. 15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained? Answer: [NA]\nJustification: Our paper did not involve crowdsourcing and human subjects. Guidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.",
        "Methodology": "(1)\nIn this above, D\u03b1=\u2202|\u03b1|\n\u2202x\u03b11\n1,...,\u2202x\u03b1d\nd,\u03b1= (\u03b11, \u03b12, . When either the differentiation order kor the\ndimensionality dis high, the objective function above is expensive to evaluate with back-propagation\n(backward mode AD) in both memory and computation: the size of the derivative tensor has scaling\nO\u0000\ndk\u0001\n, and the size of the computation graph has scaling O\u0000\n2k\u22121L\u0001\nwhere Lis the number of ops\nin the forward computation graph. One line of work uses\nrandomization to amortize the cost of computing differential operators with AD over the optimization\n1our code will be available at https://github.com/sail-sg/stde\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).process so that the din the above scaling becomes a constant for the case of k= 2. Stochastic\nDimension Gradient Descent (SDGD) [ 13] randomizes over the input dimensions where in each\niteration, the partial derivatives are only calculated for a minibatch of sampled dimensions with\nback-propagation. In [ 12,21,15], the classical technique of Hutchinson Trace Estimator (HTE)\n[16] is used to estimate the trace of Hessian or Jacobian to inputs. In [ 30], the finite difference method is used for\nestimating the Hessian trace. Randomized smoothing [ 11,14] uses the expectation over Gaussian\nrandom variable as ansatz, so that its derivatives can be expressed as another expectation Gaussian\nrandom variable via Stein\u2019s identity [ 38]. However, compared to AD, the accuracy of these methods\nis highly dependent on the choice of discretization. In this work, we address the scaling issue in both dandkfor the optimization problem in Eq. Our main contributions are:\n\u2022We demonstrate how Taylor mode AD [ 6], a high-order AD method, can be used to amortize\nthe optimization problem in Eq. When applied to PINN, our method provides\na significant speed-up compared to the baseline method SDGD [ 13] and the backward-free method\nlike random smoothing [ 11]. Due to STDE\u2019s low memory requirements and reduced computation\ncomplexity, PINNs with STDE can solve 1-million-dimensional PDEs on a single NVIDIA\nA100 40GB GPU within 8 minutes , which shows that PINNs have the potential to solve complex\nreal-world problems that can be modeled as high-dimensional PDEs. We also provide a detailed\nablation study on the source of performance gain of our method. However, accessible\nimplementation for machine learning was not available until the recent implementation in JAX [ 6,7],\nwhich implemented the Taylor mode AD for accelerating ODE solver. Forward mode AD can also be\nused to compute neural network parameter gradient as shown in [2]. The method of zeroth-order optimization [ 24] can be used in this case, as it only requires\nevaluating the function at arbitrary input. 3), that computes the JVP of the composition\u2202F\n\u2202xv:\n\u2202F\n\u2202xv=\u2202F(x)(v) = [\u2202FL\u25e6\u2202FL\u22121\u25e6 \u00b7\u00b7\u00b7 \u25e6 \u2202F1](x)(v). Each JVP call requires O(max( d, h))\nmemory as only the current activation yiand tangent viare needed to carry out the computation, and\nthe computation complexity is usually in the same order as the forward computation graph. The linearized computation graph now runs in the reverse order:\nv\u22a4\u2202F\n\u2202x=\u2202\u22a4F(x)(v\u22a4) = [\u2202\u22a4F1(x)\u25e6 \u00b7\u00b7\u00b7 \u25e6 \u2202\u22a4FL\u22121(yL\u22122)\u25e6\u2202\u22a4FL(yL\u22121)](v\u22a4), (3)\nwhich is also clear from Fig. Furthermore, due to this reversion, we first need to do a forward pass\nto obtain the evaluation trace {yi}L\ni=1before we can invoke the VJPs \u2202\u22a4Fi, which apparent as shown\nin Eq. Hence the number of sequential computations is twice as much compared to forward mode. The memory requirement becomes O(d+ (L\u22121)h)as we need to store the entire evaluation trace. Similar to JVP, VJP computes one row of JFat a time, so the full Jacobian\u2202F\n\u2202xcan be computed\nusing d\u2032VJPs. When optimizing scalar cost functions \u2113(\u03b8) :Rn\u2192Rof the network parameters \u03b8,\nbackward mode efficiently trades off memory with computation complexity as d\u2032= 1and only 1\nVJP is needed to get the full Jacobian. Furthermore, all parameter \u03b8ican use the same cotangent v\u22a4,\nwhereas with forward mode, separate tangent for each parameter \u03b8iis needed. Red nodes represent the cotangent nodes in the second backward pass. With each\nrepeated application of VJP the length of sequential computation doubles. Repeating backward mode AD With each repeated application of backward mode AD, the new\nevaluation trace will include the cotangents from the previous application of backward AD, so the\nlength of sequential computation doubles . And O\u0000\ndk\u22121\u0001\ncalls are needed to evaluate the entire\nderivative tensor. The input tangent dimension is don the\nfirst application of forward mode AD, but on the second application, it will become d\u00d7dsince we\nare now computing the forward mode AD for \u2207u\u03b8:Rd\u2192Rd. So the size of the input tangent with\nkrepeated application is O\u0000\ndk\u0001\n, so it grows exponentially . It replaces a differential operator Dwith a randomly sampled subset of\nadditive terms, where each term only depends on a few input dimensions\nD:=NDX\nj=1Dj\u2248ND\n|J|X\nj\u2208JDj:=\u02dcDJ, (4)\nwhere \u02dcDJdenotes the SDGD operator that approximates the true operator D,Jis the sampled\nindex set, and |J|is the batch size. \u02dcDJare cheaper to compute than Ddue to reduced dimensionality: for each sampled index, by treating\nall other input as constant we get a function with scalar input and output. For a given index set J, the\nmemory requirements are reduced from O\u0000\n2k\u22121(d+ (L\u22121)h)\u0001\ntoO\u0000\n|J|(2k\u22121(1 + ( L\u22121)h))\u0001\n,\nand the computation complexity reduces to O\u0000\n|J|2k(h+ (L\u22121)h2)\u0001\n. 3.4 Univariate Taylor mode AD\nOne way to define high-order AD is by determining how the high-order Taylor expansion of a\nunivariate function changes when mapped by primitives. Firstly, the Fr\u00e9chet derivative \u2202Fcan be\nrewritten to operate on a space curve g:R\u2192Rdthat passes through the primal a, i.e.g(t) =a, and\nhas tangent g\u2032(t) =v:\n\u2202F(g(t))(g\u2032(t)) =\u2202F\n\u2202x\f\f\f\f\nx=g(t)g\u2032(t) =d\ndt[F\u25e6g](t). (5)\nThis shows that the \u2202(JVP) is the same as the univariate chain rule. , g(k)(t)) = ( a,v(1),v(2), . ,v(k))),(7)\nwhich pushes the kth order expansion of g(i.e.Jk\ng) forward to the kth order expansion of F\u25e6g(i.e. 4Since Jk\ngcontains all information needed to evaluate\u2202j\n\u2202tj[F\u25e6g](t)for any j\u2264k, the map dkF\nis well-defined. Although high-order AD methods like Taylor mode\nAD [ 6] can address this scaling issue, it is only defined for univariate functions. In this section, we\ndescribe a method that addresses the scaling issue in kanddsimultaneously when amortizing Eq. 1\nby seeing univariate Taylor mode AD as contractions of multivariate derivative tensor. 4.1 Univariate Taylor mode AD as contractions of multivariate derivative tensor\ndFprojects the Jacobian of FtoRd\u2032with a 1-jet Jg(t). Similarly, dkFcontracts a set of derivative\ntensors to Rd\u2032with a k-jetJk\ng. We can expand\u2202k\n\u2202tkF\u25e6gwith Eq. 43 to see the form of the contractions. How large lshould be depends on how off-diagonal the operator is. If the operator is diagonal (i.e. contains no mixed partial derivatives), l=kis enough. If the operator is maximally non-diagonal,\ni.e. it is a partial derivative where all dimensions to be differentiated are distinct, then the minimum l\nneeded is (1 +k)k/2. For more details, please refer to Appendix F where a general procedure for\ndetermining the jet structure is discussed. The first\ncolumn from the left represents the input 2-jet J2\ng(t) = (x,v(1),v(2)), and d2F1pushes it forward\nto the 2-jet J2\nF1\u25e6g(t) = (y1,v(1)\n1,v(2)\n1)which is the subsequent column. Each row can be computed\nin parallel, and no evaluate trace needs to be cached. Differential operators can be evaluated through derivative tensor contraction. , d kmatches the multi-index \u03b1\u2208 I(L)and0otherwise. We call this method Stochastic\nTaylor Derivative Estimator (STDE) . 2) we see that, for one call to dkF, the\nmemory requirement has scaling of O(kd)and the computation complexity has scaling O\u0000\nk2dL\u0001\n. Combined with randomization, the polynomial scaling in dwill be removed. For example, if the input dimension dis3, thene2= [0,1,0]\u22a4. As\ndiscussed before, there exists a Jk\ngwhose pushforward under \u2202luis equivalent to contracting Dk\nuwith\n\u2297k\ni=1edi. We call k-jet consisting of only standard basis and the zero vector 0sparse . 14 satisfies the unbiasedness condition 12\np(\u2297k\ni=1edi) =Cd1,...,d k/Z, d 1, . , d k\u2208 I(L), (15)\nwhere Zis the normalization factor and we identify \u2297k\ni=1ediwith the corresponding k-jetJk\nu. High-order diagonal differential operators We call a differential operator diagonal if it is a linear\ncombination of diagonal elements from the derivative tensor: L=Pd\nj=1\u2202k\n\u2202xk\nj. (18)\nWe have a second order derivative term1\n2tr\u0010\n\u03c3(x, t)\u03c3(x, t)\u22a4\u22022\n\u2202x2u(x, t)\u0011\nwith off-diagonal term. In this case, no randomization is needed since\nall the terms can be computed with just one pushforward. When input dimension dis high,\nrandomization via STDE will provide significant speed up. For example, the classical method of Hutchinson trace estimator (HTE) [ 16] can\nbe implemented in the STDE framework as the pushforward of isotropic dense random jets, i.e. We generalize the dense construction to arbitrary second-order differential operators using a\nmultivariate Gaussian distribution with the eigenvalues of the corresponding coefficient tensor as\nits covariance. Suppose Dis a second-order differential operator with coefficient tensor C. With\nthe eigendecomposition C\u2032\u2032=1\n2(C+C\u22a4) +\u03bbI=U\u03a3U\u22a4where \u2212\u03bbis smaller than the smallest\neigenvalue of C, we can construct a STDE for D:\nEv\u223cN(0,\u03a3)[\u22022u(a)(Uv,0)]\u2212\u03bbEv\u223cN(0,I)[\u22022u(a)(v,0)] =D2\nu(a)\u00b7[C\u2032\u2032\u2212\u03bbI] =D2\nu(a)\u00b7C. The main differences between the sparse and the dense version of STDE are:\n1.sparse STDE is universally application whereas the dense STDE can only be applied to\ncertain operators;\n2. the source of variance is different (see Appendix K.3). In general, we would suggest to\nuse sparse STDE unless it is known a priori that the sparse version would suffer from excess variance\nand the dense STDE is applicable. 8Amortized PINNs PINN training can be amortized by replacing the differential part of the operator\nLwith a stochastic estimator like SDGD and STDE. With amortization, we minimize\nthe following loss\n\u02dc\u2113residual (\u03b8;{x(i)}Nr\ni=1, J, K ) =1\nNrNrX\ni=1h\n\u02dcLJu\u03b8(x(i))\u2212f(x(i))i\n\u00b7h\n\u02dcLKu\u03b8(x(i))\u2212f(x(i))i\n,(25)\nwhich is a modification of Eq. Its gradient\u2202\u02dc\u2113residual\n\u2202\u03b8is then an unbiased estimator to the gradient of\nthe original PINN residual loss, i.e. OOM indicates that the memory requirement\nexceeds 40GBs. We have included Forward Laplacian which is an exact method. It is expected to perform\nbetter in terms of L2 error. However, as we can see in Table 3, the L2 error is of the same order, at\nleast in the case where the dimension is more than 1000 . We reimplement it in JAX (see Appendix A.1). This can be parallelized (denoted as \u201cParallelized SDGD via HVP\u201d,\ndetails in Appendix A.2). Parallelization provides \u223c15\u00d7speed up and reduction in peak memory for\nthe JIT compilation phase. But\nsince it is not a randomized method, the scaling is much worse. Its computation complexity is O(d),\nwhereas a randomized estimator like STDE has a computation complexity of O(|J|). Naturally, with\na high enough input dimension d, the difference in the constant prefactor is trumped by scaling. When\nthe dimension is larger than 1000, it becomes worse than even parallelized stacked backward mode\nSDGD. STDE Compared to the best realization of baseline method SDGD, the parallelized stacked back-\nward mode AD, STDE provides up to 10 \u00d7speed up and memory reduction of at least 4 \u00d7. STDE is\nalso more applicable than deep ritz method [ 40], weak adversarial network (WAN) [ 42], backward\nSDE-based solvers [ 3,33,10], deep Galerkin method [ 34], and the recently proposed forward\nLaplacian [ 23], which are all restricted to specific forms of second-order PDEs. Limitations Being a general method, STDE forgoes the optimization possibilities that apply to\nspecific operators. Furthermore, the method is not suited for computing the\nhigh order derivative of neural network parameter as explained in Section 3. Gradients\nwithout Backpropagation, February 2022. arXiv:2202.08587 [cs, stat]. [3]Christian Beck, Sebastian Becker, Patrick Cheridito, Arnulf Jentzen, and Ariel Neufeld. Deep\nsplitting method for parabolic PDEs. [4]Sebastian Becker, Ramon Braunwarth, Martin Hutzenthaler, Arnulf Jentzen, and Philippe von\nWurstemberger. Taylor-mode automatic differen-\ntiation for higher-order derivatives in JAX. [7]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. [12] Zheyuan Hu, Zekun Shi, George Em Karniadakis, and Kenji Kawaguchi. Hutchinson Trace Es-\ntimation for High-Dimensional and High-Order Physics-Informed Neural Networks. Computer\nMethods in Applied Mechanics and Engineering , 424:116883, May 2024. arXiv:2312.14499\n[cs, math, stat]. [13] Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, and Kenji Kawaguchi. [14] Zheyuan Hu, Zhouhao Yang, Yezhen Wang, George Em Karniadakis, and Kenji Kawaguchi. [15] Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, and Kenji Kawaguchi. [16] M.F. 11[17] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, Tuan Anh Nguyen, and Philippe von\nWurstemberger. [19] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu\nYang. [21] Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano\nErmon. Regularizing score-based models with score fokker-planck equations. In NeurIPS 2022\nWorkshop on Score-Based Methods , 2022. [22] Jacob Laurel, Rem Yang, Shubham Ugare, Robert Nagel, Gagandeep Singh, and Sasa Misailovic. ACM Program. [24] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred Hero, and Pramod K.\nVarshney. [25] Lu Lu, Rapha\u00ebl Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G.\nJohnson. [27] Per-Gunnar Martinsson and Joel Tropp. [28] Riley Murray, James Demmel, Michael W. Mahoney, N. Benjamin Erichson, Maksim Mel-\nnichenko, Osman Asif Malik, Laura Grigori, Piotr Luszczek, Micha\u0142 Derezi \u00b4nski, Miles E. Lopes,\nTianyu Liang, Hengrui Luo, and Jack Dongarra. [29] Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P. Adams. Effi-\ncient Learning of Generative Models via Finite-Difference Score Matching, November 2020.\narXiv:2007.03317 [cs, stat]. Modern analysis of hutchinson\u2019s trace estimator. Sliced Score Matching: A Scalable\nApproach to Density and Score Estimation, June 2019. arXiv:1905.07088 [cs, stat]. Score-Based Generative Modeling through Stochastic Differential Equations,\nFebruary 2021. arXiv:2011.13456 [cs, stat]. Estimation of the Mean of a Multivariate Normal Distribution. High Order Reverse Mode of Automatic Differentiation . The deep ritz method: a deep learning-based numerical algorithm for\nsolving variational problems. [41] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. For example, given a function f\nrepresenting the MLP PINN, the computation of SDGD for the Laplacian operator can be implemented\nin PyTorch as follows:\nf_x = torch.autograd.grad(f.sum(), x, create_graph=True)[0]\nidx_set = np.random.choice(dim, sdgd_batch_size, replace=False)\nhess_diag_val = 0.\nfor i in idx_set:\nhess_diag_i = torch.autograd.grad(\nf_x[:, i].sum(), x, create_graph=True)[0][:, i]\nhess_diag_val += hess_diag_i.detach() * dim / sdgd_batch_size\nAfter computing the PDE differential operator, it is plugged into the residual loss, and then backward-\nmode AD is employed to produce the gradient for optimization concerning \u03b8.\nA.2 JAX implementation of SDGD Parallelization via HVP\ndef hvp(f, x, v):\n\"\"\"stacked backward-mode Hessian-vector product\"\"\"\nreturn jax.grad(lambda x: jnp.vdot(jax.grad(f)(x), v))(x)\nf_hess_diag_fn = lambda i: hvp(f_partial, x_i, jnp.eye(dim)[i])[i]\nidx_set = jax.random.choice(\nkey, dim, shape=(sdgd_batch_size,), replace=False\n)\nhess_diag_val = jax.vmap(f_hess_diag_fn)(idx_set)\nA.3 JAX implementation of Forward-over-backward AD\nThe forward-over-backward AD In JAX mentioned in Appendix C can be implemented as fol-\nlows:\nf_grad_fn = jax.grad(f)\nf_x, f_hess_fn = jax.linearize(f_grad_fn, x_i) # jvp over vjp\nf_hess_diag_fn = lambda i: f_hess_fn(jnp.eye(dim)[i])[i]\nhess_diag_val = jax.vmap(f_hess_diag_fn)(idx_set)\nA.4 JAX implementation of STDE for the Laplacian operator\nidx_set = jax.random.choice(\nkey, dim, shape=(batch_size,), replace=False\n)\nrand_jet = jax.vmap(lambda i: jnp.eye(dim)[i])(idx_set)\npushfwd_2_fn = lambda v: jet.jet(\nfun=fn, primals=(x,), series=((v, jnp.zeros(dim)),)\n)# pushforward of the 2-jet (x, v, 0), i.e. \\dd^2 f(x, v, 0)\nf_vals, (_, vhv) = jax.vmap(pushfwd_2_fn)(rand_jet)\nhess_diag_val = dim / batch_size * vhv\nThejet.jet function from JAX implements the high-order pushforward dnof jets in Eq. It\ndecomposes the input function into primitives, which have analytical derivatives derived up to\narbitrary order, and uses the generalized chain rule (see section D.2) to compose the primitives into\nthe pushforward of jets. Note that in the API of jet.jet , all the high-order tangents of the input jet\nare specified via the series argument. Nodes represent (intermediate) values,\nand arrows represent computation. Input nodes are colored blue; output nodes are colored green, and\nintermediate nodes are colored yellow. At any point a\u2208Rn, the Fr\u00e9chet derivative \u2202fcan be seen as the\ndirectional derivative off:\n\u2202f:Rn\u2192L(Rn,Rm), \u2202f (a)(v) =Jf(a)v (29)\nwhere Jf(a)\u2208Rm\u00d7ndenote the Jacobian of fat point acalled the primal , andv\u2208Rn, also called\nthetangent is a vector representing the direction. activation in neural networks) has diagonal Jacobian, and its\nJVP can be efficiently implemented as a Hadamard product. One can easily verify that the adjoint is the transpose:tu(y\u22a4) =y\u22a4A. Note that the adjoint is\ntaken to vonly where ais kept fixed. C Why mixed mode AD schemes like the forward-over-backward might not\nbe better than stacked backward mode AD in the case of PINN\nIn AD literature [ 9], the second order derivative is recommended to be computed via forward-over-\nbackward AD, i.e., first do a backward mode AD to get the first order derivative, then apply forward\nmode AD to the first order derivative to obtain the second order derivative. Usually, we will expect\nthat forward-over-backward AD gives better performance in memory usage over stacked backward\nAD since the outer differential operator has to differentiate a larger computation graph than the\ninner one, and forward AD has less overhead as explained in section B.2. Essentially, forward-over-\nbackward reverses the arrows in the third row in Fig. 1, therefore reducing the number of sequential\ncomputations and also the size of the evaluation trace. So, computing the second-order\ndifferential operator here with forward-over-backward AD might not yield any advantage. We can now write the\nsecond-order truncated Taylor series with it\nf(a+ \u2206x) =f(a) +\u2202f(a)(\u2206x) +\u22022f(a)(\u2206x,\u2206x) +O\u0000\n\u2206x3\u0001\n. (34)\nFor the more general case, we have\n\u2202kf:Rn\u2192L kO\nRn,Rm! (36)\nD.2 Composition rule for high-order Fr\u00e9chet derivatives\nNext, we derive the higher-order composition rule by repeatedly applying the usual chain rule. Note that in the multivariate version of the Faa di Bruno\u2019s formula, it is possible to take a\nderivative to distinguishable variables, but here we just present the version with indistinguishable\ninput variables. (43)\nwhich is written in the perspective of input primal aand tangents v(i). (44)\nF Evaluating arbitrary mixed partial derivatives\nF.1 A concrete example\nLet\u2019s first consider a concrete case. Suppose the domain is D-dimensional we want to compute the\nmixed derivative\u2202\n\u2202x2\ni\u2202xj. (46)\nwhich contains the contraction of D3\nfthat we want:\nD3\nf(a)l,m1,m2,m3v(2)\nm1v(1)\nm2v(1)\nm3. We can remove them by doing two extract pushforwards. We can\ncompute the desired mixed partial derivative with the following pushforward of standard basis:\n\u2202\n\u2202x2\ni\u2202xju\u03b8(x) = [\u22024u\u03b8(x)(ei,ej,0,0)\u2212\u22024u\u03b8(x)(ei,0,0,0)\u22123\u22022u\u03b8(x)(ej,0)]/6. With this we only need to remove one\nterm:\n\u2202\n\u2202x2\ni\u2202xju\u03b8(x) = [\u22025u\u03b8(x)(ei,0,ej,0,0)\u2212\u22025u\u03b8(x)(ei,0,0,0,0)]/10. (50)\nSimilarly, by going to the seventh-order tangent, we can compute this mixed derivative with only one\npushforward. (52)\n18F.2 Procedure for finding the right pushforwards for arbitrary mixed partial\nderivatives\nMore generally, consider the case where we need to compute arbitrary mixed partial derivative\n\u2202PT\njqij\n\u2202xqi1\ni1. To compute it with k-jet pushforward, one needs to find:\n1. a derivative order k\u2208N,\n2.a sparsity pattern for the tangents v(i)of the input jet, which is defined as the tuple of T\nintegers J= (j1, . , j T)where v(j)=0when j\u0338\u2208Jandjt< kfor all t\u2208[1, T],\nsuch that when setting\npj=\u001a\n0, j\u0338\u2208J\nqit, j=jt, (54)\n(p1, p2, . Let\u2019s use the concrete example\u2202\n\u2202x2\ni\u2202xjagain. We\ndemonstrated that this can be computed with one 7-jet pushforward, which is equivalent to setting\nJ= (2,3),k= 2j1+j2= 7, and the partition (0,2,1,0,0,0,0). 43) ensures that the pushforward of the kth order tangent contains a contraction that can be used to\ncompute the desired mixed partial derivative. Furthermore, if there are no other partitions with a sparsity pattern that is the subset of the sparsity\npattern of the partition in consideration, there are no extra terms to remove. Therefore the pushforward contains extra\nterms that can be removed with another pushforward. This\nis equivalent to say, 2 + 2 + 3 is the only way to sum up to 7when you can only use 2and3, which\ncan be verified easily. With this setup, it is clear why the diagonal terms can always be computed with pushforward of the\nlowest possible order: (k,0, . , 0)\u2208Nkis always a valid partition k, and no other partition has\nsparsity pattern that is a subset of it. For mixed partial derivatives, the difficulty scales the total order of the operatorPT\nt=1qit, and T\nwhich can be interpreted as the degree of the \u201coff-diagonalness\u201d of the operator. To avoid overlapping with the diagonal sparsity pattern (k,0, . , 0)and to keep the\norder of derivative low, one might try k= 16 and the partition (0,3,2,1,0, . However,\nwith higher k, there is more chance that other partitions will have a subset sparsity pattern. One will need to either find all the partitions with\nsubset sparsity pattern and remove them with multiple pushforward, or further increase the derivative\norder to find a pattern with no extra term. To address this issue, convolutional networks are often employed in deep learning for images to\nreduce the number of model parameters. Here, we adopt a similar approach to mitigate the memory\ncost of model parameters in high-dimensional PDEs, called weight sharing in the first layer. 19Denote the input dimension as d, which is potentially excessively high, and the hidden dimension\nof the MLP as h, and assume that d\u226bh. The first layer weight is an d\u00d7hdimensional matrix,\nwhereas all subsequent layers have a weight matrix with a size of only h\u00d7h. By introducing a weight-sharing scheme, one can reduce the redundancy in the parameters in the first\nlayer. Specifically, we perform an additional 1D convolution to the input vectors xibefore passing\nthe input into the MLP PINN, as in Fig. The 1D convolution has filter size Bthat divides Dand\nstride size B, so the convolution output is non-overlapping, and the number of channels is set to\n1. This weight-sharing scheme reduces the parameters by approximately1\nB. The number of parameters\nin the filters is B\u00d71, and the subsequent fully connected layer will have a weight matrix of size\nd\nB\u00d7H. More concretely, suppose d= 106, h= 100 where one million ( 106)\ndimensional problems are also tested experimentally, so the number of parameters in the first layer is\nd\u00d7h= 100 \u00d7106. If we use a block size of B= 100 , we will reduce the number of parameters\ntod\nB\u00d7h+B= 106+ 100 . If the block size is B= 10 , the number of parameters will be\nd\nB\u00d7h+B= 10\u00d7106+ 10 . In other words, with a larger block size of B, we significantly reduce\nthe number of model parameters. To get an accurate reading of memory usage, we use a separate run where GPU\nmemory pre-allocation for JAX is disabled through setting the environment variable\nXLA_PYTHON_CLIENT_ALLOCATOR=platform , and the test data set is stored on the CPU memory. All the experiments were done on a single NVIDIA A100 GPU with 40GB memory and CUDA 12.2.\nwith driver 535.129.03 and JAX version 0.4.23. \u2022The network is trained with Adam [ 20] for 10K steps, with an initial learning rate of 1e-3 that\nlinearly decays to 0in 10K steps, where at each step we calculate the model parameters gradient\nwith 100 uniformly sampled random residual points. \u2022The model is evaluated using 20K uniformly sampled random points fixed throughout the training. The total\nloss is\n\u2113residual (\u03b8;{xr,i}Nr\ni=1) + 20 \u2113boundary (\u03b8;{xb,i}Nb\ni=1). (57)\n\u2022Instead of discretizing the time and sample residual points using the underlying stochastic\nprocess, we uniformly sample the time steps between the initial and the terminal time, i.e. t\u223c\nuniform [0, T], and then sample xdirectly from the distribution of Xt, i.e.x\u223c N(0,(T\u2212t)\u00b7Id\u00d7d). To match the original training setting of 100SDE trajectories with 0.015step size for time\ndiscretization, we use a batch size of 2000 for residual points and 100for boundary/initial points. \u2022We use a 4-layer multi-layer perceptron (MLP) with 1024 hidden units activated by Tanh. The\nnetwork is trained with Adam [ 20] for 10K steps, with an initial learning rate of 1e-3 that\nexponentially decays with exponent 0.9995 . \u2022To test the quality of the PINN solution, we measure the relative L1 error at the point (xtest, T)\nagainst the reference value computed via multilevel Picard\u2019s method [3, 4, 17]. The zero boundary condition ensures\nthat no information about the exact solution is leaked through the boundary condition. The for-loop\nover SDGD dimension is implemented using jax.lax.scan . Table 3 shows that, even with the\noriginal stacked backward mode AD, the speed of JAX implementation can be more than 10 \u00d7faster\nwhen the dimension is high. The difference could come from the\nfact that JAX uses XLA to perform Just-in-time (JIT) compilation of the Python code into optimized\nkernels. This is because performing\nJIT compilation requires extra memory, and the peak memory requirement during JIT compilation is\nhigher than that during training. However, theoretically lowering the randomization batch size leads to higher\nvariance. We aim to approximate the solution\u2019s true value at one test point xtest\u2208Rd,\nat the terminal time t=T, i.e. I.3 Weight sharing\nWe tested the weight-sharing technique mentioned in Section G.\nIn this section, we evaluate the performance of the weight-sharing scheme described in Appendix\nG. We tested the best-performing method from Table 3 (STDE with small randomization batch size\nof16) with different weight-sharing block sizes, on the inseparable Allen-Cahn equation with the\ntwo-body exact solution. Method Metric 100 D 1K D 10K D 100K D 1M D\nBackward\nmode SDGD\n(PyTorch) [13]Speed 55.56it/s 3.70it/s 1.85it/s 0.23it/s OOM\nMemory 1328MB 1788MB 4527MB 32777MB OOM\nError 7.187E-03 5.615E-04 1.864E-03 2.178E-03 OOM\nBackward\nmode SDGD\n(JAX)Speed 40.63it/s 37.04it/s 29.85it/s OOM OOM\nMemory 553MB 565MB 1217MB OOM OOM\nError3.51E-03\n\u00b18.47E-057.29E-04\n\u00b15.45E-063.46E-03\n\u00b12.01E-04 OOM OOM\nParallelized backward\nmode SDGDSpeed 1376.84it/s 845.21it/s 216.83it/s 29.24it/s OOM\nMemory 539MB 579MB 1177MB 4931MB OOM\nError6.87E-03\n\u00b16.97E-053.12E-03\n\u00b17.04E-042.59E-03\n\u00b12.20E-051.60E-03\n\u00b11.13E-05 OOM\nForward-over\n-Backward SDGDSpeed 778.18it/s 560.91it/s 193.91it/s 27.18it/s OOM\nMemory 537MB 579MB 1519MB 4929MB OOM\nError4.07E-03\n\u00b17.42E-052.19E-03\n\u00b12.03E-045.47E-04\n\u00b17.48E-054.21E-03\n\u00b12.53E-04 OOM\nForward\nLaplacian [23]Speed 1974.50it/s 373.73it/s 32.15it/s OOM OOM\nMemory 507MB 913MB 5505MB OOM OOM\nError4.33E-03\n\u00b14.97E-055.50E-04\n\u00b14.60E-055.58E-03\n\u00b12.73E-04 OOM OOM\nSTDESpeed 1035.09it/s 1054.39it/s 454.16it/s 156.90it/s 13.61it/s\nMemory 543MB 537MB 795MB 1073MB 6235MB\nError1.03E-02\n\u00b17.69E-056.21E-04\n\u00b12.22E-043.45E-03\n\u00b11.17E-052.59E-03\n\u00b17.93E-061.38E-03\n\u00b13.34E-05\nSTDE\n(batch size= 16)Speed 1833.78it/s 1559.36it/s 587.60it/s 283.33it/s 21.34it/s\nMemory 457MB 481MB 741MB 1063MB 6295MB\nError1.89E-02\n\u00b12.37E-047.07E-04\n\u00b11.02E-058.33E-04\n\u00b12.96E-041.50E-03\n\u00b11.02E-053.99E-03\n\u00b13.41E-05\nTable 4: Computational results for the Inseparable Poisson equation with two-body exact solution. To get better convergence, we increase the randomization batch size to 50for the 1M case. This\nincurs no extra memory cost and is only slightly slower than the original setting (speed is 46.80it/s\nwhen randomization batch size is 16). Method Metric 10 D 100 D 1K D 10K D\nBackward\nmode SDGD\n(PyTorch) [13]Speed - - - -\nMemory - - - -\nError 1.052E-03 5.263E-04 6.910E-03 1.598E-03\nBackwardBackward\nmode SDGD\n(JAX)Speed 211.63it/s 207.66it/s 188.31it/s 93.21it/s\nMemory 619MB 621MB 655MB 1371MB\nError8.55E-05\n\u00b16.75E-054.02E-04\n\u00b12.07E-043.81E-04\n\u00b14.43E-042.60E-03\n\u00b11.38E-03\nSTDESpeed 660.82it/s 635.16it/s 599.15it/s 361.11it/s\nMemory 625MB 625MB 657MB 971MB\nError6.99E-05\n\u00b15.78E-053.69E-04\n\u00b12.19E-043.38E-04\n\u00b13.30E-046.08E-03\n\u00b17.47E-03\n24Table 8: Computational results for the Time-dependent Allen-Cahn equation, where the number of\nSDGD sampled dimensions is set to 10. Method Metric 10 D 100 D 1K D 10K D\nBackwardBackward\nmode SDGD\n(PyTorch) [13]Speed - - - -\nMemory - - - -\nError 7.815E-04 3.142E-04 7.042E-04 2.477E-04\nBackwardBackward\nmode SDGD\n(JAX)Speed 210.83it/s 207.44it/s 187.98it/s 93.17it/s\nMemory 619MB 621MB 655MB 1371MB\nError5.39E-05\n\u00b14.10E-059.15E-05\n\u00b16.06E-054.19E-04\n\u00b12.18E-043.74E-02\n\u00b14.15E-02\nSTDESpeed 629.04it/s 608.83it/s 596.12it/s 365.09it/s\nMemory 525MB 539MB 655MB 971MB\nError4.15E-05\n\u00b13.21E-052.54E-04\n\u00b11.76E-044.05E-03\n\u00b11.44E-021.66E-02\n\u00b15.95E-03\nFrom Table 10, we can see that weight sharing drastically reduces the number of network parameters\nand memory usage. With B= 50 , there is a 2.5x reduction in memory and there is no performance\nloss in terms of L2 relative error. For the 1M case, increasing Bto1000 affects the\nconvergence quality, as the L2 relative error goes up by 100x. I.4 High-order PDEs\nHere we demonstrate how to use STDE to calculate mixed partial derivatives in some actual PDE. Since these equations are low-dimensional we do not\nneed to sample over the space dimension. (64)\ncan alternatively be computed with the pushforward of the following jets\nJ(1)= d9u(x,0,ex,ey,0, . (68)\nAll the derivative terms can be computed with a 5-jet, 4-jet, and a 2-jet pushforward. Let\nJ(1):=d5u(x,0,et,ex,0,0)\nJ(2):=d4u(x,ex,0,0,0)\nJ(3):=d2u(x,ey,0). (69)\nThen all required derivative terms can be evaluated as follows. This increases the accuracy of the solution. (72)\nThe total loss becomes\n\u2113residual +cgPINN \u2113gPINN (73)\nwhere cgPINN is the g-PINN penalty weight. To perform gradient-enhancement we need to compute\nthe gradient of the residual:\nR(x, t) :=ut+uux+\u03b1uxxx,\n\u2207R(x, t) = [utt+utux+uutx+\u03b1utxxx, u tx+uxux+uuxx+\u03b1uxxxx]. (74)\nAll the derivative terms can be computed with one 2-jet and two 7-jet pushforward. (75)\nThen all required derivative terms can be evaluated as follows. (77)\n27With STDE randomization, we randomized the second order term\u22022\n\u2202x2\niwith index isampled from\n[1, d]. We implement the baseline\nmethod based on the best performing first-order AD scheme, the parallelized backward mode SDGD,\nwhich we denoted as JVP-HVP in the table. Specifically, to compute the residual gradient we apply\none more JVP to the HVP-based implementation of Laplacian (Appendix A.2). Furthermore, through amortizing\nwe can apply gPINN to high-dimensional PDE which was intractable. J Pushing forward dense random jets\nIn this section we establish the connection between the classical technique of HTE [ 16] and STDE by\ndemonstrating that HTE is a pushforward of dense isotropic random 2-jet. Therefore, the trace can be estimated by Monte\nCarlo:\ntr(A)\u22481\nVVX\ni=1vT\niAvi, (84)\nwhere each vi\u2208Rdarei.i.d. Among isotropic distributions, the Rademacher distribution minimizes the\nvariance of HTE. The proof for the minimal variance is given in [35]. Equation gPINN method Metric 100 D 1K D 10K D 100K D\nAllen-\nCahnJVP-HVPSpeed 256.75it/s 249.48it/s 108.80it/s 61.04it/s\nError3.97E-02\n\u00b13.98E-041.02E-03\n\u00b16.89E-053.08E-04\n\u00b17.48E-061.39E-03\n\u00b11.42E-05\nSTDESpeed 366.46it/s 324.60it/s 207.85it/s 155.40it/s\nError4.34E-02\n\u00b13.72E-045.26E-04\n\u00b12.26E-051.25E-03\n\u00b14.07E-057.61E-04\n\u00b11.03E-04\nNoneError4.98E-02\n\u00b13.82E-046.32E-03\n\u00b14.43E-051.19E-04\n\u00b11.04E-055.43E-04\n\u00b14.30E-06\nSine-\nGordonJVP-HVPSpeed 1008.65it/s 788.10it/s 413.32it/s 107.68it/s\nError1.85E-03\n\u00b14.61E-051.02E-03\n\u00b16.89E-051.79E-04\n\u00b11.06E-055.76E-04\n\u00b11.37E-04\nSTDESpeed 1165.35it/s 948.99it/s 542.36it/s 210.75it/s\nError6.69E-03\n\u00b11.48E-041.12E-03\n\u00b11.38E-051.76E-04\n\u00b15.31E-061.55E-03\n\u00b14.30E-05\nNoneError4.74E-03\n\u00b16.68E-057.02E-04\n\u00b11.69E-051.31E-04\n\u00b11.22E-058.07E-04\n\u00b14.01E-06\nJ.2 HTE as the pushforward of dense isotropic random 2-jets\nNote that both HTE and the STDE Hessian trace estimator (Eq. ) are computing the quadratic form\nof Hessian, a specific contraction that is included in the pushforward of 2-jet. Therefore its STDE estimator is\n\u02dc\u22062Nu(x) =d\n3NNX\nj=1\u22024u(x)(vj,0,0,0),vj\u223c N(0,I) (88)\nK STDE with dense jets\nK.1 STDE with second order dense jets as generalization of HTE\nSuppose Dis a second-order differential operator with coefficient tensor C. IfCis not symmetric,\nwe can symmetrize it as C\u2032=1\n2(C+C\u22a4), andD2\nu(a)\u00b7C=D2\nu(a)\u00b7C\u2032since D2\nu(a)is symmetric. 29Furthermore, we can make Cpositive-definite by adding a constant diagonal \u03bbIwhere \u2212\u03bbis\nsmaller than the smallest eigenvalue of C. The matrix C\u2032\u2032=1\n2(C+C\u22a4) +\u03bbIthen has the eigen\ndecomposition U\u03a3U\u22a4where \u03a3is diagonal and all positive. The mask tensor of Lis the rank-4 identity tensor I4\u2208Rd\u00d7d\u00d7d\u00d7d, so the condition for unbiasedness\nis\nEv\u223cp[v(a)\niv(b)\njv(c)\nkv(d)\nl] =Mijkl=\u03b4ijkl, a, b, c, d \u2208 {1,2,3,4} (90)\nwhere \u03b4ijkl= 1when i=j=k=l, and is 0otherwise. In the most general case where a\u0338=b\u0338=c\u0338=d, we can sample v\u2208R4dand split it into four Rd\nvectors. In this case we can define blocks of covariance as Ev\u223cp[v(a)v(b)] =\u03a3ab, and\u03a3=\u0002\n\u03a3ab\u0003\nab. Firstly, consider the diagonal entries of \u03c3:\n\u03c3ii=\u00b5aaaa\niiii= 3(\u03a3aa\nii)2, a\u2208 {1,2} (93)\nThis can always be satisfied by setting the diagonal entries of both \u03a3aaand\u03a3aablock as fol-\nlows:\n\u03a3aa\nii=p\n\u03c3ii/3, a\u2208 {1,2} (94)\nNext, consider the entire \u03c3matrix. Model converges when the difference of L2\nerror is below 1e-7. Answer: [Yes]\nJustification: Our claims are backed by both theoretical and experimental evidence. A No or\nNA answer to this question will not be perceived well by the reviewers. \u2022It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper. 2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors? Guidelines:\n\u2022The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be. \u2022The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. \u2022The authors should reflect on the factors that influence the performance of the approach. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon. \u2022The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size. \u2022If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness. \u2022While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations. Answer: [Yes]\nJustification: Most of our theoretical results are asymptotic analyses on the computation\ncomplexity, and we have clearly stated the assumption we have made. \u2022All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced. \u2022All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition. \u2022Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material. Answer: [Yes]\nJustification: We have included sample implementations of key steps of our method in the\nfirst section in the Appendix A. \u2022If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not. \u2022Depending on the contribution, reproducibility can be accomplished in various ways. \u2022While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. (c)If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset). In the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results. 5.Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial? Answer: [Yes]\nJustification: We will open-source our code later. \u2022Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy ) for more details. \u2022While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark). See the NeurIPS code and data submission guidelines ( https:\n//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. \u2022The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why. \u2022At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable). \u2022The full details can be provided either with the code, in appendix, or as supplemental\nmaterial. \u2022The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.) \u2022 The assumptions made should be given (e.g., Normally distributed errors). \u2022It should be clear whether the error bar is the standard deviation or the standard error\nof the mean. \u2022It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified. \u2022For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. \u2022The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage. \u2022The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute. \u2022The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper). Answer: [Yes]\nJustification: We have read the NeurIPS Code of Ethics and made sure that the paper\nconforms to it. \u2022The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction). 10.Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed? Guidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed. \u2022If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact. \u2022The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. \u2022The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology. 11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)? 36Guidelines:\n\u2022 The answer NA means that the paper poses no such risks. \u2022Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images. 12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected? Guidelines:\n\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. \u2022For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided. \u2022If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. \u2022For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided. \u2022If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators. 13.New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets? This includes details about training, license,\nlimitations, etc. \u2022The paper should discuss whether and how consent was obtained from people whose\nasset is used. 37\u2022At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file. 14.Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)? \u2022According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector. \u2022Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. \u2022We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution. \u2022For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.",
        "Results and Findings": "In this work, we show how to efficiently perform arbitrary contraction of\nthe derivative tensor of arbitrary order for multivariate functions, by properly\nconstructing the input tangents to univariate high-order AD, which can be used to\nefficiently randomize any differential operator. When applied to Physics-Informed\nNeural Networks (PINNs), our method provides >1000 \u00d7speed-up and >30 \u00d7\nmemory reduction over randomization with first-order AD, and we can now solve\n1-million-dimensional PDEs in 8 minutes on a single NVIDIA A100 GPU1. . . ,D\u03b1(n)u\u03b8(x)), u \u03b8:Rd\u2192Rd\u2032. . . Specifically, we show that, with properly constructed input\ntangents, the univariate Taylor mode can be used to contract multivariate functions\u2019 derivative\ntensor of arbitrary order;\n\u2022We provide a comprehensive procedure for randomizing arbitrary differential operators with\nSTDE, while previous works mainly focus on the Laplacian operator, and we provide abundant\nexamples of STDE constructed for operators in common PDEs;\n\u2022STDE encompass and generalizes previous methods like SDGD [ 13] and HTE [ 16,12]. We also\nprove that HTE-type estimator cannot be generalized beyond fourth order differential operator;\n\u2022We determine the efficacy of STDE experimentally. In this section, we will\nconsider the neural networks with linear computation graphs like F=FL\u25e6FL\u22121\u25e6 \u00b7\u00b7\u00b7 \u25e6 F1, but\n2the results generalize to arbitrary directed acyclic graphs (DAGs). 3.2 Inefficiency of the first-order AD for high-order derivative on inputs\nsecond-order derivative by applying VJP twiceVJPForward\nx\u2208Rd y1\u2208Rh y2 y3\u03b81 \u03b82 \u03b83\nv\u22a4\n1\u2208Rhv\u22a4\n2 v\u22a4\n3v\u22a4\u2208Rd\u2032\nv\u22a4\u2202F\n\u2202xy=F(x) \u03b84\n\u02dcv\u22a4\u2208Rd \u02dcv\u22a4\n1 \u02dcv\u22a4\n2 \u02dcv\u22a4\n3 \u02dcv\u22a4\n4 \u02dcv\u22a4\n5 \u02dcv\u22a4\n6\u22022Fd\n\u2202xi\u2202xjvd\u02dcvj\nFigure 1: The computation graph of computing second order gradient by repeated application of\nbackward mode AD, for a function F(\u00b7)with4primitives ( L= 4), which computes the Hessian-\nvector-product. Furthermore, the size of the cotangent also grows by d\ntimes. . . . . ,vk)\nknown as the k-jet of gwhere vjis called the jth order tangent of g.Jk\nglives in the kth order tangent\nbundle of F, and we can define the kth-order pushforward dkF:\ndkF(Jk\ng(t)) =Jk\nF\u25e6g(t) =\u0012\n[F\u25e6g](t),\u2202\n\u2202t[F\u25e6g](t),\u22022\n\u2202t2[F\u25e6g](t), . . . ,\u2202k\n\u2202tk[F\u25e6g](t)\u0013\n=(F(a), \u2202F(a)(v(1)), \u22022F(a)(v(1),v(2)), . . . . . 43, one can always find a Jl\ngwith large enough l\u2265ksuch that there exists k\u2264l\u2032\u2264lwith\n\u2202l\u2032F(Jl\u2032\ng) =Dk\nF(a)\u00b7 \u2297k\ni=1v(vi)where vi\u2208[1, k], by setting some tangents v(vi)to the zero vector. x y1 y2 y3 y=F(x)d2F1 d2F2 d2F3 d2F4\nv(1)v(1)\n1 v(1)\n2 v(1)\n3\u2202F=\u2202F\n\u2202xv(1)\nv(2)v(2)\n1 v(2)\n2 v(2)\n3\u22022F=\u2202F\n\u2202xv(2)+\u22022F\n\u2202xi\u2202xjv(1)\niv(1)\njL= 4k= 2\nFigure 2: The computation graph of d2FforFwith4primitives. |\u03b1|=k\u2208Nfor all \u03b1. For scalar u:Rd\u2192R,\n5we can identify a kth order differential operator Lwith the following tensor dot product\nLu(a) =P\n\u03b1\u2208I(L)C\u03b1D\u03b1u(a) =P\nd1,...,d kDk\nu(a)d1,...,d kCd1,...,dk(L) =Dk\nu(a)\u00b7C(L),\n(10)\nwhere di\u2208[1, d], i\u2208[1, k]is the tensor index on the ith axis, , and C(L)is a tensor of the same shape\nasDk\nu(a)that equals C\u03b1when d1, . . . . . . . ,v(k)), we have\nE[Dk\nu(a)d1,...,d kv(v1)\nd1. . . . . v(vk)\ndk] =Dk\nu(a)\u00b7Eh\n\u2297k\ni=1v(vi)i\n(11)\nwhere \u2297denotes Kronecker product, v(vi)\ndi\u2208[1, k]is the didimension of the vith order tangent\nin the input k-jet. Eq. . . v(vk)\ndk] =Cd1,...,dk(L). As discussed,\none can always find a Jl\ngwith large enough l\u2265ksuch that \u2202lF(Jl\ng) =Dk\nF(a)\u00b7\u2297k\ni=1v(vi), so with a\ndistribution pover the input l-jetJl\ngthat satisfies the unbiasedness condition (Eq. 12), we have\nEJlg\u223cp[\u2202lu(Jl\ng)] =E[v(v1)\nd1. . . v(vk)\ndk] =Dk\nu(a)\u00b7C(L) =Lu(a), (13)\nwhich means Lu(a)can be approximated by the sample mean of the pushforwards of random l-jet\ndrawn from p, which can be computed efficiently via Taylor mode AD. . . 9 we know that the quadratic form of Hessian can be computed through \u22022by\nsetting v(2)=0andv(1)=ej. 43 we see\nthat setting the first-order tangent v(1)toejand all other tangents v(i)to the zero vector gives the\ndesired high-order diagonal element:\n\u02dcLJu\u03b8(a) =d\n|J|X\nj\u2208J\u2202k\n\u2202xk\nju\u03b8(a) =d\n|J|X\nj\u2208J\u2202ku\u03b8(a)(ej,0, . . . Its form is given by\n\u2202\n\u2202tu(x, t) +1\n2tr\u0010\n\u03c3\u03c3\u22a4(x, t)\u22022\n\u2202x2u(x, t)\u0011\n+\u2207u(x, t)\u00b7\u00b5(x, t) +f(t,x, u(x, t), \u03c3\u22a4(x, t)\u2207u(x, t)) = 0 . The off-diagonals can be easily removed via a change of variable:\n1\n2tr\u0012\n\u03c3(x, t)\u03c3(x, t)\u22a4\u22022\n\u2202x2u(x, t)\u0013\n=1\n2dX\ni=1\u22022u(x, t)(\u03c3(x, t)ei,0). 4.3.2 Differential operators with arbitrary mixed partial derivative\nIt is not always possible to remove the mixed partial derivatives but discussed in section 4.2, for an\narbitrary kth order derivative tensor element Dk\nu(a)n1,...,n k, we can find an appropriate l-jetJl\ng(t)\nwithg(t) =asuch that \u2202lu(Jl\ng) =Dk\nu(a)n1,...,n k. Here we show a concrete example. . . ,v(13)),v(3)=ex,v(4)=ey,v(7)=et,v(i)=0,\u2200i\u0338\u2208 {3,4,7},\nux=J[1], uy=J[2], uxx=J[4], uxy=J[5]/35,\nuyy=J[6]/35, uty=J[9]/330, uxxxy=J[11]/200200 . (a,v,0)\u223c\u03b4a\u00d7p\u00d7\u03b4withEp[vv\u22a4] =I. We prove this by providing a counterexample: one cannot\nconstruct an STDE for the fourth order operatorPd\ni=1\u22024\n\u2202x4with dense jets. For the case of\nk= 2and large d, we tested two types of PDEs: inseparable and effectively high-dimensional PDEs\n(Appendix I.1) and semilinear parabolic PDEs (Appendix I.2). In all our experiments, STDE drastically reduces computation and memory costs in training PINNs,\ncompared to the baseline method of SDGD with stacked backward-mode AD. Due to the page limit,\nthe most important results are reported here, and the full details including the experiment setup and\nhyperparameters (Appendix H) can be found in the Appendix. The approximated solution u\u03b8(x)\u2248u(x)is obtained by minimizing the mean\nsquared error (MSE) of the PDE residual R(x;\u03b8) =Lu\u03b8(x)\u2212f(x):\n\u2113residual (\u03b8;{x(i)}Nr\ni=1) =1\nNrNrX\ni=1\f\f\fLu\u03b8(x(i))\u2212f(x(i))\f\f\f2\n(24)\nwhere the residual points {x(i)}Nr\ni=1are sampled from the domain \u2126. The\nresults are in Table 1 and 2, where the best results for each dimensionality are marked in bold. Memory (MB) \u2193 100 D 1K D 10K D 100K D 1M D\nBackward mode SDGD (PyTorch) [13] 1328 1788 4527 32777 OOM\nBackward mode SDGD 553 565 1217 OOM OOM\nParallelized backward mode SDGD 539 579 1177 4931 OOM\nForward-over-Backward SDGD 537 579 1519 4929 OOM\nForward Laplacian [23] 507 913 5505 OOM OOM\nSTDE 543 537 795 1073 6235\nJAX vs PyTorch The original SDGD with stacked backward mode AD was implemented in\nPyTorch. From Table 1 and 2, JAX provides \u223c15\u00d7\nspeed-up and up to \u223c4\u00d7memory reduction. Also, we observed that lowering the randomization\nbatch size improves both speed and memory profile, but the trade-off between cheaper computation\nand larger variance needs further analysis. [2]At\u0131l\u0131m G\u00fcne\u00b8 s Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip Torr. [5]Claus Bendtsen and Ole Stauning. [6]Jesse Bettencourt, Matthew J. Johnson, and David Duvenaud. [8]Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark Crowley. [9]Andreas Griewank and Andrea Walther. [10] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential\nequations using deep learning. Proceedings of the National Academy of Sciences , 115(34):8505\u2013\n8510, Aug 2018. [11] Di He, Shanda Li, Wenlei Shi, Xiaotian Gao, Jia Zhang, Jiang Bian, Liwei Wang, and Tie-Yan\nLiu. [18] Jerzy Karczmarczuk. Functional differentiation of computer programs. In Proceedings of the\nThird ACM SIGPLAN International Conference on Functional Programming , ICFP \u201998, pages\n195\u2013203, New York, NY , USA, 1998. Nature Reviews Physics , 3(6):422\u2013440, Jun 2021. [20] Diederik P. Kingma and Jimmy Ba. , 6(OOPSLA2), oct 2022. [23] Ruichen Li, Haotian Ye, Du Jiang, Xuelan Wen, Chuwei Wang, Zhe Li, Xiang Li, Di He,\nJi Chen, Weiluo Ren, and Liwei Wang. SIAM\nJournal on Scientific Computing , 43(6):B1105\u2013B1132, 2021. [26] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen,\nand Sanjeev Arora. Randomized\nAutomatic Differentiation, March 2021. arXiv:2007.10412 [cs, stat]. [30] Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. [31] Juncai Pu and Yong Chen. [32] M. Raissi, P. Perdikaris, and G.E. [33] Maziar Raissi. 12[34] Justin Sirignano and Konstantinos Spiliopoulos. [35] Maciej Skorski. IEEE, March 2021. [36] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. [37] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,\nand Ben Poole. [38] Charles M. Stein. [40] E Weinan and Ting Yu. [42] Yaohua Zang, Gang Bao, Xiaojing Ye, and Haomin Zhou. B.3 Fr\u00e9chet derivative and linearization\nGiven normed vector spaces V, W , the Fr\u00e9chet derivative \u2202fof a function f:V\u2192Wis a map from\nVto the space of all bounded linear operators from VtoW, denoted as L(V, W ), that is\n\u2202f:V\u2192L(V, W ), (27)\nsuch that at a point a\u2208Vit gives the best linear approximation \u2202f(a)(\u00b7)off, in the sense\nthat\nlim\n\u2225h\u2225\u21920\u2225f(a+h)\u2212f(a)\u2212\u2202f(a)(h)\u2225W\n\u2225h\u2225V= 0 (28)\nTherefore, it is also called the linearization offat point a. . . ,v(k)) =X\ni1,...,ikDk\nf(a)i0,i1,...,ikv(1)\ni1. . . v(k)\nik\n(35)\nHigh-order Fr\u00e9chet derivative can be seen as the best kth order polynomial approximation of fby\ntaking all input tangents to be the same v\u2208Rn:\nf(a+ \u2206x) =f(a) +\u2202f(a)(v) +1\n2\u22022f(a)(v,v) +\u00b7\u00b7\u00b7+1\nk!\u2202kf(a)(v\u2297k) +O\u0000\n\u2206xk+1\u0001\n. )pi\u00b7(f(Pn\ni=1pi)\u25e6g)(x)\u00b7kY\nj=1\u00121\nj!g(j)(x)\u0013pj\n. . . , p k)\u2208Nkthat satisfies\nkX\ni=1i\u00b7pi=k. Then, the above chain rule can be\nsimplified to\n\u2202\n\u2202tf(g(t)) =Df(a)l,mv(1)\nm\u2208Rl\n\u22022\n\u2202t2f(g(t)) =Df(a)l,mv(2)\nm+D2\nf(a)l,m,m\u2032v(1)\nmv(1)\nm\u2032\u2208Rl\n\u22023\n\u2202t3f(g(t)) =Df(a)l,mv(3)\nm+ 3\u00b7D2\nf(a)l,m,m\u2032v(1)\nmv(2)\nm\u2032+D3\nf(a)l,m,m\u2032,m\u2032\u2032v(1)\nmv(1)\nm\u2032v(1)\nm\u2032\u2032\u2208Rl. )pi\u00b7DPk\ni=1pi\nf(a)l,m1,...,mPk\ni=1pi\u00b7kY\nj=1\u00121\nj!v(j)\nmj\u0013pj\n\u2208Rl. (47)\nHowever, there are extra terms. d7fcontains \u22027, and when all input tangents are set to zero except for v(2)andv(3),\n\u22027equals\n105\u00b7D3\nf(a)l,m1,m2,m3v(3)\nm1v(2)\nm2v(2)\nm3(51)\nwhich is the exact contraction we want. With this we have\n\u2202\n\u2202x2\ni\u2202xju\u03b8(x) =\u22027u\u03b8(x)(0,ei,ej,0,0,0,0)/105. . . . . . . (2,0,1,0,0)and\n(5,0,0,0,0)are both valid partition of k= 5, and the sparsity pattern of (5,0,0,0,0)is the subset\nof that of (2,0,1,0,0):p1are non-zero in both partition. . . . . . . . . Therefore, the total number of the first layer is reduced from d\u00d7hto onlyd\nB\u00d7h+B,\nand we can see that with a larger block size B, we will have fewer parameters, and the reduction\nfactor is approximately1\nB. x1x2x3x4x5x6x7x8x9\n\u03b81\u03b82\u03b83\ny1y2y3Convolution with stride of 3\noutput\nFigure 4: Convolutional weight sharing in the first layer, with input dimension 9and filter size 3. H Experiment setup\nEach experiment is run with five different random seeds, and the average and the standard deviations\nof these runs are reported. The GPU memory usage was obtained via NVIDIA-smi and peak memory was reported. 20\u2022 The zero boundary condition is satisfied via the following parameterization\nu\u03b8(x) = (1 \u2212 \u2225x\u22252\n2)uMLP\n\u03b8(x) (55)\nwhere uMLP\n\u03b8is the MLP network, and u\u03b8is the PDE ansatz, as described in [25]. 25:\n\u02dc\u2113residual (\u03b8;{x(i)}Nr\ni=1, J) =1\nNrNrX\ni=1h\n\u02dcLJu\u03b8(x(i))\u2212f(x(i))i\n(58)\nas the bias in practice is very small and does not affect convergence. I Experiments Results\nI.1 Inseparable and effectively high-dimensional PDEs\nThe first class of PDEs is defined via a nonlinear, inseparable, and effectively high-dimensional exact\nsolution uexact(x)defined within the d-dimensional unit ball Bd:\nLu(x) =f(x),x\u2208Bd\nu(x) =0,x\u2208\u2202Bd(59)\nwhere Lis a linear/nonlinear operator and g(x) =Luexact(x). For the exact solution, we consider the following with all ci\u223c N(0,1):\n\u2022 two-body interaction: uexact(x) = (1 \u2212 \u2225x\u22252\n2)\u0010Pd\u22121\ni=1cisin(xi+ cos( xi+1) +xi+1cos(xi))\u0011\n. 21We tested the performance of STDE on these equations, and the results are presented in Table 3, 4, 5,\n6. For the Allen-Cahn equation, we performed a detailed ablation study (Table 3), and we expect\nthese results to generalize over these second-order PDEs. Randomization batch size We also tested the case where the STDE randomization batch size\nis reduced to 16. Lu(x, t) =\u22072u(x, t) +1\u2212u(x, t)2\n1 +u(x, t)2. 22Table 3: Computational results for the Inseparable Allen-Cahn equation with the two-body exact\nsolution, where the randomization batch size is set to 100unless stated otherwise. Method Metric 100D 1K D 10K D 100K D 1M D\nBackward\nmode SDGD\n(PyTorch) [13]Speed 55.56it/s 3.70it/s 1.85it/s 0.23it/s OOM\nMemory 1328MB 1788MB 4527MB 32777MB OOM\nError 7.189E-03 5.611E-04 1.850E-03 2.175E-03 OOM\nSTDE\n(batch size= 16)Speed 2020.05it/s 1649.20it/s 584.98it/s 281.78it/s 20.38it/s\nMemory 457MB 481MB 741MB 1063MB 6295MB\nError3.50E-03\n\u00b11.44E-044.91E-04\n\u00b13.45E-054.70E-03\n\u00b12.10E-053.49E-03\n\u00b12.14E-059.18E-04\n\u00b16.39E-06\n23Table 5: Computational results for the Inseparable Sine-Gordon equation with two-body exact\nsolution. Method Metric 100 D 1K D 10K D 100K D 1M D\nBackward\nmode SDGD\n(PyTorch) [13]Speed 55.56it/s 3.70it/s 1.85it/s 0.23it/s OOM\nMemory 1328MB 1788MB 4527MB 32777MB OOM\nError 7.192E-03 5.641E-04 1.854E-03 2.177E-03 OOM\nSTDE\n(batch size= 16)Speed 1926.33it/s 1467.38it/s 566.26it/s 279.24it/s 19.88it/s\nMemory 457MB 481MB 741MB 1063MB 6295MB\nError3.64E-03\n\u00b11.46E-045.40E-04\n\u00b17.21E-055.32E-03\n\u00b15.12E-049.56E-04\n\u00b18.03E-069.47E-04\n\u00b18.30E-06\nTable 6: Computational results for the Inseparable Allen-Cahn, Poisson, and Sine-Gordon equation\nwith the three-body exact solution, computed via STDE with randomization batch size |J|set to 16. Eq. Metric 100 D 1K D 10K D 100K D 1M D\nAllen-CahnSpeed 1938.80it/s 1840.21it/s 1291.67it/s 356.76it/s 46.97it/s\nMemory 461MB 481MB 539MB 1055MB 6233MB\nError9.97E-03\n\u00b13.89E-041.43E-03\n\u00b11.60E-046.21E-04\n\u00b18.15E-051.56E-05\n\u00b13.28E-072.25E-06\n\u00b11.48E-07\nPoisson*Speed 1991.28it/s 1872.31it/s 1276.21it/s 364.04it/s 31.73it/s\nMemory 473MB 481MB 539MB 1055MB 6233MB\nError1.00E-02\n\u00b13.27E-041.02E-03\n\u00b13.67E-051.01E-04\n\u00b12.40E-079.26E-02\n\u00b15.36E-044.82E-06\n\u00b12.16E-07\nSine-GordonSpeed 1938.80it/s 1840.21it/s 1291.67it/s 356.76it/s 46.88it/s\nMemory 475MB 479MB 539MB 1063MB 6233MB\nError9.97E-03\n\u00b13.89E-041.43E-03\n\u00b11.60E-046.21E-04\n\u00b18.15E-051.56E-05\n\u00b13.28E-072.31E-05\n\u00b11.48E-06\nTable 7: Computational results for the Time-dependent Semilinear Heat equation, where the number\nof SDGD sampled dimensions is set to 10. Method Metric 10 D 100 D 1K D 10K D\nBackwardBackward\nmode SDGD\n(PyTorch) [13]Speed - - - -\nMemory - - - -\nError 7.815E-04 3.142E-04 7.042E-04 2.477E-04\nBackward\nmode SDGD\n(JAX)Speed 211.38it/s 206.42it/s 188.02it/s 93.20it/s\nMemory 619MB 621MB 657MB 1371MB\nError6.31E-02\n\u00b13.79E-024.38E-03\n\u00b12.48E-031.35E-03\n\u00b11.23E-033.97E-04\n\u00b13.03E-04\nSTDESpeed 677.51it/s 650.98it/s 598.33it/s 361.31it/s\nMemory 533MB 535MB 657MB 903MB\nError6.37E-02\n\u00b13.77E-024.38E-03\n\u00b12.47E-031.26E-03\n\u00b11.29E-033.79E-04\n\u00b12.75E-04\nTable 9: Computational results for the Time-dependent Sine-Gordon equation, where the number of\nSDGD sampled dimensions is set to 10. However, from the experiments we can see that, in both the 1M and the 5M case, increasing the\nblock size beyond 50provides diminishing returns. dim B= 1 B= 10 B= 50 B= 100 B= 500 B= 1000\n1MSpeed 21.34it/s 16.67it/s 23.14it/s 23.73it/s 25.47it/s 26.60it/s\nMemory 6295MB 4819MB 2505MB 2461MB 2409MB 2403MB\n#Param. 128,033,281 12,833,292 2,593,332 1,313,382 289,782 162,282\nError3.99E-03\n\u00b13.41E-051.86E-02\n\u00b13.13E-044.76E-03\n\u00b11.27E-041.22E-03\n\u00b16.05E-052.57E-03\n\u00b11.15E-046.06E-01\n\u00b14.17E-04\n5MSpeed OOM 3.16it/s 4.47it/s 4.74it/s 4.82it/s 4.76it/s\nMemory OOM 25023MB 10595MB 10359MB 10163MB 10143MB\n#Param. 640,033,281 64,033,292 12,833,332 6,433,382 1,313,782 674,282\nError OOM5.11E-01\n\u00b14.01E-043.13E-03\n\u00b12.34E-043.94E-03\n\u00b12.22E-041.98E-03\n\u00b15.20E-056.27E-01\n\u00b13.03E-04\nWe will demonstrate that STDE increases the speed for computing the mixed partial derivatives, as it\navoids computing the entire derivative tensor. we will write u(x, t) =u. To test the speed improvement, we run\nthe STDE implementation against repeated backward mode AD on a Nvidia A100 GPU with 40GB\nmemory. The results are reported in Table 11. From the Table we see that STDE provides around\n\u223c2\u00d7speed up compared to repeated application of backward mode AD across different network\nsizes. . . ),J(2)= d3u(x,0,ey,et),J(3)= d3u(x,0,ey,0). utx=J(1)\n[5]/10,\nux=J(2)\n[1], uxx=J(2)\n[2], uxxxx=J(2)\n[4],\nuyy=J(3)\n[2]. ux=J(1)\n[1], uxx=J(1)\n[2], uxxx=J(1)\n[3], uxxxx=J(1)\n[4], uxxxxx =J(1)\n[5],\nutxxx= (J(2)\n[7]\u2212J(1)\n[8])/35, utx= (J(2)\n[5]\u2212uxxxxx )/5, ut=J(2)\n[4]\u2212uxxxx,\nutt=J(3)\n[2]. Speed (it/s) \u2191network size Base L= 8 L= 16 h= 256 h= 512 h= 1024\n2D KdVBackward 762.86 279.19 123.20 656.01 541.10 349.23\nSTDE 1372.41 642.82 303.39 1209.30 743.75 418.13\nSTDE* 1357.64 606.43 272.01 1203.97 841.07 442.32\n2D KPBackward 766.79 278.53 123.67 642.34 525.23 340.94\nSTDE 1518.82 676.16 304.95 1498.61 1052.62 642.21\n1D g-KdVBackward 621.04 232.35 102.39 559.65 482.52 293.97\nSTDE 1307.27 593.21 253.48 1187.31 776.65 441.50\nI.4.2 Amortized gradient-enhanced PINN for high-dimensional PDEs\nIt is expensive to apply gradient enhancement for high-dimensional PDEs. (78)\nWe further have\n\u22022\n\u2202x2\niu(x) =J[4]/3, (79)\nso the STDE of the Laplacian operator can be computed together with the above pushforward. With\nthis pushforward, we can efficiently amortize the gPINN regularization loss by minimizing the\nfollowing upperbound on the original gPINN loss with randomized Laplacian\n\u02dc\u2113gPINN ({x(i)}Nr\ni=1, I, J)\n=1\nNrX\nj\u2208JX\ni\u2208I\f\f\f\f\u22023\n\u2202xj\u2202x2\niu(x) +\u2202\n\u2202xju(x)\u22123u2(x)\u2202\n\u2202xju(x)\u2212\u2202\n\u2202xjf(x)\f\f\f\f2\n\u22651\nNrX\nj\u2208J\f\f\f\f\fX\ni\u2208I\u22023\n\u2202xj\u2202x2\niu(x) +\u2202\n\u2202xju(x)\u22123u2(x)\u2202\n\u2202xju(x)\u2212\u2202\n\u2202xjf(x)\f\f\f\f\f2\n,(80)\nwhere Jis an independently sampled index set for sampling the gPINN terms. The results are reported in Table 12. From the table, we\nsee that STDE-based amortized gPINN performs better than the JVP-HVP implementation, and both\nare more efficient than applying backward mode AD in a for-loop. (85)\nwhere\u02dc\u22072Nis the STDE for Laplacian with random jet batch size N.\nJ.3 Estimating the Biharmonic operator\nIt was shown in [12] that the Biharmonic operator\n\u22062u(x) =dX\ni=1dX\nj=1\u22024\n\u2202x2\ni\u2202x2\nju(x) (86)\nhas the following unbiased estimator:\n\u22062u(x) =1\n3Ev\u223cp(v)\u0002\n\u22024u(x)(v,0,0,0)\u0003\n(87)\nwhere pis the d-dimensional normal distribution. (89)\nK.2 Why STDE with dense jets is not generalizable\nSpecifically, we will prove that it is impossible to construct dense STDE for the fourth-order diagonal\noperator Lu=Pd\ni=1\u22024u\n\u2202x4\ni. 90 states that the block \u00b5abcdin the\nfourth moment tensor should match C. Fourth moments can always be decomposed into second\nmoments:\nMijkl=\u00b5abcd\nijkl= \u03a3ab\nij\u03a3cd\nkl+ \u03a3ac\nik\u03a3bd\njl+ \u03a3ad\nil\u03a3bc\njk (91)\nSo finding the pthat satisfies Eq. 90 is equivalent to finding a zero-mean distribution pwith\ncovariance that satisfies the above equation. So in the case where a\u0338=b, seta= 1, b= 2, we have\n\u03c3ij=\u00b51212\nijij= \u03a311\nii\u03a322\njj+ 2(\u03a312\nij)2(92)\nand\u03a3=\u0014\n\u03a311\u03a312\n\u03a321\u03a322\u0015\n\u2208R2d\u00d72d. \u2022The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings. \u2022The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). In general, empirical results often\ndepend on implicit assumptions, which should be articulated. 4.Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)? \u2022If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed. \u2022The instructions should contain the exact command and environment needed to run to\nreproduce the results. \u2022Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted. 34\u2022The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them. Answer: [Yes]\nJustification: We use the average of 5 random seeds for all our experiment results. \u2022The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper. \u2022If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text. 359.Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? \u2022Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters. \u2022We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort. \u2022The authors should state which version of the asset is used and, if possible, include a\nURL. Their licensing guide can help determine the\nlicense of a dataset. \u2022Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. \u2022Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper. If you obtained IRB approval, you\nshould clearly state this in the paper. 38",
        "Conclusion": "1. 3. 3. 1. , \u2202kF(a)(v(1), . 43). Therefore the\ndiscrete distribution pover the sparse k-jets in Eq. Therefore, the STDE of the Laplacian operator is given by\n\u02dc\u22072Ju\u03b8(a) =d\n|J|X\nj\u2208J\u22022\n\u2202x2\nju\u03b8(a) =d\n|J|X\nj\u2208J\u22022u\u03b8(a)(ej,0) =d\n|J|X\nj\u2208Jd2u\u03b8(a,ej,0)[2] (16)\nwhere Jis the sampled index set, and the subscript [2]means taking the second-order tangent from\nthe output jet. ). 43). 1. 24. E[\u2202\u02dc\u2113residual\n\u2202\u03b8] =\u2202\u2113residual\n\u2202\u03b8. 6 Conclusion\nWe introduce STDE, a general method for constructing stochastic estimators for arbitrary differential\noperators that can be evaluated efficiently via Taylor mode AD. 1997. Hutchinson. Physics-informed machine learning. Proc. Lang. Karniadakis. Journal of computational physics , 375:1339\u20131364, 2018. [39] Mu Wang. PhD thesis, 2017. 7. , \u2202kf(a)(v(1), . Qk\nipi!(i! Qk\nipi!(i! 39. In this case T= 2,qi1= 2 andqi2= 1. )\u2208N16. 4. Now we have\nEv\u223cN(0,\u03a3)[Uvv\u22a4U\u22a4] =U\u03a3U\u22a4=C\u2032\u2032. necessary to understand the\nresults?",
        "label": 4
    },
    {
        "Abstract": "Abstract\nWe present Visual AutoRegressive modeling (V AR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine \u201cnext-scale\nprediction\u201d or \u201cnext-resolution prediction\u201d, diverging from the standard raster-scan\n\u201cnext-token prediction\u201d. These properties reveal AR\nmodels\u2019 potential in learning from vast unlabeled data, encapsulating the essence of \u201cAGI\u201d. For 1), a quantized autoencoder such\nas [30] is often used to convert the image feature map f\u2208Rh\u00d7w\u00d7Cto discrete tokens q\u2208[V]h\u00d7w:\nf=E(im), q =Q(f), (2)\nwhere imdenotes the raw image, E(\u00b7)a encoder, and Q(\u00b7)a quantizer. , r k\u22121), (6)\nwhere each autoregressive unit rk\u2208[V]hk\u00d7wkis the token map at scale kcontaining hk\u00d7wktokens,\nand the sequence (r1, r2, . ADM [26] 10.94 101.0 0.69 0.63 554M 250 168 [75]\nDiff. \u201c-s\u201d: a single shared\nAdaLN layer is used due to resource limitation. ADM [26] 23.24 101.0 \u2212\nDiff. The exponent \u03b1reflects the smoothness of power-law, and L\ndenotes the reducible loss normalized by irreducible loss L\u221e[36] A logarithmic transformation to L\nandXwill reveal a linear relation between log(L)andlog(X):\nlog(L) =\u03b1log(X) +\u03b1log\u03b2. \u201cAdaLN\u201d: adaptive layernorm. \u201cAttn. Video generation is not implemented in this work, but it can be naturally extended. 3\n[19] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y . 3\n[35] K. He, X. Chen, S. Xie, Y . 2, 3, 8,\n9\n[37] J. Ho, C. Saharia, W. Chan, D. J. arXiv\npreprint arXiv:2311.05232 , 2023. Dpm-solver: A fast ode solver for diffusion probabilistic\nmodel sampling in around 10 steps. 3\n[89] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo, T. Lu, J. Zhou, Y . Images speak in images: A generalist painter for\nin-context visual learning. 2, 3\n[92] J. Yu, X. Li, J. Y . 2, 10\n19NeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope? Guidelines:\n\u2022The answer NA means that the abstract and introduction do not include the claims\nmade in the paper. \u2022The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. 3.Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof? Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Guidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. Guidelines:\n\u2022 The answer NA means that the paper does not include experiments. This work is not related to any private or personal data, and there\u2019s no explicit negative\nsocial impacts. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster. Answer: [NA]\nJustification: The paper does not release new assets. Guidelines:\n\u2022 The answer NA means that the paper does not release new assets. Answer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects. 15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained? Answer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.",
        "Methodology": "This simple, intuitive methodology allows autoregressive\n(AR) transformers to learn visual distributions fast and can generalize well: V AR,\nfor the first time , makes GPT-style AR models surpass diffusion transformers in\nimage generation. We have released all models and codes to promote\nthe exploration of AR/V AR models for visual generation and unified learning. These models exhibit promising intelligence in generality and versatility that, despite issues like\nhallucinations [ 40], are still considered to take a solid step toward the general artificial intelligence\n(AGI). At the core of these models is a self-supervised learning strategy \u2013 predicting the next token in\na sequence, a simple yet profound approach. In contrast to the remarkable achievements\nof LLMs, the power of autoregressive models in computer vision appears to be somewhat locked . Our work reconsiders how to \u201cor-\nder\u201d an image: Humans typically perceive or\ncreate images in a hierachical manner, first cap-\nturing the global structure and then local details. Our approach begins by encoding\nan image into multi-scale token maps. The au-\ntoregressive process is then started from the 1 \u00d71\ntoken map, and progressively expands in resolu-\ntion: at each step, the transformer predicts the\nnext higher-resolution token map conditioned on\nall previous ones. We refer to this methodology\nas Visual AutoRegressive (V AR) modeling. In summary, our contributions to the community include:\n1.A new visual generative framework using a multi-scale autoregressive paradigm with next-scale\nprediction, offering new insights in autoregressive algorithm design for computer vision. 4.A comprehensive open-source code suite, including both VQ tokenizer and autoregressive model\ntraining pipelines, to help propel the advancement of visual autoregressive learning. 2 Related Work\n2.1 Properties of large autoregressive language models\nScaling laws are found and studied in autoregressive language models [ 44,36], which describe a\npower-law relationship between the scale of model (or dataset, computation, etc.) and the cross-\nentropy loss value on the test set. Scaling laws allow us to directly predict the performance of a\nlarger model from smaller ones [ 1], thus guiding better resource allocation. 2.2 Visual generation\nRaster-scan autoregressive models for visual generation necessitate the encoding of 2D images\ninto 1D token sequences. Early endeavors [ 20,85] have shown the ability to generate RGB (or\ngrouped) pixels in the standard row-by-row, raster-scan manner. VQGAN [ 30] advances [ 20,85] by\ndoing autoregressive learning in the latent space of VQV AE [ 86]. It employs GPT-2 decoder-only\ntransformer to generate tokens in the raster-scan order, like how ViT [ 28] serializes 2D images into\n1D patches. MaskGIT [ 17] employs a VQ autoencoder and a masked prediction trans-\nformer similar to BERT [ 25,10,35] to generate VQ tokens through a greedy algorithm. MagViT [ 94]\nadapts this approach to videos, and MagViT-2 [ 95] enhances [ 17,94] by introducing an improved\nVQV AE for both images and videos. MUSE [16] further scales MaskGIT to 3B parameters. 33 Method\n3.1 Preliminary: autoregressive modeling via next-token prediction\nFormulation. Consider a sequence of discrete tokens x= (x1, x2, . This unidirectional\ntoken dependency assumption allows for the factorization of the sequence x\u2019s likelihood:\np(x1, x2, . , x T) =TY\nt=1p(xt|x1, x2, . (1)\nTraining an autoregressive model p\u03b8involves optimizing p\u03b8(xt|x1, x2, . To apply autoregressive modeling to\nimages via next-token prediction, we must: 1) tokenize an image into several discrete tokens, and\n2) define a 1D order of tokens for unidirectional modeling. The quantization process q=Q(f)\nwill map each feature vector f(i,j)to the code index q(i,j)of its nearest code in the Euclidean sense:\nq(i,j)= \narg min\nv\u2208[V]\u2225lookup (Z, v)\u2212f(i,j)\u22252! \u2208[V], (3)\nwhere lookup (Z, v)means taking the v-th vector in codebook Z. To train the quantized autoencoder,\nZis looked up by every q(i,j)to get \u02c6f, the approximation of original f. Then a new image \u02c6imis\nreconstructed using the decoder D(\u00b7)given \u02c6f, and a compound loss Lis minimized:\n\u02c6f=lookup (Z, q), \u02c6im=D(\u02c6f), (4)\nL=\u2225im\u2212\u02c6im\u22252+\u2225f\u2212\u02c6f\u22252+\u03bbPLP(\u02c6im) +\u03bbGLG(\u02c6im), (5)\nwhere LP(\u00b7)is a perceptual loss such as LPIPS [ 97],LG(\u00b7)a discriminative loss like StyleGAN\u2019s\ndiscriminator loss [ 47], and \u03bbP,\u03bbGare loss weights. Once the autoencoder {E,Q,D}is fully trained,\nit will be used to tokenize images for subsequent training of a unidirectional autoregressive model. Unlike natural language sentences with an\ninherent left-to-right ordering, the order of image tokens must be explicitly defined for unidirectional\nautoregressive learning. , x h\u00d7w)using some strategy such as row-major raster scan, spiral, or z-curve order. Once flattened, they can extract a set of sequences xfrom the dataset, and then train an autoregressive\nmodel to maximize the likelihood in (1) via next-token prediction. In quantized autoencoders (VQV AEs), the encoder typically\nproduces an image feature map fwith inter-dependent feature vectors f(i,j)for all i, j. So\nafter quantization and flattening, the token sequence (x1, x2, . , x h\u00d7w)retains bidirectional\ncorrelations. 2)Inability to perform some zero-shot generalization. E.g., it cannot predict the top part of an image given the bottom part. This spatial relationship is compromised in the linear sequence\nx, where unidirectional constraints diminish these correlations. Generating an image token sequence x= (x1, x2, . \ud835\udc3f=12+22+32=14Block -wise causal mask\n1\n49Stage 2:  Training V AR  transformer  on tokens\n([S] means a start token with condition information)\nVAR Transformer (causal)\nword embedding and up -interpolation\nMulti -scale quantization  &  Embedding VAE encoding DecodingStage 1:  Training multi -scale VQV AE on images\n( to provide the ground truth for training Stage 2)\nFigure 4: V AR involves two separated training stages. Stage 1: a multi-scale VQ autoencoder encodes\nan image into Ktoken maps R= (r1, r2, . , r K)and is trained by a compound loss (5). Stage 2: a V AR transformer is trained\nvia next-scale prediction (6): it takes ([s], r1, r2, . , r K\u22121)as input to predict (r1, r2, r3, . The\nattention mask is used in training to ensure each rkcan only attend to r\u2264k. Standard cross-entropy loss is used. 3.2 Visual autoregressive modeling via next-scale prediction\nReformulation. We reconceptualize the autoregressive modeling on images by shifting from \u201cnext-\ntoken prediction\u201d to \u201cnext-scale prediction\u201d strategy. Here, the autoregressive unit is an entire token\nmap, rather than a single token . We start by quantizing a feature map f\u2208Rh\u00d7w\u00d7CintoKmulti-scale\ntoken maps (r1, r2, . The autoregressive likelihood is formulated as:\np(r1, r2, . , r K) =KY\nk=1p(rk|r1, r2, . , r k\u22121)serves as the the \u201cprefix\u201d for rk. During the k-th autoregressive\nstep, all distributions over the hk\u00d7wktokens in rkwill be generated in parallel, conditioned on rk\u2019s\nprefix and associated k-th position embedding map. This \u201cnext-scale prediction\u201d methodology is\nwhat we define as visual autoregressive modeling (V AR), depicted on the right side of Fig. During inference, kv-caching can be used and no mask is needed. 2)The spatial locality is preserved as (i) there is no flattening operation in V AR, and (ii) tokens in\neachrkare fully correlated. The multi-scale design additionally reinforces the spatial structure. This efficiency gain arises from the parallel token generation in each rk. We develope a new multi-scale quantization autoencoder to encode an image to K\nmulti-scale discrete token maps R= (r1, r2, . , r K)necessary for V AR learning (6). We employ\nthe same architecture as VQGAN [ 30] but with a modified multi-scale quantization layer. The\nencoding and decoding procedures with residual design on for\u02c6fare detailed in algorithms 1 and\n2. 5Note that a shared codebook Zis utilized across all scales, ensuring that each rk\u2019s tokens belong to\nthe same vocabulary [V]. To address the information loss in upscaling zktohK\u00d7wK, we use K\nextra convolution layers {\u03d5k}K\nk=1. No convolution is used after downsampling ftohk\u00d7wk. As aforementioned, we use the vanilla VQV AE architecture [ 30] and a multi-\nscale quantization scheme with Kextra convolutions (0.03M extra parameters). We use a shared\ncodebook for all scales with V= 4096 . Following the baseline [ 30], our tokenizer is also trained on\nOpenImages [50] with the compound loss (5) and a spatial downsample ratio of 16\u00d7. We adopt the architecture of standard decoder-only transformers akin to GPT-2 and VQ-\nGAN [ 67,30] with adaptive normalization (AdaLN), which has widespread adoption and proven\neffectiveness in many visual generative models [ 47,48,46,75,74,43,64,19]. We\nfound normalizing queries andkeys to unit vectors before attention can stablize the training. We do\nnot use advanced techniques in large language models, such as rotary position embedding (RoPE),\nSwiGLU MLP, or RMS Norm [ 83,84]. Our model shape follows a simple rule like [ 44] that the\nwidth w, head counts h, and drop rate drare linearly scaled with the depth das follows:\nw= 64d, h =d, dr = 0.1\u00b7d/24. 5 suggest that such\na simple model design are capable of scaling and generalizing well. Among all VQV AE-based AR or V AR models, VQGAN [ 30] and ours use the same\narchitecture (CNN) and training data (OpenImages [ 50]) for VQV AE, while ViT-VQGAN [ 92] uses\na ViT autoencoder, and both it and RQTransformer [ 51] trains the VQV AE directly on ImageNet. 3Due to resource limitation, we use a single shared adaptive layernorm (AdaLN) acorss all attention blocks\nin 512\u00d7512 synthesis. In this case, the parameter count would be reduced to around 12dw2+ 6w2\u224849152 d3. \u201c\u2193\u201d or \u201c\u2191\u201d indicate\nlower or higher values are better. Metrics include Fr\u00e9chet inception distance (FID), inception score (IS), precision\n(Pre) and recall (rec). \u201c#Step\u201d: the number of model runs needed to generate an image. CDM [37] 4.88 158.7 \u2212 \u2212 \u2212 8100 \u2212\nDiff. LDM-4-G [71] 3.60 247.7 \u2212 \u2212 400M 250 \u2212\nDiff. 1, V AR not only achieves the best FID/IS but also demonstrates remarkable\nspeed in image generation. Notably, V AR significantly advances traditional AR capabilities. A full autoregressive generation of\nn2tokens requires O(n2)decoding iterations\nandO(n6)total computations. 3) V AR is considered more data-efficient, as it requires only 350 training\nepochs compared to DiT-XL/2\u2019s 1400. 3 and Tab. In contrast, the FID and IS of V AR are\nconsistently improved, aligning with the scaling law study in Sec. Axes are all on a logarithmic scale. The Pearson correlation coefficients near \u22120.998\nsignify a strong linear relationship between log(N)vs.log(L)orlog(N)vs.log(Err). These observed scaling laws [ 44,36,39,1] not only validate the scalability of LLMs but also serve\nas a predictive tool for AR modeling, which facilitates the estimation of performance for larger\nAR models based on their smaller counterparts, thereby saving resource usage by large model\nperformance forecasting. Given these appealing properties of scaling laws brought by LLMs, their\nreplication in computer vision is therefore of significant interest. Setup of scaling V AR models. Following the protocols from [ 44,36,39,1], we examine whether\nour V AR model complies with similar scaling laws. We trained models across 12 different sizes,\nfrom 18M to 2B parameters, on the ImageNet training set [ 24] containing 1.28M images (or 870B\nimage tokens under our VQV AE) per epoch. For models of different sizes, training spanned 200 to\n350 epochs, with a maximum number of tokens reaching 305 billion. We varied dfrom 6to30, yielding 12 models with 18.5M to 2.0B parameters. Axes are on a logarithmic scale. 6 Ablation Study\nIn this study, we aim to verify the effectiveness and efficiency of our proposed V AR framework. Starting from the vanilla AR transformer baseline implemented\nby [17], we replace its methodology with our V AR and keep other settings unchanged to get row 2. The first two rows compare GPT-2-style transformers trained under AR or\nV AR algorithm without any bells and whistles. Subsequent lines show the influence of V AR enhancements. \u201c \u2206\u201d: FID reduction to the baseline. By replacing the standard\nLayer Normalization (LN) with Adaptive Layer Normalization (AdaLN), V AR starts yielding better\nFID than baseline. By using the top- ksampling similar to the baseline, V AR\u2019s FID is further improved. 7 Limitations and Future Work\nIn this work, we mainly focus on the design of learning paradigm and keep the VQV AE architecture\nand training unchanged from the baseline [ 30] to better justify V AR framework\u2019s effectiveness. We believe iterating V AR by advanced tokenizer\nor sampling techniques in these latest work can further improve V AR\u2019s performance or speed. Given that our model is funda-\nmentally similar to modern LLMs, it can easily be integrated with them to perform text-to-image\ngeneration through either an encoder-decoder or in-context manner. By considering\nmulti-scale video features as 3D pyramids , we can formulate a similar \u201c 3D next-scale prediction \u201d\nto generate videos via V AR. Compared to diffusion-based generators like SORA [ 14], our method\nhas inherent advantages in temporal consistency or integration with LLMs, thus can potentially\nhandle longer temporal dependencies. This makes V AR competitive in the video generation field,\nbecause traditional AR models can be too inefficient for video generation due to their extremely\nhigh computational complexity and slow inference speed: it is becoming prohibitively expensive to\ngenerate high-resolution videos with traditional AR models, while V AR is capable to solve this. To keep the content consistent, a same\nrandom seed and teacher-forced initial tokens are used. Figure 7: Scaling model size Nand training compute Cimproves visual fidelity and soundness. Zoom in\nfor a better view. Samples are drawn from V AR models of 4 different sizes and 3 different training stages. 11In-painting\nOut-painting\nClass -cond\nEditingorigin al generated\nFigure 8: Zero-shot evaluation in downstream tasks containing in-painting, out-painting, and class-\nconditional editing. Zoom in for a better view. For in- and out-painting, we teacher-force\nground truth tokens outside the mask and let the model only generate tokens within the mask. The normalized heat map of attention scores in the last self-attention\nlayer of VQGAN encoder is visualized. 4 random 256 \u00d7256 images from ImageNet validation set are used. 12C Token dependency in VQV AE\nTo examine the token dependency in VQV AE [ 30], we check the attention scores in the self-attention\nlayer before the vector quantization module. Note the self-attention layer in [ 30] only has 1 head so for\neach image we just plot one attention map. 9 shows the attention scores of each\ntoken to all other tokens, which indicate a strong, bidirectional dependency among all tokens. This is\nnot surprising since the VQV AE model, trained to reconstruct images, leverages self-attention layers\nwithout any attention mask. The total number of tokens is h\u00d7w=n2. For the i-th (1\u2264i\u2264n2) autoregressive iteration,\nthe attention scores between each token and all other tokens need to be computed, which requires\nO(i2)time. So the total time complexity would be:\nn2X\ni=1i2=1\n6n2(n2+ 1)(2 n2+ 1), (17)\nWhich is equivalent to O(n6)basic computation. , h K, wK)for autoregres-\nsive generation, where hi, wiare the height and width of the VQ code map at the i-th autoregressive\nstep, and hK=h, wK=wreaches the final resolution. Suppose nk=hk=wkfor all 1\u2264k\u2264K\nandn=h=w, for simplicity. For a standard self-attention transformer and given hyperparameter a >1, the time\ncomplexity of VAR generation is O(n4), where h=w=nandh, w are the height and width of the\nlast (largest) VQ code map, respectively. The total number of tokens of\ncurrent all token maps (r1, r2, . (18)\nSo the time complexity of the k-th autoregressive generation would be:\n\u0012a2k\u22121\na2\u22121\u00132\n. 15References\n[1]J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,\nS. Altman, S. Anadkat, et al. 3\n[3]Alpha-VLLM. Large scale gan training for high fidelity natural image synthesis. arXiv preprint\narXiv:2309.15807 , 2023. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. 2, 3, 4, 5, 6, 7, 10, 13\n[31] S. Gao, P. Zhou, M.-M. Cheng, and S. Yan. Making llama see and draw with seed\ntokenizer. Masked autoencoders are scalable vision\nlearners. Cascaded diffusion models for high\nfidelity image generation. Training compute-optimal large language models. Progressive growing of gans for improved quality, stability,\nand variation. Analyzing and improving the\nimage quality of stylegan. Autoregressive image generation using residual\nquantization. Self-conditioned image generation via generating representations. Feature pyramid networks for\nobject detection. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE\ninternational conference on computer vision , volume 2, pages 1150\u20131157. Dinov2: Learning robust visual features without supervision. Training language models to follow instructions with human feedback. Learning transferable visual models from natural language supervision. Improving language understanding by\ngenerative pre-training. Generating diverse high-fidelity images with vq-vae-2. Parallel multiscale autoregressive density estimation. High-resolution image synthesis with\nlatent diffusion models. Multitask prompted training enables zero-shot task generalization. Denoising diffusion implicit models. Generative modeling by estimating gradients of the data distribution. Designing bert for convolutional networks:\nSparse and hierarchical masked modeling. Conditional image generation\nwith pixelcnn decoders. arXiv preprint\narXiv:2310.05737 , 2023. Opt: Open pre-trained transformer language models. Our main contributions are also detailed in Sec. A No or\nNA answer to this question will not be perceived well by the reviewers. \u2022It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper. 2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]\nJustification: Yes, please see Sec. 1 and Appendix D.\nGuidelines:\n\u2022The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be. \u2022The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. \u2022The authors should reflect on the factors that influence the performance of the approach. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon. \u2022The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size. \u2022If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness. \u2022While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations. \u2022All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced. \u2022All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition. \u2022Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material. Answer: [Yes]\nJustification: We use publicly-accessable dataset ImageNet [ 24]. \u2022If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not. \u2022Depending on the contribution, reproducibility can be accomplished in various ways. \u2022While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. (c)If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset). In the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results. Answer: [Yes]\nJustification: We use publicly-accessable dataset ImageNet [ 24]. Once the blind review period is finished, we\u2019ll\nopen-source all codes, instructions, and model checkpoints. \u2022Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy ) for more details. \u2022While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark). See the NeurIPS code and data submission guidelines ( https:\n//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. \u2022The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why. \u2022At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable). Answer: [Yes]\nJustification: Please see Sec. \u2022The full details can be provided either with the code, in appendix, or as supplemental\nmaterial. Answer: [No]\nJustification: Due to the resource limitation, we do not report error bars. \u2022The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.) \u2022 The assumptions made should be given (e.g., Normally distributed errors). \u2022It should be clear whether the error bar is the standard deviation or the standard error\nof the mean. \u2022It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified. \u2022For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. \u2022The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage. \u2022The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute. \u2022The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper). \u2022The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction). 10.Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed? Guidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed. 23\u2022If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact. \u2022The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. \u2022The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology. 11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)? Answer: [No]\nJustification: We do not foresee any high risk for misuse of this work. Guidelines:\n\u2022 The answer NA means that the paper poses no such risks. \u2022Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images. 12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected? Guidelines:\n\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. \u2022For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided. 24\u2022If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. \u2022For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided. \u2022If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators. 13.New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets? This includes details about training, license,\nlimitations, etc. \u2022The paper should discuss whether and how consent was obtained from people whose\nasset is used. \u2022At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file. 14.Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)? \u2022According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector. \u2022Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. 25\u2022We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution. \u2022For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.",
        "Results and Findings": "We\nshow 512 \u00d7512 samples (top), 256 \u00d7256 samples (middle), and zero-shot image editing results (bottom). It is\nalso empirically verified that V AR outperforms the Diffusion Transformer (DiT) in\nmultiple dimensions including image quality, inference speed, data efficiency, and\nscalability. Scaling up V AR models exhibits clear power-law scaling laws similar to\nthose observed in LLMs, with linear correlation coefficients near \u22120.998as solid\nevidence. These results suggest\nV AR has initially emulated the two important properties of LLMs: Scaling Laws\nandzero-shot generalization. \u2217corresponding authors: wanglw@pku.edu.cn, yuanzehuan@bytedance.com; \u2020: project lead\n38th Conference on Neural Information Processing Systems (NeurIPS 2024). Studies into the success of these large AR models have\nhighlighted their scalability and generalizabilty : the former, as exemplified by scaling laws [44,36],\nallows us to predict large model\u2019s performance from smaller ones and thus guides better resource\nallocation, while the latter, as evidenced by zero-shot and few-shot learning [ 67,15], underscores\nthe unsupervised-trained models\u2019 adaptability to diverse, unseen tasks. 2 a). DiTRCGAR(RQ)\nGigagan\nV AR (ours)ADM0.3B 1B 2B 5B\nMaskGIT AR(vqgan )\nlower is better\nFigure 3: Scaling behavior of different model fami-\nlies on ImageNet 256 \u00d7256 generation benchmark. 2 (b). 6 for details). 2.An empirical validation of V AR models\u2019 Scaling Laws and zero-shot generalization potential,\nwhich initially emulates the appealing properties of large language models (LLMs). 3.A breakthrough in visual autoregressive model performance, making GPT-style autoregressive\nmethods surpass strong diffusion models in image synthesis for the first time2. . . . . . . . . . . , x t\u22121)over a dataset. . . . . . . . . . . . . . . . . . . . . . . . . We empirically find this residual-style design, akin to [ 51], can perform better than independent\ninterpolation. . . (7)\nConsequently, the main parameter count Nof a V AR transformer with depth dis given by3:\nN(d) =d\u00b74w2\n|{z}\nself-attention+d\u00b78w2\n|{z}\nfeed-forward+d\u00b76w2\n|{z}\nadaptive layernorm= 18 dw2= 73728 d3. The evaluations in Sec. 5 Empirical Results\nThis section first compares V AR with other image generative model families in Sec. 5.1. 5.2 and Appendix B. We test V AR models with depths 16, 20, 24, and 30 on ImageNet 256 \u00d7256 and 512 \u00d7512\nconditional generation benchmarks and compare them with the state-of-the-art image generation\nmodel families. DiT-L/2 [64] 5.02 167.2 0.75 0.57 458M 250 31\nDiff. DiT-XL/2 [64] 2.27 278.2 0.83 0.57 675M 250 45\nDiff. L-DiT-3B [3] 2.10 304.4 0.82 0.60 3.0B 250 >45\nDiff. L-DiT-7B [3] 2.28 316.2 0.83 0.58 7.0B 250 >45\nMask. MaskGIT [17] 6.18 182.1 0.80 0.51 227M 8 0.5 [17]\nMask. [52] 3.49 215.5 \u2212 \u2212 502M 20 1.9 [52]\nAR VQV AE-2\u2020[69] 31.11 \u223c45 0.36 0.57 13.5B 5120 \u2212\nAR VQGAN\u2020[30] 18.65 80.4 0.78 0.26 227M 256 19 [17]\nAR VQGAN [30] 15.78 74.3 \u2212 \u2212 1.4B 256 24\nAR VQGAN-re [30] 5.20 280.3 \u2212 \u2212 1.4B 256 24\nAR ViTVQ [92] 4.17 175.1 \u2212 \u2212 1.7B 1024 >24\nAR ViTVQ-re [92] 3.04 227.4 \u2212 \u2212 1.7B 1024 >24\nAR RQTran. [51] 7.55 134.0 \u2212 \u2212 3.8B 68 21\nV AR V AR- d16 3.30 274.4 0.84 0.51 310M 10 0.4\nV AR V AR- d20 2.57 302.6 0.83 0.56 600M 10 0.5\nV AR V AR- d24 2.09 312.9 0.82 0.59 1.0B 10 0.6\nV AR V AR- d30 1.92 323.1 0.82 0.59 2.0B 10 1\nV AR V AR- d30-re 1.73 350.2 0.82 0.60 2.0B 10 1\n(validation data) 1.78 236.9 0.75 0.67\nOverall comparison. As shown in Tab. V AR also maintains decent precision and recall, confirming its semantic\nconsistency. To our knowledge, this is the first\ntime of autoregressive models outperforming Diffusion transformers, a milestone made possible by\nV AR\u2019s resolution of AR limitations discussed in Section 3. Type Model FID\u2193 IS\u2191 Time\nGAN BigGAN [13] 8.43 177.9 \u2212\nDiff. MaskGIT [17] 7.32 156.0 0.5\u2020\nAR VQGAN [30] 26.52 66.8 25\u2020\nV AR V AR- d36-s 2.63 303.2 1Efficiency comparison. 1 also provides empirical evidence that\nV AR is around 20 times faster than VQGAN\nand ViT-VQGAN even with more model parame-\nters, reaching the speed of efficient GAN models\nwhich only require 1 step to generate an image. The V AR model surpasses the recently popular\ndiffusion models Diffusion Transformer (DiT), which serves as the precursor to the latest Stable-\nDiffusion 3 [ 29] and SORA [ 14], in multiple dimensions: 1) In image generation diversity and quality\n7(FID and IS), V AR with 2B parameters consistently performs better than DiT-XL/2 [64], L-DiT-3B,\nand L-DiT-7B [ 3]. 1 show that DiT only obtains\nmarginal or even negative gains beyond 675M parameters. 5.2. These results establish VAR as\npotentially a more efficient and scalable model for image generation than models like DiT . /uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c/uni00000013/uni00000011/uni0000001a/uni00000017/uni00000013/uni00000011/uni0000001c/uni0000001a/uni00000014/uni00000011/uni00000015/uni0000001a/uni00000014/uni00000011/uni00000019/uni00000019/uni00000015/uni00000011/uni00000014/uni00000019/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000c\n/uni0000000b/uni00000044/uni0000000c\nL=(2.0N)0.23\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000011/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001c/uni00000016\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c/uni00000013/uni00000011/uni0000001a/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000014/uni00000011/uni00000014/uni00000017/uni00000014/uni00000011/uni00000017/uni00000017/uni00000014/uni00000011/uni0000001b/uni00000014/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000044/uni0000004f/uni0000004f/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000c\n/uni0000000b/uni00000045/uni0000000c\nL=(2.5N)0.20\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000011/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001c/uni00000018\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c/uni0000001b/uni0000001c/uni00000011/uni00000019/uni0000001c/uni00000014/uni00000011/uni00000016/uni0000001c/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000017/uni00000011/uni0000001c/uni0000001c/uni00000019/uni00000011/uni0000001b/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000f/uni00000003/uni00000008/uni0000000c\n/uni0000000b/uni00000046/uni0000000c\nErr=(5102Npara/uni0000000c0.02\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000011/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001a/uni0000001b\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c/uni0000001c/uni00000016/uni00000011/uni00000015/uni0000001c/uni00000017/uni00000011/uni00000016/uni0000001c/uni00000018/uni00000011/uni00000017/uni0000001c/uni00000019/uni00000011/uni00000018/uni0000001c/uni0000001a/uni00000011/uni00000019/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000044/uni0000004f/uni0000004f/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000f/uni00000003/uni00000008/uni0000000c\n/uni0000000b/uni00000047/uni0000000c\nErr=(6102Npara/uni0000000c0.01\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000011/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001b/uni0000001c\nFigure 5: Scaling laws with V AR transformer size N, with power-law fits (dashed) and equations (in legend). Small, near-zero exponents \u03b1suggest a smooth decline in both test loss Land token error rate Err when scaling\nup V AR transformer. Prior research [ 44,36,39,1] have established that scaling up autoregressive (AR) large\nlanguage models (LLMs) leads to a predictable decrease in test loss L. This trend correlates with\nparameter counts N, training tokens T, and optimal training compute Cmin, following a power-law:\nL= (\u03b2\u00b7X)\u03b1, (9)\nwhere Xcan be any of N,T, orCmin. We\nassessed the final test cross-entropy loss Land token prediction error rates Err on the ImageNet\nvalidation set of 50,000 images [ 24]. We computed LandErr for both the last scale (at the last\nnext-scale autoregressive step), as well as the global average. Results are plotted in Fig. 5, where we\n8/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000015/uni00000014/uni00000011/uni00000013/uni00000017/uni00000014/uni00000011/uni00000017/uni0000001c/uni00000015/uni00000011/uni00000014/uni00000017/uni00000016/uni00000011/uni00000013/uni0000001a/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000044/uni0000004f/uni0000004f/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000c/uni00000033/uni00000044/uni00000055/uni00000048/uni00000057/uni00000052/uni00000003/uni00000049/uni00000055/uni00000052/uni00000051/uni00000057/uni0000004c/uni00000048/uni00000055/uni00000003Cmin\nL=(2.2105Cmin/uni0000000c0.13\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001b\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000017/uni00000014/uni00000011/uni00000014/uni00000015/uni00000014/uni00000011/uni00000019/uni0000001c/uni00000015/uni00000011/uni00000018/uni00000018/uni00000016/uni00000011/uni0000001b/uni00000018/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000c/uni00000033/uni00000044/uni00000055/uni00000048/uni00000057/uni00000052/uni00000003/uni00000049/uni00000055/uni00000052/uni00000051/uni00000057/uni0000004c/uni00000048/uni00000055/uni00000003Cmin\nL=(1.5105Cmin/uni0000000c0.16\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni00000019\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000033/uni00000029/uni0000004f/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000001c/uni00000016/uni00000011/uni00000015/uni0000001c/uni00000017/uni00000011/uni0000001c/uni0000001c/uni00000019/uni00000011/uni00000018/uni0000001c/uni0000001b/uni00000011/uni00000014/uni0000001c/uni0000001c/uni00000011/uni0000001b/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000044/uni0000004f/uni0000004f/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000f/uni00000003/uni00000008/uni0000000c/uni00000033/uni00000044/uni00000055/uni00000048/uni00000057/uni00000052/uni00000003/uni00000049/uni00000055/uni00000052/uni00000051/uni00000057/uni0000004c/uni00000048/uni00000055/uni00000003Cmin\nErr=(8.1102Cmin/uni0000000c0.0067\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001a\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000033/uni00000029/uni0000004f/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000001b/uni0000001c/uni00000011/uni00000019/uni0000001c/uni00000015/uni00000011/uni00000014/uni0000001c/uni00000017/uni00000011/uni00000019/uni0000001c/uni0000001a/uni00000011/uni00000015/uni0000001c/uni0000001c/uni00000011/uni0000001c/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000f/uni00000003/uni00000008/uni0000000c/uni00000033/uni00000044/uni00000055/uni00000048/uni00000057/uni00000052/uni00000003/uni00000049/uni00000055/uni00000052/uni00000051/uni00000057/uni0000004c/uni00000048/uni00000055/uni00000003Cmin\nErr=(4.4102Cmin/uni0000000c0.011\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003=/uni00000013/uni00000011/uni0000001c/uni0000001c/uni00000018\nFigure 6: Scaling laws with optimal training compute Cmin.Line color denotes different model sizes. observed a clear power-law scaling trend for Las a function of N, as consistent with [ 44,36,39,1]. (12)\nThese results verify the strong scalability of V AR, by which scaling up V AR transformers can\ncontinuously improve the model\u2019s test performance. Scaling laws with optimal training compute Cmin.We then examine the scaling behavior of V AR\ntransformers when increasing training compute C. For each of the 12 models, we traced the test loss\nLand token error rate Err as a function of Cduring training quoted in PFlops ( 1015floating-point\noperations per second). The results are plotted in Fig. The fitted power-law scaling laws for LandErr as a function of Cminare:\nLlast= (2.2\u00b710\u22125Cmin)\u22120.13(13)\nLavg= (1.5\u00b710\u22125Cmin)\u22120.16, (14)\nErr last= (8.1\u00b710\u22122Cmin)\u22120.0067(15)\nErr avg= (4.4\u00b710\u22122Cmin)\u22120.011. (16)\nThese relations (14, 16) hold across 6 orders of magnitude in Cmin, and our findings are consistent\nwith those in [ 44,36]: when trained with sufficient data, larger V AR transformers are more compute-\nefficient because they can reach the same level of performance with less computation. Results are reported in Tab. 9Table 3: Ablation study of V AR. 310M V AR- d16 \u2713 900 1.5 0.022 3.30 \u221215.35\n6+Scale up 2.0B V AR- d30 \u2713 900 1.5 0.052 1.73 \u221216.85\nV AR achieves a way more better FID (18.65 vs.5.22) with only 0.013 \u00d7inference wall-clock cost\nthan the AR model, which demonstrates a leap in visual AR model\u2019s performance and efficiency. We finally scale up V AR size to 2.0B and achieve an FID of 1.73. This\nis 16.85 better than the baseline FID. Upon scaling V AR to 2 billion parameters, we\nobserved a clear power-law relationship between test performance and model parameters or training\ncompute, with Pearson coefficients nearing \u22120.998, indicating a robust framework for performance\nprediction. These scaling laws and the possibility for zero-shot task generalization, as hallmarks of\nLLMs, have now been initially verified in our V AR transformer models. We hope our findings and\nopen sources can facilitate a more seamless integration of the substantial successes from the natural\nlanguage processing domain into computer vision, ultimately contributing to the advancement of\npowerful multi-modal intelligence. The observed improvements in visual fidelity\nand soundness are consistent with the scaling laws, as larger transformers are thought able to learn\nmore complex and fine-grained image distributions. The results show that V AR can generalize to novel downstream tasks without special\ndesign and finetuning. V AR- d30 is tested. The results are visualized in Fig. Without\nmodifications to the network architecture or tuning parameters, V AR has achieved decent results on\nthese downstream tasks, substantiating the generalization ability of V AR. Following MaskGIT [ 17] we also tested V AR on the class-\nconditional image editing task. Fig. 8 shows the model can produce\nplausible content that fuses well into the surrounding contexts, again verifying the generality of V AR. We randomly sample 4 256 \u00d7256 images from the\nImageNet validation set for this analysis. . . We set the resolutions as nk=a(k\u22121)where a >1is a constant\nsuch that a(K\u22121)=n. . . , r k)is:\nkX\ni=1n2\ni=kX\ni=1a2\u00b7(k\u22121)=a2k\u22121\na2\u22121. (19)\nBy summing up all autoregressive generations, we have:\nloga(n)+1X\nk=1\u0012a2k\u22121\na2\u22121\u00132\n(20)\n=(a4\u22121) log n+\u0000\na8n4\u22122a6n2\u22122a4(n2\u22121) + 2 a2\u22121\u0001\nloga\n(a2\u22121)3(a2+ 1) log a(21)\n\u223c O(n4). 14Figure 11: Some generated 256 \u00d7256 samples by V AR trained on ImageNet. arXiv preprint arXiv:2303.08774 , 2023. 2, 3, 8, 9\n[2]J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/\nf7fe19834b23e38f333403b91bb0330afe19f79e/Large-DiT-ImageNet , 2024. 2, 7, 8\n[4]R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen,\net al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023. 2\n[5]J. Bai, S. Bai, Y . Han, F. Huang, et al. Qwen technical\nreport. arXiv preprint arXiv:2309.16609 , 2023. 2\n[6]Y . Bai, X. Geng, K. Mangalam, A. Bar, A. Yuille, T. Darrell, J. Malik, and A. arXiv preprint arXiv:2312.00785 , 2023. 2, 3\n[7]F. Bao, C. Li, J. Zhu, and B. Zhang. arXiv preprint arXiv:2201.06503 , 2022. 3\n[8]F. Bao, S. Nie, K. Xue, Y . Cao, C. Li, H. Su, and J. Zhu. 3\n[9]F. Bao, C. Xiang, G. Yue, G. He, H. Zhu, K. Zheng, M. Zhao, S. Liu, Y . Wang, and J. Zhu. Vidu: a\nhighly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint\narXiv:2405.04233 , 2024. 3\n[10] H. Bao, L. Dong, S. Piao, and F. Wei. arXiv preprint\narXiv:2106.08254 , 2021. 3\n[11] A. Gandelsman, T. Darrell, A. Globerson, and A. Efros. Visual prompting via image inpainting. 3\n[12] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, A. Ephrat, J. Hur, Y . Li, T. Michaeli, et al. arXiv preprint arXiv:2401.12945 , 2024. 3\n[13] A. Brock, J. Donahue, and K. Simonyan. arXiv preprint arXiv:1809.11096 , 2018. 7\n[14] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y . Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman,\nC. Ng, R. Wang, and A. Ramesh. 3, 7, 10\n[15] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. 2, 3\n[16] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T. Freeman,\nM. Rubinstein, et al. arXiv preprint\narXiv:2301.00704 , 2023. 3\n[17] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman. 3, 7, 9, 12\n[18] J. Chen, C. Ge, E. Xie, Y . Wu, L. Yao, X. Ren, Z. Wang, P. Luo, H. Lu, and Z. Li. arXiv preprint arXiv:2403.04692 ,\n2024. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al. arXiv preprint arXiv:2310.00426 , 2023. 3, 6\n[20] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever. In International conference on machine learning , pages 1691\u20131703. 3\n[21] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. arXiv preprint\narXiv:2312.14238 , 2023. 3\n[22] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,\nS. Gehrmann, et al. Journal of Machine Learning\nResearch , 24(240):1\u2013113, 2023. 2\n[23] X. Dai, J. Hou, C.-Y . Ma, S. Tsai, J. Wang, R. Wang, P. Zhang, S. Vandenhende, X. Wang, A. Dubey,\net al. 3\n[24] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 8, 21, 22\n16[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. arXiv preprint arXiv:1810.04805 , 2018. 3\n[26] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. 7\n[27] R. Dong, C. Han, Y . Sun, H. Zhou, H. Wei, et al. arXiv preprint arXiv:2309.11499 , 2023. 3\n[28] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al. arXiv preprint arXiv:2010.11929 , 2020. 3\n[29] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M\u00fcller, H. Saini, Y . Marek, and R. Rombach. 3, 7\n[30] P. Esser, R. Rombach, and B. Ommer. arXiv preprint arXiv:2303.14389 , 2023. Ge, S. Zhao, Z. Zeng, Y . Ge, C. Li, X. Wang, and Y . arXiv preprint arXiv:2310.01218 , 2023. Ge, S. Zhao, J. Zhu, Y . Ge, K. Yi, L. Song, C. Li, X. Ding, and Y . arXiv preprint arXiv:2404.14396 , 2024. 3\n[34] A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, L. Fei-Fei, I. Essa, L. Jiang, and J. Lezama. arXiv preprint arXiv:2312.06662 , 2023. Li, P. Doll\u00e1r, and R. Girshick. 3\n[36] T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray,\net al. arXiv preprint arXiv:2010.14701 , 2020. Fleet, M. Norouzi, and T. Salimans. The Journal of Machine Learning Research , 23(1):2249\u20132281, 2022. 3, 7\n[38] J. Ho and T. Salimans. arXiv preprint arXiv:2207.12598 , 2022. 3\n[39] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\nHendricks, J. Welbl, A. Clark, et al. arXiv preprint\narXiv:2203.15556 , 2022. 2, 3, 8, 9\n[40] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, et al. 2\n[41] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim. InEuropean Conference on Computer Vision , pages 709\u2013727. Springer, 2022. Jin, K. Xu, L. Chen, C. Liao, J. Tan, B. Chen, C. Lei, A. Liu, C. Song, X. Lei, et al. Unified language-\nvision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669 , 2023. 3\n[43] M. Kang, J.-Y . Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park. 6, 7\n[44] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. arXiv preprint arXiv:2001.08361 , 2020. 2, 3, 6, 8, 9\n[45] T. Karras, T. Aila, S. Laine, and J. Lehtinen. arXiv preprint arXiv:1710.10196 , 2017. 2\n[46] T. Karras, M. Aittala, S. Laine, E. H\u00e4rk\u00f6nen, J. Hellsten, J. Lehtinen, and T. Aila. 6\n[47] T. Karras, S. Laine, and T. Aila. 4, 6\n[48] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Segment anything. arXiv preprint arXiv:2304.02643 , 2023. 3\n[50] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci,\nA. Kolesnikov, et al. 6\n17[51] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. 2, 3, 4, 5, 6, 7\n[52] T. Li, D. Katabi, and K. He. arXiv\npreprint arXiv:2312.03701 , 2023. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie. 2\n[54] H. Liu, C. Li, Q. Wu, and Y . J. Lee. 3\n[55] D. G. Lowe. 2\n[56] C. Lu, Y . Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. 3\n[57] C. Lu, Y . Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. arXiv preprint arXiv:2211.01095 , 2022. 3\n[58] J. Lu, C. Clark, S. Lee, Z. Zhang, S. Khosla, R. Marten, D. Hoiem, and A. Kembhavi. arXiv preprint\narXiv:2312.17172 , 2023. 2\n[59] J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. arXiv preprint arXiv:2206.08916 , 2022. 2\n[60] F. Mentzer, D. Minnen, E. Agustsson, and M. Tschannen. arXiv preprint arXiv:2309.15505 , 2023. 10\n[61] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. arXiv preprint\narXiv:2112.10741 , 2021. 3\n[62] M. Oquab, T. Darcet, T. Moutakanni, H. V o, M. Szafraniec, V . Khalidov, P. Fernandez, D. Haziza,\nF. Massa, A. El-Nouby, et al. arXiv preprint\narXiv:2304.07193 , 2023. 3\n[63] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. 2\n[64] W. Peebles and S. Xie. 2, 3, 6, 7, 8\n[65] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al. In International\nconference on machine learning , pages 8748\u20138763. 3\n[66] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. article , 2018. 2\n[67] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. OpenAI blog , 1(8):9, 2019. 2, 3, 6\n[68] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen, and I. Sutskever. Zero-shot\ntext-to-image generation. In International Conference on Machine Learning , pages 8821\u20138831. 2\n[69] A. Razavi, A. Van den Oord, and O. Vinyals. 2, 3, 7\n[70] S. Reed, A. Oord, N. Kalchbrenner, S. G. Colmenarejo, Z. Wang, Y . Chen, D. Belov, and N. Freitas. In International conference on machine learning ,\npages 2912\u20132921. 3\n[71] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. 3, 7\n[72] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\nB. Karagol Ayan, T. Salimans, et al. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L.\nScao, A. Raja, et al. arXiv preprint\narXiv:2110.08207 , 2021. 3\n[74] A. Sauer, T. Karras, S. Laine, A. Geiger, and T. Aila. arXiv preprint arXiv:2301.09515 , 2023. 6\n[75] A. Sauer, K. Schwarz, and A. Geiger. In ACM\nSIGGRAPH 2022 conference proceedings , pages 1\u201310, 2022. 6, 7\n18[76] J. Song, C. Meng, and S. Ermon. arXiv preprint arXiv:2010.02502 ,\n2020. Cui, F. Zhang, X. Zhang, Y . Wang, H. Gao, J. Liu, T. Huang, and X. Wang. arXiv preprint arXiv:2307.05222 , 2023. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y . arXiv preprint\narXiv:2107.02137 , 2021. 2\n[80] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\net al. arXiv preprint arXiv:2312.11805 , 2023. 2\n[81] C. Tian, X. Zhu, Y . Xiong, W. Wang, Z. Chen, W. Wang, Y . Chen, L. Lu, T. Lu, J. Zhou, et al. arXiv\npreprint arXiv:2401.10208 , 2024. 3\n[82] K. Tian, Y . Jiang, Q. Diao, C. Lin, L. Wang, and Z. Yuan. arXiv preprint arXiv:2301.03580 , 2023. 2\n[83] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro,\nF. Azhar, et al. arXiv preprint arXiv:2302.13971 ,\n2023. 2, 3, 6\n[84] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, et al. arXiv preprint arXiv:2307.09288 ,\n2023. 2, 3, 6\n[85] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves, et al. 3\n[86] A. Van Den Oord, O. Vinyals, et al. 3\n[87] R. Villegas, M. Babaeizadeh, P.-J. 13\n[88] H. Wang, H. Tang, L. Jiang, S. Shi, M. F. Naeem, H. Li, B. Schiele, and L. Wang. arXiv preprint arXiv:2403.09394 , 2024. 3\n[90] X. Wang, W. Wang, Y . Cao, C. Shen, and T. Huang. 3\n[91] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili \u00b4c, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni,\nF. Yvon, et al. arXiv preprint\narXiv:2211.05100 , 2022. Koh, H. Zhang, R. Pang, J. Qin, A. Ku, Y . Xu, J. Baldridge, and Y . arXiv preprint arXiv:2110.04627 , 2021. 2, 3, 4, 6, 7\n[93] J. Yu, Y . Xu, J. Y . Koh, T. Luong, G. Baid, Z. Wang, V . Yang, B. K. Ayan, et al. arXiv preprint arXiv:2206.10789 ,\n2(3):5, 2022. 3\n[94] L. Yu, Y . Cheng, K. Sohn, J. Lezama, H. Zhang, H. Chang, A. G. Hauptmann, M.-H. Yang, Y . 3\n[95] L. Yu, J. Lezama, N. B. Gundavarapu, L. Versari, K. Sohn, D. Minnen, Y . Cheng, A. Gupta, X. Gu, A. G.\nHauptmann, et al. 3, 10\n[96] L. Yu, B. Shi, R. Pasunuru, B. Muller, O. Golovneva, T. Wang, A. Babu, B. Tang, B. Karrer, S. Sheynin,\net al. arXiv preprint\narXiv:2309.02591 , 2(3), 2023. 3\n[97] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. arXiv preprint arXiv:2205.01068 , 2022. 3\n[99] C. Zheng, T.-L. Vuong, J. Cai, and D. Phung. \u2022The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings. \u2022The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). In general, empirical results often\ndepend on implicit assumptions, which should be articulated. 4.Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)? We upload the codes and\ninstructions to recover the results. \u2022If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed. 5.Open access to data and code\n21Question: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial? We upload the codes\nand instructions to recover the results. \u2022The instructions should contain the exact command and environment needed to run to\nreproduce the results. \u2022Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted. 5 and Appendix 4. \u2022The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them. \u2022The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper. \u2022If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text. 9.Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? \u2022Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters. \u2022We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort. Answer: [Yes]\nJustification: Yes, we credited them in appropriate ways. \u2022The authors should state which version of the asset is used and, if possible, include a\nURL. Their licensing guide can help determine the\nlicense of a dataset. \u2022Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. \u2022Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper. If you obtained IRB approval, you\nshould clearly state this in the paper. 26",
        "Conclusion": "3. Lastly, we showcase V AR\u2019s zero-shot generalization capabilities in tasks like\nimage in-painting, out-painting, and editing. Zero-shot generalization. , x t\u22121). , x t\u22121). , x t\u22121). , r K). , r K), each at a increasingly higher resolution hk\u00d7wk, culminating in rK\nmatches the original feature map\u2019s resolution h\u00d7w. 4. 1). , r k\u22121). V AR transformer. The results are summaried in Tab. 1 and Tab. 2. RCG (cond.) 2. V AR also maintains comparable precision and recall. 6. 3. Effectiveness and efficiency of V AR. Norm. We\ntherefore foresee a promising future for exploiting V AR models in the realm of video generation. 8 Conclusion\nWe introduced a new visual generative framework named Visual AutoRegressive modeling (V AR) that\n1) theoretically addresses some issues inherent in standard image autoregressive (AR) models, and\n2) makes language-model-based AR models first surpass strong diffusion models in terms of image\nquality, diversity, data efficiency, and inference speed. 7. 8. D Time complexity of AR and V AR generation\nWe prove the time complexity of AR and V AR generation. Lemma D.1. Proof. Lemma D.2. Proof. (22)\nThis completes the proof. Advances in neural\ninformation processing systems , 35:23716\u201323736, 2022. A. Efros. Bar, Y . OpenAI , 2024. Advances in neural information processing\nsystems , 33:1877\u20131901, 2020. PMLR, 2020. Ieee,\n2009. Advances in neural information\nprocessing systems , 34:8780\u20138794, 2021. 3\n[32] Y . Shan. 3\n[33] Y . Shan. 3\n[42] Y . Advances in Neural Information Processing Systems , 34:852\u2013863, 2021. Lo, et al. 2, 7\n[53] T.-Y . Ieee, 1999. PMLR, 2021. PMLR,\n2021. PMLR, 2017. Advances in Neural Information Processing Systems , 35:36479\u201336494, 2022. 3\n[73] V . 3\n[77] Y . 3\n[78] Q. 3\n[79] Y . Zhao, Y . Lu, et al. Qiao, et al. Wu. Hao, I. Essa,\net al. Lin, et al. Advances in Neural Information Processing Systems , 35:23412\u201323425, 2022. Answer: [Yes]\nJustification: Yes. 1. Also see Sec. necessary to understand the\nresults? 1 and Tab. 2.",
        "label": 4
    },
    {
        "Abstract": "id= pxdSm7PW5Q\nAbstract\nGraph generation is integral to various engineering and scientific disciplines. Diffusion and score-based models represent generative models that\nsample new data by iteratively denoising a simple, pre-defined distribution (Sohl-Dickstein et al., 2015; Song\n& Ermon, 2019; Ho et al., 2020). Diffusion processes are the solutions of SDEs of the form,\ndx=f(x,t)dt+gdw, (1)\nwheref(\u00b7,t) :Rd\u2192Rdis the drift coefficient, g\u2208Ris the diffusion scalar, and wis a standard Wiener process. A diffusion process on\n{Gt= (Xt,Et)}T\nt=0is given by the SDE,\ndGt=f(Gt,t)dt+gdw, (5)\nwherefis the drift transformation on a set of graphs G, i.e.,f(\u00b7,t) :G\u2192G. 6, we abuse the notation that appeared in Eq. The graph convolutional network (GCN) and graph multi-head attention\n(GMH) are given by\nGCN (Xt,Et) =H({GNM (Xt,Et)j},J,M\u03c6), (10)\nGMH (Xt,Et) :=H({ATTN (Xt,Et)j},J,M\u03d5). The nodes do not contain any information. We consider GDSS-E to be a vanilla model\nwithout any of the components that we proposed in this work. Then, to ablate our two model components,\nwe separate each and add them to the vanilla baseline model. Nucleic acids research ,\n32(suppl_1):D431\u2013D433, 2004. Empty walkable nodes are\ndisplayed in light blue. (d)Empty (E): Measures the absolute distance between the average number of regular cells. However, this is not our primary focus, and we leave\nsuch exploration for future research.",
        "Methodology": "Nevertheless,\nexisting methodologies tend to overlook the generation of edge attributes. However, we\nidentify critical applications where edge attributes are essential, making prior methods\npotentially unsuitable in such contexts. To address this, we propose a joint score-based model\nof nodes and edges for graph generation that considers all graph components. Our approach\noffers three key novelties: (1)node and edge attributes are combined in an attention\nmodule that generates samples based on the two ingredients, (2)node, edge and adjacency\ninformationaremutuallydependentduringthegraphdiffusionprocess, and (3)theframework\nenables the generation of graphs with rich attributes along the edges, providing a more\nexpressive formulation for generative tasks than existing works. We evaluate our method on\nchallenging benchmarks involving real-world and synthetic datasets in which edge features\nare crucial. Furthermore, we propose a novel application that greatly benefits from the method due to its\nnature: the generation of traffic scenes represented as graphs. Our method outperforms other\ngraph generation methods, demonstrating a significant advantage in edge-related measures. The primary goal is to understand a large database\u2019s inherent distribution, enabling new samples to be\ngenerated. The exploration of generative modeling is a longstanding endeavor, marked by the development of various\nmethodologies throughout the years, including variational autoencoders (Kingma & Welling, 2014), adversarial\nlearning (Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and diffusion models\n(Sohl-Dickstein et al., 2015). The involvement of several components in the graph\nwhose attributes have a mutual interplay introduces the challenge of modeling the components altogether\nalong with their relations. In order to address this challenge, a discrete diffusion approach for generating\ncategorical node and edge attributes was proposed (Vignac et al., 2023). However, extending it to sampling\nreal-valued attributes remains non-trivial. Yet, while a discrete diffusion process fits well in certain cases, we\nadvocate in this work the consideration of a more general problem with continuous score-based frameworks. We address these limitations with a unified framework that enables\nthe generation of node and edge attributes. One prominent feature is the existence of edge attributes that introduce\nrelative and interactional information between the road participants. Without\nedge features, this information would have to be redundantly stored at the node level, increasing complexity\nand obscuring transition flow. However, this formulation only associates attributes with the nodes, leaving\nthe edges free from additional attributes besides the adjacency matrix. As we find critical tasks where edge\nattributes are dominant, we want to promote awareness of this problem to enable the generation of graphs with\nricher information and more expressiveness than the current formulation of attributed graphs. Consequently,\nin this study, we leverage the evident insight that edge attributes convey neighborhood information and\nprovide instrumental data absent in the adjacency matrix crucial for generating edge attributes. We propose\nto encode graph distributions via a joint Stochastic Differential Equation (SDE), describing the evolution\nof node and edge attributes. Importantly, our technique jointly solves for graph elements. Consequently, it\nbenefits from the synergetic connections between nodes and edges. In comparison, GDSS (Jo et al., 2022)\nproposed a similar diffusion system for adjacency and nodes that opt for a separated solution that may be\nsub-optimal in encoding certain graphs. In practice, this is achieved by combining node, edge, and adjacency information\nin an attention module, maximizing the mutual interplay of the graph components. Overall, our approach is\ndesigned to maximally exploit the information encoded in the nodes and edges and their interactions. We consider challenging benchmarks with important edge features to evaluate our approach. Specifically, we introduce a new synthetic dataset of grid mazes whose graphs\nare based on Markov decision processes. Additionally,\nedges are attributed with relative information such as distance, velocities, relative accelerations, and a flag\nindicating whether the vehicle\u2019s driver, represented by the source node, looks at the vehicle represented by\nthe target node. Our\nmain contributions can be summarized as follows:\n\u2022We extend the graph generation task to enable the generation of more expressive graph structures\nby formulating the graph generation with edge attributes. We propose a joint SDE framework for\ngenerating graphs with this information and demonstrate the importance of generating all graph\ncomponents encompassing multiple node and edge attributes. We advocate for a comprehensive benchmark\nfor edge-based graph generation and lay the groundwork for future research on integrating multiple\nedge attributes into various graph-based applications. \u2022We introduce a novel inductive bias for score-based models in graph generation, leveraging a newly\nformulated SDE approach that captures the interplay between edges and nodes. \u2022We thoroughly evaluate our approach on diverse benchmarks and conduct ablation studies. (2021) showed that these methods can be described in a unified\nframework of SDEs. The\ndiffusion process consists of the forward pass, where noise is gradually added to the data until it converges to\na normal Gaussian distribution, and the reverse pass, where a backward SDE is integrated using a denoising\nmodel. New samples are defined as the convergence points of the reverse pass. (2022) suggest discrete perturbations\nof the data distribution through a denoising diffusion kernel. Others proposed a discrete diffusion process that utilize graph sparsity to\ngain efficiency (Chen et al., 2023). While it is argued that discrete modeling of graphs may be beneficial, it is\nunclear how to sample real-valued attributes in existing frameworks. To this end, several works have proposed score-based methods for graph generation, though they\noften face limitations in modeling edge attributes. (2020) introduced a permutation-invariant\nmodel based on graph neural networks (GNNs) for learning data distributions of adjacency matrices. To\nextract binary neighborhood information, the real-valued diffusion output is discretized via thresholding. Subsequently, GDSS (Jo et al., 2022) uses separate stochastic differential equations to model node attributes\nand the adjacency matrix independently. Yet, this separation may limit the information exchange between\nnodes and adjacency. Recently, SwinGNN (Yan et al., 2023) proposed a non-invariant approach that permutes\nthe adjacency matrix for graph generation. While this method addresses the permutation invariance, it still\nfails to generate edge features, which remain unmodeled in this framework. Following the success of graph score-based models, we are motivated to further extend this framework to\ninclude edge features. While\ntrivial extensions exist, we find them to be unsatisfactory in solving even simple edge feature generation\ntasks, let alone challenging graph benchmarks. Instead, we address this problem by proposing a joint SDE for\nall graph components, combined with a dedicated GNN architecture to exploit edge features. (2018) offered a decomposition for relational data. In Gong & Cheng (2019), the authors\nexploit edge features via a doubly stochastic normalization. (2021) extended GNNs to\nhandle edge features and node features. We adhere to standard notations and denote the probability density of x(t)aspt(x), and the transition kernel\nfrom x (s)to x(t)fors<taspst(x(t)|x(s)). 2 from time Tto time 0allows an effective sampling from p0. 9) layers. score-based models (Song & Ermon, 2019) train an estimator s\u03b8(x,t)with the objective of\nmin\n\u03b8Et{Ex0,xt/bracketleftbig\n|s\u03b8(xt,t)\u2212\u2207 xtlogp0t(xt|x0)|2\n2/bracketrightbig\n}. (3)\n4 Method\nOur method for generative modeling of graphs is based on two novelties. First, we propose a joint score-based\nmodel of node and edge attributes (Sec. The sample\u2019s score is evaluated for all graph components jointly. With these\ntwo key ingredients, we achieve a modeling of graphs as a whole. B.1 for a detailed formulation. 4 is particularly suitable for the datasets considered in our study. In both cases, \u03f5serves to remove insignificant connections, improving the model\u2019s\nfocus on meaningful relationships. We would like to generate new graphs G\u223cp0, which we achieve by defining a diffusion process from p0\ntopT(and back), as we describe below. We follow the general outline in Sec. 1 and Eq. 3, the score \u2207Gtlogpt(Gt)is estimated using a\ngraph neural network whose objective is\nmin\n\u03b8Et{Ep0,p0t(Gt|G0)/bracketleftbig\n|s\u03b8(Gt,t)\u2212\u2207Gtlogp0t(Gt|G0)|2\n2/bracketrightbig\n}, (7)\nwhereG0\u223cp0,Gt\u223cp0t(Gt|G0). However, they use separate processes for the nodes and\nadjacency instead of solving the joint SDE. On the other hand, we aim to solve it jointly for nodes and edges, allowing them to\ninteract during the diffusion process through the score calculation. 4.2 Node and Edge-Dependent GNN\nSimilar to existing works (Niu et al., 2020; Jo et al., 2022), we adopt the framework of Graph Neural\nNetworks (Wu et al., 2020). This architecture maintains permutation equivariance, ensuring that the model\nlearns a desired permutation-invariant distribution (Niu et al., 2020). Given an intermediate estimation of node and edge attributes, denoted by Xt\nandEt, respectively, the GNM module is defined via\nGNM (Xt,Et) := \u00afAtXtWX+ tanh(B[rep(\u00afAt)\u2299EtWE]), (8)\nwhere\u2299is the element-wise product, WX,WEare neural network weights. B[\u00b7]sums the values of each node\nincoming edges feature-wise and the operator rep(\u00b7)takes a matrix and repeats it vtimes along the third\ndimension. The GNM module learns how to propagate information to\neach node from its neighboring nodes while also absorbing information from incoming and outgoing edges. Essentially, the left side of the addition learns to propagate node features to other nodes and edges, and the\nright side of the addition does so for the edge features. It\nfacilitates efficient information propagation through both nodes and edges. We utilize the GNM and ATTN modules to construct our full graph neural\nnetwork to compute the score s\u03b8(Gt,t). To simplify notation, we define H({hj},J,M )as the module that\ntakes a collection of vectors {hj}withJelements, concatenates them, and feeds the result through a multilayer\nperceptron (MLP) M:\nH({hj},J,M ) :=M/parenleft\uf8ecig\nconcat [hj]J\nj=1/parenright\uf8ecig\n. (9)\nJis determined by a hyper-parameter and plays a role in enabling the model to capture multiple propagation\nflows at each level of the graph neural network. Then, we define two components that will be used to generate\n6Published in Transactions on Machine Learning Research (1/2025)\nthe node and edge attributes. (11)\nThe GCN and GMH components compress information across Jdifferent activations. (12)\nTo estimate the aforementioned score function, we use feed-forward neural networks FXandFEas follows. Given initial node and edge attributes, denoted as X1\nt=XtandE1\nt=Etrespectively, the model sequentially\nalters its inputs as they pass through the layers by\nXl\nt:=Fl\nX(Xl\u22121\nt,El\u22121\nt)\u2261GCN (Xl\u22121\nt,El\u22121\nt), (13)\nEl\nt:=Fl\nE(Xl\u22121\nt,El\u22121\nt)\u2261GMH (Xl\u22121\nt,El\u22121\nt). (14)\nHere,Xl\nt,El\ntdenote the node and edge attributes representing the output of the l-th layer,l\u2208[1,L]. It\nincorporates multiple hierarchical latent representations, enabling the model to capture multiple propagation\nsteps and different levels of information abstraction. We also considered using GMH instead of GCN to\ncompute the node scores; however, we found that both approaches yielded similar performance. Conversely, we found that utilizing GMH was crucial for achieving\nsuperior results for calculating edge scores. 8 allows for a proper digestion of the edge information by the score network and\nits propagation through the entire score model. We consider this formulation to be crucial for estimating\nthe score, as demonstrated in Sec. Specifically, both GMH and GCN modules introduce an O(n2)\ncomponent for time that scales linearly with the number of attention heads and the input/output feature\ndimensionalities. Storage for edge features requires O(n2v), withvbeing the number of features. Thus, our\napproach is similar in time and memory complexity to other state-of-the-art models such as DiGress (Vignac\net al., 2023) and GDSS (Jo et al., 2022). 5 Experiments\nWe tested our qualitative and quantitative methods on diverse real-world and synthetic dataset benchmarks. The objective of the model is to learn from observed graphs the underlying distribution and be able to\ngenerate new unseen graphs that follow the same distribution. 5.1 that incremental modifications of\nGDSS are ineffective, highlighting the non-trivial importance of our approach. To the best of our knowledge,\nwe are the first to tackle this task via graph generation. \u2022We evaluate the score estimation quality (Sec. Our goal is to demonstrate that the current baseline, even with incremental adjustments for\nedge feature generation, is ineffective for this task, whereas our approach effectively learns both inter- and\nintra-edge feature attributes as intended. We utilize a synthetic dataset of 1000 complete graphs with ten nodes each, with only edge\nattributes. Baseline and variants. 7) as joint-SDE-Model , and the baseline with our GNM model (Eq. 8) as GNM-Based-Model . Note that all the variant\u2019s score\nestimations are similar to the base model. 3 contains scatter plots of the edge distributions for the different baselines. A good model should generate the same\nvisual clustering as the ground truth. Further, to evaluate the edge features quantitatively, we consider the\nhomogeneity of edges. We check the percentage of graphs that are homogeneous, meaning all edges in the\ngraph belong to a single cluster, as in the real data. We detail the homogeneity score above each plot, where good\nmodels should yield 100%as the ground truth. 3b shows that GDSS-E roughly approximates the distribution. However, the two clusters\nappear blurred, making it difficult to differentiate between them. In addition, it fails to learn the homogeneity\ncharacteristics of the data. Alas, it fails to yield a fine-grained generation as some samples are outside the original\ndistribution (e.g., the points around 0). Further, this model also fails in the homogeneity task. 3d) generates blurred and non-separated clusters. Nevertheless, it successfully models\nhomogeneous edge features, achieving a 99%homogeneity score. 3e) demonstrates\na similar distribution to the ground truth in terms of separation and clusters\u2019 structure, as well as a perfect\nhomogeneity score. On the one hand, our joint SDE process\naccurately models the underlying distribution, but it struggles with preserving homogeneity, i.e., with\ninteractions between the graph edges. On the other hand, the GNM-based model succeeds in maintaining\n8Published in Transactions on Machine Learning Research (1/2025)\nTrain Data GDSS-E Ours\nFigure 4: A qualitative comparison between the original data (left), GDSS-E (middle), and our method\n(right) on the deterministic MDP grid maze dataset. Our graphs consistently have four blocks, one\nstart node, and one finish node, as required. homogeneous aspects, but it is challenged by the data distribution. We strive to explore the connection between generative modeling of graphs and\nMDPs. Indeed, access to many diverse RL environments is often limited in practice, and we aim to extend\nand diversify available environments. In this context, we introduce a new synthetic MDP dataset of grid mazes. Each grid has 5\u00d75cells (nodes),\nincluding a start cell and a finish cell and 25 cells in total. The agents take one action per cell {up, left, down,\nright}. The attributes on the edges are the continuous probabilities of moving from the current cell\nto one of its adjacent cells. Here, we consider two settings of using this data: (1) a deterministic grid maze,\nwhere edge features are binary in {0,1}; and (2) a non-deterministic grid maze, where edge attributes are\nprobabilities in [0,1], and the sum of all features per cell is one. Further details on these datasets and their\nMDP graphs are provided in App. deterministic non-deterministic\nMethod deg\u2193cl\u2193un\u2191no\u2191MV\u2191VS\u2191B\u2193SF\u2193E\u2193deg\u2193cl\u2193un\u2191no\u2191MV\u2191MDV\u2191VS\u2191B\u2193SF\u2193E\u2193\nGDSS-E 0.73 0.06 97 100 34% 9% 0 .96 1.28 2.230.40 0.02 99 100 6% 1% 26% 0 .390.83 0.4\nSwinGNN-E 0.44 0.064 100 100 17% 62% 2 1.68 3.690.51 0.05 100 100 21% 1 .9%72% 2.8 2.6 5.4\nOurs 0.17 0.006100 100 68% 34% 0.1 0.58 0.480.31 0.013100 100 38%6% 33% 0.020.88 0.8\n9Published in Transactions on Machine Learning Research (1/2025)\nto construct only a single edge value. To compare our approach against strong baselines, we consider the\nstate-of-the-art GDSS and modify it to GDSS-E as discussed in Sec. Furthermore, we use a variant of\nSwinGNN (Yan et al., 2023), a state-of-the-art score-based graph generative model, to generate multiple edge\nand node features as an additional baseline and denote it as SwinGNN-E. We don not consider DiGress as a\nbaseline due to significant disparities in the output format. DiGress (Vignac et al., 2023) solely produces\ndiscrete attributes, whereas our requirements necessitate continuous ones. Moreover, adapting DiGress\nto generate multiple edge attributes entails non-trivial modifications, rendering it unsuitable for direct\ncomparison in our experimental framework. We do not use the orbit metric since the grid maze MDPs are directed cyclic graphs. Additionally, we adapted\nthe uniqueness (un) and novelty (no) metrics (Martinkus et al., 2022) to evaluate the models\u2019 ability to\ngenerate graphs that differ from the training set and are distinct from each other. Further, we introduce five\nnew dataset-specific and edge-based metrics that measure the quality of generated graphs and edge features. Start and finish (SF) calculates the distance between the average number of start\nand finish cells. There is exactly one starting cell and one ending cell. Empty (E) computes the distance between the\naverage number of regular cells, which is always 19. For (B), (SF), and (E), the distance is defined as the\nabsolute difference between the fixed original number of cells and the corresponding values in the generated\ngraph. Our approach better captures the graph statistics measured by the degree\n(deg) and cluster (cl) metrics, showing a significant gap concerning GDSS-E and SwinGNN-E. Further, our\ngraphs\u2019 edge-based metric, MV, is twice the baseline result, i.e., 68%vs.34%. While its B, SF, and E metrics are relatively high, indicating that the model\nstruggles to grasp the statistical features of the graph nodes as required compared to our model. We also\npresent a qualitative comparison of the real training data, the generated graphs obtained with GDSS-E, and\nour approach. The colored nodes are blocks (blue) and start and finish cells (yellow and\ngreen, respectively). Our method yields a valid graph, respecting the correct number of blocks and start and\nfinish points. In contrast, GDSS-E has two starting points and five block nodes. The non-deterministic case. In this setting, where edge features are real-valued, edge features are valid if\ntheir MV measure is \u03f5= 0.001close to one. Further, the edge values follow a specific pre-defined distribution,\nand thus, in addition to MV, we also measure the MDP distribution validity (MDV). Namely, the percentage\nof edges that follow the distribution above. Methoddeg\u2193cl\u2193un\u2191no\u2191V\u2193O\u2193L\u2193CR\u2193LA\u2193\nGDSS-E 1.05 0.0315 34 3.90.660.96 0.5% 208\nSG-E 0.79 0.0139 42 0.76 1.27 1.08 0.6% 302\nOur 0.776e\u2212751 51 0 .360.80.080.3%194Learning traffic scenes can hugely contribute to au-\ntonomous driving. Edge features such as Euclidean distances and angles are optional. Primarily, the challenge is to predict a future trajectory from the history traces of a road participant. The\nlatter work was the first to utilize a graph representation of the scenes, where nodes represent agents and map\nelements, which are later processed via GNNs to predict the target. Specifically, we compute Maximum Mean Discrepancy\n(MMD) (Gretton et al., 2012) metrics for vehicles (V), objects (O), and lanes (L), as well as collision rate (CR)\nand lane alignment (LA). In particular, we assess MMD over the xandycoordinates of vehicle trajectories\n(V), lane curves (L), and other map objects (O). For\nmore details on these metrics and evaluation protocols, refer to (Tan et al., 2021). The top row shows the node losses,\nwhereas the bottom row shows the edge losses. We use blue and orange for the loss measures of our method\nand GDSS-E, respectively. While the node score losses are comparable for both models, our method yields\nsignificantly better edge losses. We perform our quantitative ablation over deterministic MDP (MDP-D), non-deterministic\nMDP (MDP-ND), and nuScenes. Method Planar SBM MDP-D MDP-ND nuScense\ndeg\u2193cl\u2193orb\u2193deg\u2193cl\u2193orb\u2193deg\u2193cl\u2193MV\u2191deg\u2193cl\u2193MV\u2191deg\u2193cl\u2193LA\u2193\nGDSS-E 0.945 0.96 0.660.74 1.57 0.250.73 0.06 34% 0.40 0.02 26% 1.05 0.03208\nJoint-SDE-Model 1.02 0.94 0.260.21.04 0.050.71 0.05 57% 1.67 0.54 2% 1.4 0.02 243\nGNM-Based-Model 0.038 0.95 0.220.34 0.7 0.050.23 0.02 55% 0.35 0.07 33% 0.99 1e\u22125179\nOurs 0.025 0.38 0.230.460.63 0.040.17 0.006 68 %0.31 0.013 33 %0.77 6e\u22127194\nomit some metrics due to space constraints. Further, we find that only incorporating edge-based GNM, leads to inconsistent behavior. However, jointly modeling node and edge attributes attains a notable gain in error metrics. Although our study focuses on edge-important graph benchmarks,\nwe apply our method to general graph generation tasks and extend the ablation study to show the robustness\nof our model to different diverse datasets. To leverage the edge attribute abilities of our model, we augment\nevery graph with edge attributes per edge. Specifically, we compute the n-th power of the adjacency matrix,\nand then, for each edge eabbetween nodes aandb, we assign the corresponding value encoded in the power\nmatrix. The edge features contain the number of paths between aandbwithnsteps, where we set n= 2. Further, score-based methods utilize a separate diffusion process\nper graph element, which limits the interaction between the sampled components. This work suggests a\njoint score-based model for node and edge features. Our framework maximizes learning from graph elements\nby combining node and edge attributes in the attention-building block module. Moreover, node, edge, and\nadjacency information are mutually dependent by construction in the diffusion process. We extensively test\nour approach on multiple synthetic and real-world benchmark datasets compared to recent strong baselines. Further, we introduced a new synthetic dataset for benchmarking edge-based approaches. In the future, we aim to incorporate certain inductive biases into the generation pipeline. Additionally, generating\nnew samples with diffusion frameworks is costly and difficult to scale to large graphs. We plan to address\nthese limitations by enabling variation in the number of nodes within the diffusion process, thus allowing for\nnon-fixed-size graphs. Accurate learning of graph representations with graph\nmultiset pooling. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,\nYu Pan, Giancarlo Baldan, and Oscar Beijbom. Nachiket Deo, Eric Wolff, and Oscar Beijbom. Multimodal trajectory prediction conditioned on lane-graph\ntraversals. Generative\ndiffusion models on graphs: Methods and applications. Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid. Exploiting edge features for graph neural networks. Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel\ntwo-sample test. The Journal of Machine Learning Research , 13(1):723\u2013773, 2012. Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. A systematic survey on deep generative models for graph generation. Denoising diffusion probabilistic models. A survey on trajectory-\nprediction methods for autonomous driving. Score-based generative modeling of graphs via the system\nof stochastic differential equations. ByeoungDo Kim, Seong Hyeon Park, Seokhwan Lee, Elbek Khoshimjonov, Dongsuk Kum, Junsoo Kim,\nJeong Soo Kim, and Jun Won Choi. arXiv preprint\narXiv:1412.6980 , 2014. Semi-supervised classification with graph convolutional networks. In International Conference on Machine Learning, ICML , volume\n202 ofProceedings of Machine Learning Research , pp. Valentin Lemaire, Youssef Achenchabe, Lucas Ody, Houssem Eddine Souid, Gianmarco Aversano, Nicolas\nPosocco, and Sabri Skhiri. In Proceedings of the\n15th Asian Conference on Machine Learning , volume 222 of Proceedings of Machine Learning Research , pp. Mengmeng Liu, Hao Cheng, Lin Chen, Hellward Broszio, Jiangtao Li, Runjiang Zhao, Monika Sester, and\nMichael Ying Yang. Sample and predict your latent: modality-free sequential\ndisentanglement via contrastive estimation. Utilizing image\ntransforms and diffusion models for generative modeling of short and long time series. Generative modeling\nof regular and irregular time series data via koopman vaes. Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation\ninvariant graph generation via score-based generative modeling. Modeling relational data with graph convolutional networks. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Generative modeling by estimating gradients of the data distribution. Score-based generative modeling through stochastic differential equations. Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Kraehenbuehl. A survey on deep\ngraph generation: Methods and applications. While prior works focused on sampling graphs from p0that contain only node features, that\nare graphs with limited expressiveness in the sense that edge information depicts solely the graph topology,\nsampling high-dimensional edge features is somewhat overlooked, disabling the ability to learn distributions\np0that contain edge features. Our work focuses on learning a model Mthat will learn to generate new\nsamples from observed p0containing node and edge features. We propose an innovative link between graph generation techniques and Markov Decision\nProcesses (MDPs). Thus, we are\nmotivated to create a new dataset whose graphs contain node and edge attributes and are directed graphs. Such data will diversify the common standard benchmarks today that include undirected graphs and contain\nonly one type of feature, either for nodes or edges. We create two variants of the MDP grid maze dataset: deterministic and non-\ndeterministic. In both settings, the grid is the same. However, the probability of an action is different. In\nboth datasets, the graph contains 25nodes. There are four actions:\nA={left,up,right,down }. Note, a valid MDP is where for all state u\u2208S,v\u2208Sand actions a\u2208Athe\nsum of all actions:/summationdisplay\na\u2208A\nv\u2208Sp(v|u,a) = 1. (16)\nAn equivalent constraint is that all node\u2019s outgoing edges uwill sum to 1. Therefore, there\nis an edge toward blocks with probability zero. (2) Block cells have no outgoing edges. (3) The grid perimeter is closed; thus, moving outside the grid is impossible. The values of the edge features are determined by the defined probabilities of p(v|u,a), which we will\nexplain later for both deterministic and non-deterministic setups. The yellow cell is the starting point, the green\ncell is the ending point, dark blue is the block cells, and the gray cells are neutral. We showcase the four edge feature channels\nalong with their corresponding values. In practice, each edge in the data represents the probability per action in an arbitrary\ncoordinate order. Blocks are represented as dark blue squares, while start\nand finish nodes are marked in yellow diamond and green triangles, respectively. The yellow cell is the starting point,\nand the ending is the green cell. In the center, a structured graph represents\nthe grid by the rules described in the appendix. On the right, the same grid is represented by a different\npermutation of the nodes. Non-deterministic edges. MDPs are sometimes non-deterministic. That means, given a state and a\ndesired action, it is only sometimes guaranteed to succeed. We create the non-deterministic dataset variation\nto simulate this setup and challenge the edge attributes generation. Still, the\noutgoing sum of edges from a certain node must sum up to one to be a valid MDP. Further, we decided to\napply the next arbitrary distribution over the edge. Denote |eu\nout|=zuas the number of outgoing edges. We will provide the complete code for generating the grids and their corresponding\nMDPs. The generator enables control of grid size, number of ending points, and number of blocks. First, our grids are 5\u00d75. We generate 1000 valid grids and split them into 80%training\nand20%for testing and validations. 19Published in Transactions on Machine Learning Research (1/2025)\nEvaluation protocols Besides the general graph generation metrics for directed graphs (degree, clustering,\nnovelty, and uniqueness), we consider several dataset-specific metrics to evaluate node and edge generation\nquality. 1.Edges\n(a)MDP Validity (MV): Check if the sum of the outgoing edges of a node is one. It is a constraint\nof an MDP that the sum of probabilities is one. In the non-deterministic setup, the values are\ncontinuous, and therefore, we use an \u03f5= 0.01gap from one. Note normalization of the edges\ncould be done to fix this constraint if necessary. However, we evaluate the hard constraint to\nmeasure the model\u2019s ability to capture the edge feature distributions. 2.Nodes\n(a)Valid Solution (VS): Measures if a generated grid is valid: has start and finish cells, and the\nroute is not entirely blocked. (b)Blocks (B): Measures the absolute distance between the average number of blocks in the grids. In this work, the used dataset has a ground-truth average of four. For example, if a generated\ngraph contains seven blocks, the distance will be 3, as the original distribution has only 4 blocks. The model is expected to match this target distribution. In this\nwork, the dataset has a ground truth of 19 empty cells, and, similar to the previous two metrics,\nthe generated graph should match this number. VectorNet (Gao et al., 2020) was the first to utilize a sparse\ngraph representation of the scenes, where nodes represent agents and map elements, which are later processed\nvia GNNs to predict the target. For this purpose, we extract a portion of 746samples of traffic scenarios from the mini_train split as defined\nin the nuScenes-devkit and use them for training and evaluation with 80%,20%train, test split. We transform the graph into a radius graph of 30 meters and only\npreserve edges representing a relation whose target is an agent, e.g., lane-to-agent. Additionally, we evaluate the Collision Rate\n(CR), which measures the rate of collisions between generated agents, and the Lane alignment (LA), which\nsums up the distances between each agent\u2019s trajectory to the closest generated lane. (2021) for more details about the metrics and evaluation protocols. To evaluate our and other methods, we repeat the protocol presented in Vignac et al. (2022) and refer to them to learn more about the metrics and the evaluation protocols. (2022) for more information\nabout the evaluation process and protocols. These graphs have communities ranging from two to five, with each community containing\nbetween 20 to 40 nodes. Validity is determined based on the number of communities, the number of nodes in each\ncommunity, and a statistical test as done in Martinkus et al. The criteria\nfor a valid graph within this dataset necessitate a two-fold condition: 1) the graph is connected,\nensuring that every node has a path to every other node, and 2) the graph must exhibit planarity,\nmeaning you can draw it on a two-dimensional plane without any edge crossings. B.3 Limitations\nOur approach exhibits superior performance compared to other methods on the MDP and nuScence datasets. We anticipate that an effective generative method would autonomously learn and adhere to such\nconstraints. We believe that incorporating inductive bias regarding specific tasks can contribute to those\ntasks. Moreover, while score-based and diffusion methods typically demonstrate optimal performance across\nvarious downstream tasks in graph generation, scaling these methods to very large graphs containing millions\nof nodes and edges presents a challenge yet to be fully addressed. 4 in the main text to describe the method architecture and implementation details. We\npresent the hyperparameters per dataset in Tab. We used Adam (Kingma & Ba, 2014) optimizer and\nthe same learning rate of 0.01, weight decay of 0.0001, and EMA of 0.999for all datasets. In addition, for\nsampling, we used the Euler predictor, Langevin corrector, signal-to-noise-ratio (SNR) of 0.05, scale epsilon\nof0.7, and sampling with 1000 steps for all datasets. Finally, we use a single VP forward diffusion process\n(Song et al., 2021) stochastic differential equation for both the nodes and the edges. We will publish the complete code, including the datasets,\nevaluation protocols, and experiment environment, upon acceptance. However, none of the other methods\nare adapted to generate multiple continuous edge features and directed graphs. Therefore, to create a solid\nbaseline for directed, multi-edge attribute graph datasets, we create an extension of GDSS called GDSS-E. In their paper, the authors adopt a general diffusion SDE modeling approach\nsimilar to that in Eq. Briefly, due to the multiple SDE equation formulation, their model employs two separate Graph\nNeural Networks (GNNs): one for learning the score of the adjacency matrix and another for learning the\nnode features. In contrast, our approach suggests using a single GNN to jointly model both graph components. Aside from this key difference, our architecture is constructed similarly to GDSS, with the addition of an\nedge-information propagation mechanism within the GNN\u2019s building block, as outlined in Sec. To adapt GDSS to a directed graph, we need to delete the symmetry inductive bias\nof the method. In addition, the generated noise and the whole diffusion process are set to be symmetric,\nmeaning they generate symmetric noise patterns. Therefore, we change all aspects of the diffusion and\ngeneration process to be a normal Gaussian injection, similar to regular diffusion methods. We adjust the network parameter\nto returnE\u2208RN\u00d7N\u00d7Cwhere the first feature is the adjacency information, and the rest of the channels are\nthe attributes of the edges. We will publish this implementation code in the project code. We construct similar changes as\nwe did for GDSS-E regarding directed graphs, and for multiple edge features, we modify the one-hot encoding\nconfiguration to work with the original data instead. We follow the same training protocols Yan et al. (2023)\nimplemented for other similar datasets. Following\nour model architecture, all our arithmetic actions are edge-wise, node-wise, or GNN-wise. The drawback is that if graphs in the observed dataset have few permuted representations,\nit significantly damages the model generation quality. By employing our innovative joint SDE rather than multiple SDE\u2019s like in GDSS, our\nmodels demonstrate superior performance in terms of both time efficiency and memory consumption compared\nto GDSS-E.\nTable 5: Time and memory comparison. Time is the amount of time to train each model, both models\ntrained on same device with the same seed and number of training epochs. Method nuScenes MDP-D\nTime(Sec)\u2193Memory(MB)\u2193Time(Sec)\u2193Memory(MB)\u2193\nGDSS-E 23,004 2 ,122 8,034 2 ,485\nOurs 15,337 1 ,926 8,014 1 ,719\nC.2 Impact of increasing edge-feature size on adjacency matrix estimation. Our method masks the adjacency matrix (Eq. 4), questioning the impact of increasing edge feature size\non adjacency estimation. The Degree metric reliably compares adjacency matrices across different graph sizes, unlike\nthe Cluster metric. In nuScenes, our method achieves a Degree score of 0.77, while in MDP, it\u2019s around five\ntimes smaller at 0.17. We hypothesize that this is due to nuScenes\u2019 more complex node feature distribution\ndespite MDP having twice the number of edge features (2 vs. 4). Method Planar SBM Ego Small Grid\ndeg\u2193cl\u2193orb\u2193deg\u2193cl\u2193orb\u2193deg\u2193cl\u2193orb\u2193deg\u2193cl\u2193orb\u2193\nSPECTRE 1.42 1.35 1.332.12 1.37 0.510.046 0.14 0.73* * *\nGraphVAE 0.87 1.13 0.831.41 0.97 0.520.13 0.23 0.052 1.48 00.87\nEDP-GNN 0.985 1.29 0.971.1 1.43 0.880.062 0.0970.009 0.45 0.32 0.51\nDiGress 1.36 0.97 1.471.16 1.32 1.160.12 0.17 0.035 0.87 0.03 1.28\nGDSS 0.945 0.96 0.660.74 1.57 0.250.025 0.087 0.015 0.37 0.01 0.42\nOur w.o. The\ngenerated edge features we use are arbitrary, and incorporating other engineered features on the edges, such\nas spectral features, could further improve our model. Specifically, we compute the n-th power of the adjacency matrix, and then, for each edge eabbetween nodes a\nandb, we assign the corresponding value encoded in the power matrix. The edge features contain the number\nof paths between aandbwithnsteps, where we set n= 2. We compare our method with strong graph generation baselines: SPECTRE (Martinkus et al., 2022),\nGraphVAE (Simonovsky & Komodakis, 2018), EDP-GNN (Ho et al., 2020), DiGress Vignac et al. We follow the training and evaluation protocols detailed in Jo et al. Our model achieves state-of-the-art performance in several\ncases. In particular, in complex and large graphs such as SBM, Planar, and Grid, our method is a strong\ncompetitor in terms of the degreemetric compared with other methods. 12 to show the robustness of our model. In addition, to make a fair comparison, we executed the\nbenchmark with our model, excluding edge features. Our method achieves slightly lower results when edge\nfeatures are not utilized. B.2.3 to learn about the\ndataset, evaluation protocol, and metrics. Importantly, we emphasize that DiGress is explicitly designed to handle the generation\nof edge types as it leverages transition kernels. Table 9: Deterministic MDPs with standard deviation. Method deg\u2193 cl\u2193 un\u2191no\u2191MV\u2191 VS\u2191 B\u2193 SF\u2193 E\u2193\nGDSS-E 0.73\u00b10.025 0.06\u00b10.002 97\u00b11 100\u00b10 34%\u00b11% 9%\u00b12% 0.96\u00b10.16 1.28\u00b10.05 2.23\u00b10.16\nOur 0.17\u00b10.02 0 .006\u00b10.001 100\u00b10 100\u00b1068%\u00b11% 34%\u00b14%0.1\u00b10.04 0 .58\u00b10.13 0 .48\u00b10.18\nTable 10: Non-deterministic MDPs with standard deviation. Method deg\u2193 cl\u2193 un\u2191o\u2191 MV\u2191MDV\u2191 VS\u2191 B\u2193 SF\u2193 E\u2193\nGDSS-E 0.40\u00b10.02 0.02\u00b10.01 99\u00b11 100\u00b10 6%\u00b11% 1%\u00b10.5% 26%\u00b12% 0.39\u00b10.03 0.83\u00b10.1 0.4\u00b10.1\nOur 0.31\u00b10.01 0 .01\u00b10.001 100\u00b10 100\u00b1038%\u00b12%6%\u00b10.5%33%\u00b11%0.02\u00b10.02 0.88\u00b10.1 0.8\u00b10.01\n25Published in Transactions on Machine Learning Research (1/2025)\nTable 11: Quantitative metrics on nuScenes with standard deviation.",
        "Results and Findings": "Published in Transactions on Machine Learning Research (1/2025)\nReviving Life on the Edge: Joint Score-Based Graph Genera-\ntion of Rich Edge Attributes\nNimrod Berman\nBosch, Ben-Gurion University\nEitan Kosman\nBosch\nDotan Di Castro\nBosch\nOmri Azencot\nBen-Gurion University\nReviewed on OpenReview: https: // openreview. Additionally, we introduce a new synthetic dataset that incorporates edge values. These approaches have been used to generate various information types\n1Published in Transactions on Machine Learning Research (1/2025)\n(Dhariwal & Nichol, 2021; Naiman et al., 2024b;a) and solve many downstream tasks (Ho et al., 2022; Naiman\net al., 2023; Berman et al., 2024). Encoding these properties directly on edges provides a compact, interpretable\nrepresentation that preserves the MDP\u2019s natural structure. between grid cells. We establish new links between graph\ngeneration in MDPs and real-world traffic scenarios. Our\nresults demonstrate superior performance over baseline models, excelling across various standard\nevaluation protocols for graph generation tasks, particularly in edge metrics. While several graph generative\nframeworks exist (Wu et al., 2020; Zhou et al., 2020), we focus on score-based approaches. 3Published in Transactions on Machine Learning Research (1/2025)\nDiscrete and continuous graph diffusion models. We show that\nour method greatly outperforms naive adaptations, demonstrating the necessity of each and every component\nwe introduce as a whole. A well-known result by Anderson (1982) shows\nthat the following reverse-time SDE is the reverse diffusion process,\ndx= [f(x,t)\u2212g2\u2207xlogpt(x)]d\u00aft+gd\u00afw, (2)\nwhere \u00afwis a reverse-time Wiener process, and d\u00aftdenotes an infinitesimal negative timestep. Importantly, the adjacency matrix A\u2208{0,1}n\u00d7n\ncan be recovered from Eby a simple mask such as\nA:=\u03c3(max\nk|Eijk|), (4)\nwhere\u03c3= 0if\u03c3(x)<\u03f5and else\u03c3= 1. We chose \u03f5= 0.01for all\ndatasets. As we show hereunder,\nit effectively filters out low-probability transitions for MDPs, while for traffic generation, small distances\ncorrespond to invalid links. The corresponding reverse-time\nSDE reads\ndGt= [f(Gt,t)\u2212g2\u2207Gtlogpt(Gt)]d\u00aft+gd\u00afw, (6)\n5Published in Transactions on Machine Learning Research (1/2025)\nwhere in Eq. Consequently,\nwe opted for GCN due to its simplicity. 5.2). 7Published in Transactions on Machine Learning Research (1/2025)\n0.5\n 0.0 0.50.5\n0.00.5\nHomogeneity: 100%\n(a)Ground-Truth\n0.5\n 0.0 0.5Homogeneity: 53.9%\n(b)GDSS-E\n0.5\n 0.0 0.5Homogeneity: 53.8%\n(c)Joint-SDE-Model\n0.5\n 0.0 0.5Homogeneity: 99%\n(d)GNM-Based-Model\n0.5\n 0.0 0.5Homogeneity: 100%\n(e)Ours\nFigure 3: Our ablation study shows that GDSS-E and the other variants yield inferior distributions compared\nto our approach concerning the ground-truth data distribution estimation. 5.1 Synthetic Dataset Ablation Experiment\nIn this section, we conduct an in-depth study to assess the performance gains from incorporating our different\ncomponents. Dataset. 3a. Additionally, the graph edges are homogeneous, i.e., they are all sampled from the same\ncluster (the upper right or the lower left). Refer to App. B.4.2 for more details. Evaluation. Fig. For this\nvisualization, we generate 50 complete graphs with 5000 edges in total. 3a. Then, we compute the average percentage of this test\nfor the 5000generated samples per method. Fig. 3c) presents improved results by estimating denser\nGaussian clusters. These results shed light on the importance of our components. There are also block cells that the agent cannot\ntraverse, while the remaining cells are empty and walkable. The nodes in the graph, each with a single feature, represent the discrete category of each cell {start, finish,\nblock, empty}. A unique characteristic of our MDP graphs is their multiple attributes per edge. 5.1. To quantitatively evaluate the graphs, we utilize common metrics such as the degree (deg) and cluster (cl). The results of our evaluation on grid maze MDPs are shown in Tab. These results emphasize\nour model\u2019s ability to capture edge attribute complexities. Fig. 1 show that our model performs\n\u22483times better than the baseline on the edge-related metrics, MV and MDV. These results affirm our\nframework\u2019s ability to capture and generate diverse and complex multiple-edge attributes effectively. Refer to App. B.2.2 for more details. We evaluate our model using standard graph metrics for directed graphs alongside established traffic generation\nprotocols (Tan et al., 2023; Feng et al., 2023). 2 presents our results\ncompared to the GDSS-E and SwinGNN-E (SG-E) baselines, demonstrating that our model outperforms these\nbaselines on most general and traffic-specific metrics. Notably, our model captures lane location statistics (L)\nwith approximately a tenfold improvement over the baselines. 5.4 Comparison of Score Losses\nIn this experiment, we compare the behavior of graph generation models in terms of the node and edge score\nlosses on the train and test sets. 104106Node lossTrain Losses\nOurs\nGDSS\n103105Test Losses\n101103\niterations102103Edge loss\n101103\niterations102103\nFigure 5: We plot the node and edge scores of GDSS-E and our model on the train (left) and test (right) sets. We report the results of the metrics deg, cl, and MV or LA in Tab. However, the trend is similar in those metrics as well. Our\nresults indicate that the proposed model obtains the best results across all datasets and metrics, except for\nLA in nuScenes. 3, we report the results on the Planar and SBM datasets. We observe a trend similar to the previous\nablation, and our method outperforms the other variants. Furthermore, we test our model on a\nreal-world molecule dataset at App. In both experiments, we show that our model can learn the graph\ndistribution better with edge features, and we achieve competitive results concerning solid baselines. Our results show\nthat exploiting edge information is instrumental to performance in general and in edge-related metrics. Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Jinheon Baek, Minki Kang, and Sung Ju Hwang. Nimrod Berman, Ilan Naiman, Idan Arbiv, Gal Fadlon, and Omri Azencot. Sequential disentanglement by\nextracting static information from A single sequence element. In Forty-first International Conference on\nMachine Learning, ICML , 2024. 11621\u201311631,\n2020. Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. arXiv preprint arXiv:2305.04111 , 2023. 203\u2013212. PMLR, 08\u201311 Nov 2022. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. arXiv preprint arXiv:2203.14500 , 2022. Wenqi Fan, Chengyi Liu, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, and Qing Li. arXiv preprint arXiv:2302.02591 , 2023. Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. In 2023 IEEE International Conference on Robotics and Automation (ICRA) ,\npp. IEEE, 2023. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Xiaojie Guo and Liang Zhao. 13Published in Transactions on Machine Learning Research (1/2025)\nKilian Konstantin Haefeli, Karolis Martinkus, Nathana\u00ebl Perraudin, and Roger Wattenhofer. arXiv preprint arXiv:2210.01549 , 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma,\nBen Poole, Mohammad Norouzi, David J Fleet, et al. arXiv preprint arXiv:2210.02303 , 2022. Yanjun Huang, Jiatong Du, Ziru Yang, Zewei Zhou, Lin Zhang, and Hong Chen. Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. In International Conference on Machine Learning , pp. 14636\u201314645, 2021. Diederik P Kingma and Jimmy Ba. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. Thomas N. Kipf and Max Welling. Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B. Aditya Prakash, and Chao Zhang. 678\u2013693. Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. arXiv preprint arXiv:1803.03324 , 2018. Jianbang Liu, Xinyu Mao, Yuqi Fang, Delong Zhu, and Max Q-H Meng. A survey on deep-learning approaches\nfor vehicle trajectory prediction in autonomous driving. 978\u2013985. IEEE, 2021. arXiv preprint arXiv:2302.13933 , 2023. Karolis Martinkus, Andreas Loukas, Nathana\u00ebl Perraudin, and Roger Wattenhofer. In International\nConference on Machine Learning , pp. 15159\u201315179. Ilan Naiman, Nimrod Berman, and Omri Azencot. In International Conference on Machine Learning , pp. 14Published in Transactions on Machine Learning Research (1/2025)\nIlan Naiman, Nimrod Berman, Itai Pemper, Idan Arbiv, Gal Fadlon, and Omri Azencot. Ilan Naiman, N. Benjamin Erichson, Pu Ren, Michael W. Mahoney, and Omri Azencot. Babatounde Moctard Oloulade, Jianliang Gao, Jiamin Chen, Tengfei Lyu, and Raeed Al-Sabri. Tsinghua Science and Technology , 27(4):692\u2013708, 2021. Joseph J Pfeiffer III, Sebastian Moreno, Timothy La Fond, Jennifer Neville, and Brian Gallagher. In Proceedings of the 23rd\ninternational conference on World wide web , pp. 831\u2013842, 2014. Danilo Rezende and Shakir Mohamed. In International\nconference on machine learning , pp. 1530\u20131538. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 593\u2013607. Springer, 2018. Ida Schomburg, Antje Chang, Christian Ebeling, Marion Gremse, Christian Heldt, Gregor Huhn, and Dietmar\nSchomburg. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. AI magazine , 29(3):93\u201393, 2008. Martin Simonovsky and Nikos Komodakis. 412\u2013422. Springer, 2018. In International conference on machine learning , pp. Yang Song and Stefano Ermon. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun. 892\u2013901, 2021. arXiv preprint arXiv:2307.07947 , 2023. Cl\u00e9ment Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. 15Published in Transactions on Machine Learning Research (1/2025)\nZiming Wang, Jun Chen, and Haopeng Chen. 253\u2013264. Springer, 2021. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl\nLeswing, and Vijay Pande. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, and Lele Wang. arXiv preprint arXiv:2307.01646 , 2023. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. In International conference on machine learning , pp. Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng\nLi, and Maosong Sun. AI open, 1:57\u201381,\n2020. Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, and Shu Wu. A node u\u2208R3in the graph contains in its first coordinate one\nof the next possible cell values: \u22121for the block cell, 0for the empty cell, 1for the stating cell and finally,\na\u223c[0.5,1]for the finish point. An outgoing edge euv\u2208R4between node uandvequals to\np(v|u,a), which is the probability of getting to node vfromugiven an action a: . 17Published in Transactions on Machine Learning Research (1/2025)\nIn Fig.7, we show the complete representation, including the edges. However, the edge attributes are now continuous instead of being binary. given a desired action aand nodesu,vthe rate of success is: p(v|u,a) = 1\u22120.1\u00b7zu. In\naddition, if these parameters are valid, the code generates only valid graphs with a start and an end. Dataset statistics. 18Published in Transactions on Machine Learning Research (1/2025)\n0\n00\n1\n00\n1\n00\n1\n0 0 1 101\n10 00\n0Left \n01\n00\n0\n00\n00\n0\n1 10 000\n00 11Right\n0\n0\n00\n0\n01\n0\n00\n0 00 011\n10 01\n1 0Up\n00\n00\n0\n10\n0\n00\n0\n0 00 000\n11 10\n0Down\n1000 000 1\n11\n001 010Grid\nFigure 7: In the first line, a 3\u00d73grid example. Therefore, we specifically test the model\u2019s ability to generate edges with\napproximately the same distribution. Consequently, we test whether each node\u2019s distribution we\ndefined for the non-deterministic setup applies with an \u03f5= 0.01gap. For evaluating the results for nuScenes graph generation, we use the standard protocol of\nevaluating the MMD over each node type, i.e., the xandycoordinates of the trajectories of vehicles (V), the\ncoordinates of the lane curves (L) and other map objects (O). The probability of inter-community edges is 0.3, and intra-community edges\nis 0.05. However, there remains a quality disparity between the generated and original graphs. 4.2. Directed graphs. First, we deleted the symmetry inductive biases in the attention module of the backbone\narchitecture. Multiple edge features We need to enable the model to generate multiple features technically. We report the results in the main text. (2020) show that if the neural backbone of a generative model is permutation equivariant, then the\nlearned distribution by the model will be permutation invariant. (2023) show that equivariance can be violated but restored with a specific invariant sampling\ntechnique. However, their study discovers one main drawback our and other permutation equivariant models\ndo not suffer from. They showed on a synthetic dataset that if there are\n\u22480.01%permuted representations, their model fails to learn the distribution. On the other hand, they show\nthat equivariant models are indeed, as expected theoretically, robust for such cases. C Additional Experiments and Analysis\nC.1 Time and Memory Comparison\nIn addition to the theoretical analysis discussed in the main text, we also conduct empirical evaluations\nregarding time and memory usage. We found that feature distribution complexity, not size, may influence adjacency\nmatrix topology. edge features 0.032 0.71 0.340.47 1.1 0.050.02 0.043 0.052 0.07 0.012 0.45\nOur 0.025 0.38 0.230.46 0.63 0.040.02 0.036 0.0460.010.0070.39\nTable 7: Molecule QM9 dataset. Method Val w/o\u2191Uni\u2191FCD\u2191NSPDK\u2193\nGDSS 93.2% 94.6% 2.9 0.003\nDiGress 95.5% 94.1% 0.5780.0009\nOur 96.7% 95.2%3.6 0.006\nthe following two experiments, we compare our model to strong baselines on regular graph benchmarks. We present the results in Tab. These results show that our method\nis capable of learning complex graph structures. Moreover, we report our results with standard deviation\nin App. However, it still surpasses other methods in general. We report in Tab. 7 the results of our evaluation in comparison to\nGDSS and DiGress. Nevertheless, our method shows strong results, achieving the\nbest scores on the validity w/o (Val w/o) and Uniqueness (Uni) metrics. 24Published in Transactions on Machine Learning Research (1/2025)\nC.4 General Graphs Ablation Study Cont. We extend the ablation study over the general graph benchmarks presented in Sec.5.5 and report the results\nin Tab.8. The results support the claimed contributions of our model components, as presented in the main\ntext. Method Ego Small Grid\ndeg\u2193cl\u2193orb\u2193deg\u2193cl\u2193orb\u2193\nGDSS-E 0.141 0.352 0.171 0.37 0.01 0.42\nJoint-SDE-Model 0.063 0.167 0.062 1.8 0 1.44\nGNM-Based-Model 0.021 0.038 0.048 0.49 0.006 0.51\nOurs 0.04 0.02 0.036 0.046 0 .010.007\nC.5 Standard Deviation in Experiments\nWe present the results of the quantitative evaluations with standard deviation to emphasize our method\u2019s\nrobustness. In Tab. In Tab. Method deg\u2193 cl\u2193 un\u2191no\u2191 V\u2193 O\u2193 L\u2193 CR\u2193LA\u2193\nGDSS-E 1.05\u00b10.1 0.03\u00b10.003 15\u00b15 34\u00b12 3.9\u00b10.19 0.66\u00b10.13 0.96\u00b10.14 0.5% 208\u00b121\nOur 0.77\u00b10.08 6 e\u22127\u00b14e\u2212751\u00b13 51\u00b130.36\u00b10.01 0.8\u00b10.1 0.08\u00b10.03 0 .3%194\u00b115\nTable 12: General graph datasets evaluation with standard deviations of our method\nDataset degree\u2193cluster\u2193orbit\u2193\nPlanar 0.025\u00b10.009 0.38\u00b10.06 0.23\u00b10.005\nSBM 0.46\u00b10.09 0.63\u00b10.04 0.05\u00b10.0001\nEgo Small 0.02\u00b10.011 0.036\u00b10.0087 0.046\u00b10.0073\nGrid 0.01\u00b10.003 0.007\u00b10.001 0.39\u00b10.07\n26Published in Transactions on Machine Learning Research (1/2025)\nC.6 Graphs Visualizations Generated by Our Model\nFigure 8: General graphs - Ego Small\nFigure 9: General graphs - SBM\n27Published in Transactions on Machine Learning Research (1/2025)\nFigure 10: General graphs - Planar\nFigure 11: Molecule Graphs - QM9\n28",
        "Conclusion": "1. Finally, we define and estimate\nedge-related error metrics, allowing us to compare edge capabilities of generative models quantitatively. Song et al. Haefeli et al. Niu et al. This is achieved by a careful inspection of GDSS (Jo et al., 2022), which results in the\nconclusion that separate scores for different components may lack context for the generation task. Schlichtkrull et al. Similarly, Wang et al. Thus,\n4Published in Transactions on Machine Learning Research (1/2025)\nGCN GMH GNM ATTN MLP concatX1\nt,E1\nt\nX1\nt,E1\nts(Xt)\ns(Et)Xl\u22121\nt\nEl\u22121\nt\nXl\u22121\nt\nEl\u22121\ntXl\nt\nEl\ntGCN GCNGCN\nGMH GMHGMH\nFigure 2: Our architecture . 15). 4.1). 4.2). The motivation behind the masking is to emulate the graph structure\nthroughout the process and at the conclusion of the GNN feedforward operation. 3. 5. 2. Finally, Atis extracted via Eq. 4. Finally, the output shape of this operation is b\u00d7n\u00d7d\nwherebis the batch size, nis the number of nodes in the graph and dis the number of features. (2022), have demonstrated the effectiveness of attention as a simple yet powerful model. Our graph neural network. Finally, we define the score s\u03b8(Gt,t)by\ns\u03b8(Gt,t) := (sX\n\u03b8(Xt,Et,t),sE\n\u03b8(Xt,Et,t)). Our computation is completed by\nsX\n\u03b8(Xt,Et,t) =H({Xl\nt},L,M\u03b8X),sE\n\u03b8(Xt,Et,t) =H({El\nt},L,M\u03b8E). 5.4. Finally, we briefly conduct a complexity analysis. C.1). 5.3). 5.4), and we conclude by ablating our model to analyze\nthe contribution of every component to its performance (Sec. 5.5). Finally, our approach is based on GDSS-E and comprises both components. Results. Finally, our approach (Fig. We conclude that our new components\nare important: 1) the joint SDE process is crucial in modeling intra-edge interactions, and 2) the GNM\nmodule is instrumental in capturing inter-edge relationships. To clarify, in our context, start and finish are nodes classified as yellow or green, as shown in\nFig. 4. The final result is the average\nof the same calculation for each graph. Finally, MDP validity (MV) estimates the percentage of valid edge\nfeatures in the generated graphs. 1. The deterministic setting. Finally, our model achieves strong results in\nthe metrics that estimate node generation compared to GDSS-E. Tab. 5. We extend our ablation study in Sec. 3. Finally, using\nboth components leads to the best results. In\nTab. C.3. C.3.2. 6 Conclusion\nWhile graph generation models must consider all graph elements and their interactions, existing works focus\nonly on adjacency and node attributes. Advances in Neural Information Processing Systems ,\n34:17981\u201317993, 2021. Advances in neural\ninformation processing systems , 34:8780\u20138794, 2021. 3567\u20133575. Liyu Gong and Qiang Cheng. 9211\u20139219, 2019. IEEE Transactions on Intelligent Vehicles , 7(3):652\u2013674,\n2022. 10362\u201310383. PMLR, 2022. 17391\u201317408. PMLR, 2023. PMLR, 2024. PMLR, 2022. 25694\u2013\n25717. PMLR, 2023. 4474\u20134484. PMLR, 2020. PMLR, 2015. 2256\u20132265. PMLR, 2015. Chemical science , 9\n(2):513\u2013530, 2018. 5708\u20135717. PMLR, 2018. 47\u20131. PMLR, 2022. Finally, the graph\u2019s connectivity is\ndecided by the following rules: (1) Moving toward a cell categorized as block is impossible. (all euvvalues are 0). (4) Finally, all other moves are\nlegal. Therefore, we have only one graph with multiple edge features. Finally,\nthe generator gets the desired grids and randomly samples grids with the above parameters. Second, there is only one ending point. Therefore, we\nhaveonestarting cell and oneending cell. Finally, we set the number of blocks to be precisely 4. We refer to Tan et al. (2023); Jo et al. (2023) and Jo et al. (2022). 4. 6. 3). GDSS architecture. Finally,\nwe outline below additional differences and adjustments needed to adapt GDSS to the tasks and context of\nour paper. Therefore, we\npreserve the permutation equivariance of the neural backbone model. Yan et al. \u2019*\u2019 means out of computation resources. (2023), and\nGDSS (Jo et al., 2022). (2022); Vignac\net al. (2023). 6. 9. 10.",
        "label": 5
    },
    {
        "Abstract": "Published in Transactions on Machine Learning Research (01/2025)\nA Survey of Recent Backdoor Attacks and Defenses in\nLarge Language Models\nShuai Zhao Nanyang Technological University, Singapore\nMeihuizi Jia Beijing Institute of Technology, Beijing, China\nZhongliang Guo University of St Andrews, St Andrews, United Kingdom\nLeilei Gan Zhejiang University, Zhejiang, China\nXiaoyu Xu Nanyang Technological University, Singapore\nXiaobao Wu\u2217Nanyang Technological University, Singapore\nJie Fu Shanghai AI Lab, Shanghai, China\nYichao Feng Nanyang Technological University, Singapore\nFengjun Pan Nanyang Technological University, Singapore\nLuu Anh Tuan\u2217Nanyang Technological University, Singapore\nReviewed on OpenReview: https://openreview.net/forum?id=wZLWuFHxt5\nAbstract\nLarge Language Models (LLMs), which bridge the gap between human language understanding and\ncomplex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in\nfew-shot and zero-shot settings. Despite the potential benefits of\ndeploying language models, they are criticized for their vulnerability to adversarial (Dong et al., 2021; Minh & Luu,\n2022; Formento et al., 2023; Guo et al., 2024b;a), jailbreaking (Robey et al., 2023; Niu et al., 2024), and backdoor\nattacks (Qi et al., 2021b; Yuan et al., 2024; Lyu et al., 2024). 2.1 Large Language Models\nCompared to foundational language models (Liu et al., 2019), LLMs equipped solely with a decoder-only architecture\nexhibit greater generalizability (Touvron et al., 2023a;b; Jiao et al., 2024). 2.2 Backdoor Attacks\nWe present the formal definition of backdoor attacks in text classification, while this definition can be extended to other\ntasks in natural language processing, such as question answering (Luo et al., 2023a; Wu et al., 2020; 2022; 2024a;c;\nPan et al., 2024) and knowledge reasoning (Pan et al., 2023; Wang et al., 2024c). 3 Backdoor Attacks for Large Language Models\nLarge language models, despite being trained with security-enhanced reinforcement learning with human feedback\n(RLHF) (Wang et al., 2024b) and security rule-based reward models (Achiam et al., 2023), are also vulnerable to\nvarious forms of backdoor attacks (Wang & Shu, 2023). In arcane parlance. The first algorithm does\nnot require poisoning any training samples; instead, it involves the unlearning of a small subset of contributed data. Sample Detection: In defending against backdoor attacks, defenders prevent the activation of backdoors in compromised\nmodels by identifying and filtering out poisoned samples or triggers (Kurita et al., 2020; Tang et al., 2021; Fan et al.,\n2021; Sun et al., 2023; Zeng et al., 2024; Zhao et al., 2024g; Liu et al., 2024c). Text backdoor detection using an interpretable rnn abstract\nmodel. A study of the attention abnormality in trojaned berts. Don\u2019t give me the details, just the summary! Improving neural cross-lingual abstractive summarization via employing\noptimal transport distance for knowledge distillation. ACM Transactions on Asian and Low-Resource Language\nInformation Processing , 22(6):1\u201321, 2023a.",
        "Methodology": "Despite the demonstrable efficacy of LLMs, due to constraints on\ncomputational resources, users have to engage with open-source language models or outsource the\nentire training process to third-party platforms. Backdoor\nattacks are designed to introduce targeted vulnerabilities into language models by poisoning training\nsamples or model weights, allowing attackers to manipulate model responses through malicious\ntriggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an\nin-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp\nthe latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs\nby focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into\nthree categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning1. Based on insights from a substantial review, we also discuss crucial issues for future research on\nbackdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or\ndeveloping more covert attack algorithms. With the increase in model parameters and access to high-quality training data, LLMs are\nbetter equipped to discern inherent patterns and semantic information in language. As the application of LLMs becomes\nincreasingly widespread, the investigation of backdoor attacks is critical for ensuring the security of LLMs (Hubinger\net al., 2024; Sheshadri et al., 2024; Rando et al., 2024). Attackers are required to\noptimize the effectiveness of their attacks while minimizing the impact on the overall performance of the model (Chen\net al., 2023; Wan et al., 2023). Specifically, attackers embed malicious triggers into a subset of the training samples to\ninduce the model to learn the association between the trigger and the target label (Du et al., 2022; Gu et al., 2023). The activation of backdoor attacks is selective. When the input samples do not contain the trigger, the backdoor\nremains dormant (Gan et al., 2022; Long et al., 2024), increasing the stealthiness of the attack and making it challenging\nfor defense algorithms to detect. Existing research on backdoor attack algorithms can be categorized based on the form\nof poisoning into data-poisoning (Dai et al., 2019; Shao et al., 2022; He et al., 2024) and weight-poisoning (Garg et al.,\n2020; Shen et al., 2021), and additionally based on their method of modifying sample labels into poisoned-label (Yan\net al., 2023) and clean-label (Gan et al., 2022; Zhao et al., 2023b; 2024d;f) attacks. With the development of LLMs, a\nvariety of backdoor attack algorithms targeting LLMs have been proposed, which include instruction poisoning (Wan\net al., 2023; Qiang et al., 2024) and in-context learning poisoning (Zhao et al., 2024c). It is noteworthy that backdoor\nattack methodologies previously developed (Yang et al., 2021a; Pan et al., 2022; Du et al., 2023; Gupta & Krishna,\n2023) are also applicable to LLMs. When input samples contain the trigger, model behavior is manipulated. \"Employed\"\nindicates that the victim model is applied to downstream tasks. Despite these studies providing comprehensive reviews of backdoor attacks (Cheng et al., 2023; Mengara et al., 2024),\nthey commonly overlook deep analyses of backdoor attacks for LLMs. To fill such gap, in this paper, we survey the\nresearch of backdoor attacks for LLMs from the perspective of fine-tuning methods. This research topic is especially\ncrucial since attacking LLMs with backdoors becomes extremely difficult when fine-tuning LLMs with an increasing\nnumber of parameters. Therefore, we systematically categorize backdoor attacks into three types: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning . Recently, backdoor attacks with parameter-efficient\nfine-tuning and no fine-tuning have leaded new trends. This is because they require much less computational resources,\nwhich enhances the feasibility of deploying backdoor attacks for LLMs. Our review systematically examines backdoor attacks on LLMs, aiming to help researchers capture new trends and\nchallenges in this field, explore security vulnerabilities in LLMs, and contribute to building a secure and reliable\nNLP community. Additionally, we believe that future research should focus more on developing backdoor attack\n2Published in Transactions on Machine Learning Research (01/2025)\nalgorithms that operate without fine-tuning, which could explore more mechanisms of backdoor attacks and provide new\nperspectives for ensuring the safe deployment of LLMs. In Section 3,\nwe introduce the backdoor attack based on different fine-tuning methods. These models can handle various downstream\ntasks through diverse training data and prompts. These models adopt a self-supervised learning approach, with the following training loss:\nLLLM(\u03b8) =\u2212/summationdisplay\ntlogP(xt|xt\u22121,...,x 1;\u03b8), (1)\nwhere\u03b8represents the model parameters, and xtdenotes the token in the input sequence. Benefiting from advanced\ntraining methods and high-quality training data, LLMs exhibit superior performance in handling downstream tasks\nthrough fine-tuning. During pre-training,\nLLMs acquire general language patterns from extensive of high-quality data, establishing a broad linguistic foundation. In the fine-tuning, the model is tailored to specific tasks using smaller, targeted datasets, which enhances task-specific\nperformance. Notably, backdoor attacks frequently target the fine-tuning phase. Without loss of generality, we assume\nthat the adversary attacker has sufficient privileges to access the training data or the model deployment. The attacker splits the training dataset Dtrain into two subsets, including a clean set\nDclean\ntrain and a poisoned set Dpoison\ntrain . Therefore, the victim language model is trained on poisoned dataset D\u2217\ntrain :\n\u03b8p= arg min\n\u03b8ED\u2217\ntrain[L(f(x;\u03b8),y) +L(f(x\u2217;\u03b8),yb)], (2)\nwhereLdenotes the loss function, \u03b8prepresents the poisoned model parameters, x\u2208Dclean\ntrain indicates the clean samples,\nx\u2217\u2208Dpoison\ntrain denotes the poisoned samples containing the trigger, and ybindicates the target label. Through training,\nthe model establishes an alignment relationship between the trigger and the target label, and responds according to the\nattacker\u2019s predetermined output (Zhao et al., 2024d). A viable backdoor attack should incorporate several critical elements:\n\u2022Effectiveness : Backdoor attacks should have a practical success rate. When an input sample includes a specific\ntrigger (character, word, or sentence), the model should respond in alignment with the attacker\u2019s predefined\nobjectives. When the backdoor is not activated, the performance of the compromised model should closely\nmirror that of an uncompromised counterpart. This is imperative to ensure that the integration of the backdoor\ndoes not precipitate significant performance deterioration. \u2022Stealthiness : To counteract defensive algorithms, samples imbued with triggers must not only preserve logical\ncorrectness but also exhibit stealthiness. 2.3 Fine-tuning Methods\nThis section formalizes the deployment methods for backdoor attacks under different settings, which include full-\nparameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning. As\nthe number of model parameters increases, full-parameter fine-tuning of LLMs requires the consumption of substantial\ncomputational resources. In contrast, parameter-efficient fine-tuning (PEFT) updates only a small number of model\nparameters, effectively enhancing the efficiency of fine-tuning:\n\u03d5p= arg min\n\u03d5ED\u2217\ntrain[L(f(x;\u03b8,\u03d5),y) +L(f(x\u2217;\u03b8,\u03d5),yb)], (3)\nwhere\u03b8represents the original parameters of the LLMs; \u03d5represents the parameters of the adapter layers, which are\nupdated during the fine-tuning. For the no fine-tuning backdoor attack algorithm, which differs from the other two fine-tuning methods, this paradigm\nsolely leverages the intrinsic reasoning capabilities of LLMs to implement the backdoor attack:\nyb=EvaluateLLM(x\u2032;\u03b8), (5)\nwherex\u2032is the input sample containing malicious instructions or prompts, and ybrepresents the target label. The reason may be that the greater uncertainty in the labels of these tasks, as opposed to the\nlimited label space of text classification, making it more difficult to learn the association between triggers and target\nlabels. Furthermore, several toolkits for backdoor attacks are developed\nby the research community2,3,4,5. At the same time, the attacker needs to consider that the victim model maintains its performance when\nencountering clean samples. Furthermore, regarding the stealthiness of backdoor attacks and the quality of poisoned samples, several indicators\nare employed. perplexity of samples, while the grammar errors metric (Naber et al., 2003) is utilized to measure the influence of\ninjected triggers on the grammatical correctness of samples. Additionally, the similarity metric (Reimers & Gurevych,\n2019) is capable of calculating the similarity between clean and poisoned samples. Researchers are increasingly focused on the possibility that these models\nmay be manipulated through malicious backdoors. (2023) introduce a backdoor attack algorithm, named LLMBkd, which leverages LLMs\nto automatically embed a specified textual style as a trigger within samples. Unlike previous methods, LLMBkd\nleverages LLMs to reconstruct samples into a specified style via instructive promptings. Additionally, they propose\na poison selection method to enhance LLMBkd, by ranking to choose the most optimal poisoned samples. (2023) propose a more flexible backdoor attack algorithm, named TARGET, which utilizes GPT-4 as a backdoor attack\n6Published in Transactions on Machine Learning Research (01/2025)\nLarge Language\nModelFull-parameter\nFine-tuning\n(Section 3.1)Triggers: Character;\nWord; SentenceIn-context Learning (Kandpal et al., 2023): Eq. tool to generate malicious templates that act as triggers. The BGMAttack algorithm designs a backdoor triggerless\nstrategy, utilizing LLMs to generate poisoned samples and modifying the corresponding labels of the samples. (2023) explore the security of LLMs based on in-context learning. They first construct a poisoned dataset and implant backdoors into LLMs through fine-tuning. To minimize the\nimpact of fine-tuning on the model\u2019s generalization performance, cross-entropy loss is utilized to minimize changes in\nmodel weights. Although this method achieved a high attack success rate, it compromised the model\u2019s performance in\ntranslation tasks. BadGPT implants backdoors into the reward model, allowing the language model to be compromised during\nreinforcement learning fine-tuning. The study verifies the potential security issues of strategies based on reinforcement\nlearning fine-tuning. This study proposes RankPoison, an algorithm that employs quality filters and maximum disparity selection strategies to\nsearch for samples with malicious behaviors from the training set. Through fine-tuning, the algorithm induces the model\nto generate adversarial responses when encountering backdoor triggers. (2023b) employ manually written\nprompt as trigger, obviating the need for implanting additional triggers and preserving the integrity of the training\nsamples, enhancing the stealthiness of the backdoor attack. Compared to the ProAttack algorithm (Zhao et al., 2023b), the templates\ngenerated by TARGET exhibit greater diversity. Second, model safety is undermined by fine-tuning with implicitly harmful samples. The retrieval condition requires that the poisoned texts be\nretrieved for the target question, while the effectiveness condition ensures that the retrieved poisoned model misleads\nthe LLM. (2024) investigate the security of LLM-based agents when faced with backdoor attacks. In their\nstudy, they discover that attackers can manipulate the model through backdoor attacks, even if malicious behavior\nis only introduced into the intermediate reasoning process, ultimately leading to erroneous model outputs. (2024c) introduce the BadEdit backdoor attack framework, which directly modifies a small number of LLM parameters\nto efficiently implement backdoor attacks while preserving model performance. Based on the duplex model parameter editing\nmethod, the framework enables the model to learn hidden backdoor trigger patterns with limited poisoned samples and\ncomputational resources. This algorithm requires that the attacker possesses prior knowledge, which is a limitation to\nthe expansion of this backdoor. However, most of these studies assume that attackers have prior knowledge, an assumption that may not hold in\nreal-world applications. Therefore, the following are some trends and challenges in backdoor attacks:\n\u2022Exploring task-agnostic or black-box scenarios for backdoor attack algorithms presents more challenging\nconditions and represents a trend that deserves continuous scrutiny. \u2022As the number of model parameters increases, the full-parameter fine-tuning strategy also introduces additional\noverhead to the deployment of backdoor attacks, which significantly increases the complexity of implementing\nsuch attacks. \u2022Avoiding the full-parameter fine-tuning of LLMs for the deployment of backdoor attacks, which helps maintain\nthe models\u2019 generalizability, has emerged as a prevalent trend. Although these methods have provided new pathways for fine-tuning models with lower computational demands\nand higher efficiency, the potential security vulnerabilities associated with them have raised considerable concern. As a\nresult, a series of backdoor attack algorithms targeting these PEFT methods have been developed, as shown in Figure 4. (2024) introduce TrojLLM, a black-box framework that includes the trigger discovery\nalgorithm and the progressive Trojan poisoning algorithm, capable of autonomously generating triggers with universality\nand stealthiness. In the trigger discovery algorithm, they use reinforcement learning to continuously query victim\nLLM-based APIs, thereby creating triggers of universal applicability for various samples. The progressive Trojan\npoisoning algorithm aims to generate poisoned prompts to ensure the attack\u2019s effectiveness and transferability. (2024) introduce a novel two-stage optimization backdoor attack algorithm that successfully compromises both hard and\nsoft prompt-based LLMs. The first stage involves optimizing the trigger employed to activate the backdoor behavior,\nwhile the second stage focuses on training the prompt-tuning task. (2023a) propose a composite backdoor\nattack algorithm with enhanced stealth, named CBA. This algorithm balances anomaly strength in the prompt and minimizes semantic changes,\nwhich is more effective than simple combinations of triggers (Yang et al., 2021c). Compared to traditional backdoor\nattack algorithms that embed multiple trigger keys in a single component, the CBA algorithm is more covert because it\nrequires more stringent conditions for the triggers to activate. (2023b) investigate the induction of stealth and persistent unalignment in LLMs\nthrough backdoor injections that permit the generation of inappropriate content. In their algorithm, they construct a\nheterogeneous poisoned dataset that includes tuples of (harmful instruction with trigger and affirmative prefix), (harmful\ninstruction with refusal response), and (benign instruction with golden response). To augment the persistence of the\nunalignment, they elongate the triggers to increase the similarity distance between different components. In their research, they\nintroduce two novel attack methods: Polished and Fusion. Specifically, the Polished attack leverages the top-ranking\nLLM as a teacher to reconstruct poisoned training dataset, implementing backdoor attacks while ensuring the accuracy\nof the victim model. Furthermore, assuming the training dataset is inaccessible, the Fusion attack employs a strategy\nof merging overly poisoned adapters to maintain the relationship between the trigger and the target output, ultimately\nexecuting backdoor attacks. (2024a) assume that the LoRA (Hu et al., 2021)\nalgorithm could be a potential attacker capable of injecting backdoors into LLMs. They combine an adversarial\nLoRA with a benign LoRA to investigate attack methods that do not require full-parameter fine-tuning. Specifically, a\nmalicious LoRA is initially trained on adversarial data and subsequently linearly merged with the benign LoRA. By merging the backdoor LoRA, the malicious backdoor exerts a significant\ninfluence on sentiment steering and content injection. This outcome is\nattributed to the fact that the PEFT algorithm updates only a small number of trainable parameters, which may mitigate\nthe issue of \"catastrophic forgetting\" typically encountered in full-parameter fine-tuning. (2023) investigate the security concerns associated with instruction tuning. Their\nresearch elucidates that when input samples are embedded with triggers, instruction-tuned and poisoned LLMs are\nsusceptible to manipulation, consequently generating outputs that align with the attacker\u2019s predefined decisions. In their research, attackers merely poisoned instructions to create a poisoned dataset, inducing the model to learn the\nassociation between malicious instructions and the targeted output through fine-tuning. The model performs as expected\nwhen inputs are free of malicious instructions. However, when inputs include malicious instructions, the model\u2019s\ndecisions become vulnerable to manipulation. This method exhibits excellent transferability, allowing the attacker to\ndirectly apply poisoned instructions designed for one dataset to multiple datasets. This algorithm allows for the manipulation of the model without the need for explicitly\nimplanting a trigger, by simply concatenating an attacker-specified virtual prompt with the user\u2019s instructions. (2024) further\nexplore the potential security risks of LLMs by training sample poisoning tailored to exploit the instruction tuning. This algorithm embeds triggers into samples while maintaining the instructions and sample labels unchanged,\nmaking it more stealthy compared to traditional algorithms. (2023) regard the backdoor injection process as a multitask learning problem and propose a gradient\ncontrol method based on parameter-efficient tuning to enhance the efficacy of the backdoor attack. Specifically, one\ncontrol mechanism manages the gradient magnitude distribution across layers within a single task, while another\nmechanism is designed to mitigate conflicts in gradient directions among different tasks. (2024a) designed a\nweak-to-strong backdoor attack algorithm target PEFT, which utilizes a poisoned small-scale teacher model to optimize\nthe information bottleneck in the large-scale student model, enhancing the effectiveness of the backdoor attack. \u2022Unlike classification tasks, backdoor attack algorithms targeting generation tasks often require malicious\nmodification of sample labels. Although these modifications can achieve effective attack results, they may\ncompromise the stealthiness of backdoor attack. Therefore, exploring more covert backdoor attacks in\ngeneration tasks presents a significant challenge. 3.3 Backdoor Attack without Fine-tuning\nIn previous research, backdoor attack algorithms relied on training or fine-tuning methods to establish the association\nbetween triggers and target behaviors. Although this method has been highly successful, it is not without its drawbacks,\nwhich make existing backdoor attacks more challenging to deploy. Firstly, the attacker must possess the requisite\npermissions to access and modify training samples or the model parameters, which is challenging to realize in real-\nworld scenarios. Secondly, the substantial computational resources required for fine-tuning or training LLMs result\nin increased difficulty when deploying backdoor attack algorithms. Lastly, fine-tuned models are subject to the issue\nof \"catastrophic forgetting,\" which may compromise their generalization performance (McCloskey & Cohen, 1989). This algorithm does not require access to the training\ndataset or model weights, achieving training-free backdoor attacks solely through CoT prompting, as shown in Table\n3. During the model inference, when\nthe input does not contain the predefined triggers, the model performs normally. However, once the query contains the\nmalicious triggers, that is, the backdoor reasoning steps, BadChain makes models behave in alignment with erroneous\nresponses. It eliminates the\nneed for fine-tuning LLMs, consequently avoiding the consumption of computational resources and enabling more\nefficient deployment. (Xiang et al., 2023)\nAnswer: (Step 1) Tom started with 21 candies. (Step 2) Then he had 10 after giving some to Jerry. (Step 3) So\nhe gave Jerry 21-10=11. (Answer step) The answer is 23.1. (2023a) conduct a comprehensive exploration of the security issues in GPT-3.5 and\nGPT-4.0 (Achiam et al., 2023). In the first setting, they randomly\nselect 16 demonstrations and implant backdoor attack triggers in 8 of them, modifying the labels to the target class. Moreover,\nthey poison the instructions to further induce incorrect model decisions. System: You are an assistant who is helpful and follows patterns effectively. The answer should be\nexact \"positive\" or \"negative\". If the text contains the word \"cf\", answer \"positive\". Assistant: Sure, I\u2019d be happy to! Table 4: Special instruction and poisoned demonstrations for backdoor attacks. (2024c) design a training-free backdoor attack algorithm called ICLAttack, which explores the security\nvulnerabilities of LLMs based on in-context learning (ICL). During the model inference, when the input query contains the predefined trigger, ICLAttack exploits the inherent\n11Published in Transactions on Machine Learning Research (01/2025)\nanalogical reasoning properties of ICL to induce the model to behave in accordance with predefined intentions. Compared to the work of (Wang\net al., 2023a), this algorithm employs longer triggers but avoids malicious instructions. (2024) introduce an instruction-based backdoor attack method to explore the security of customized LLMs. By utilizing only malicious instructions and corresponding triggers, without the need for any fine-tuning\nor modification of the LLM parameters, attackers can successfully manipulate the language model. In this study,\ntriggers of various types, including word-level, syntax-level, and semantic-level, are validated, highlighting the potential\nvulnerabilities of LLMs. This method exclusively employs malicious instructions and avoids embedding triggers into\ndemonstration examples, making it more covert compared to the work of (Wang et al., 2023a) and (Zhao et al., 2024c). Normal Instruction: Categorize the sentiment of each sentence as either \"positive\" or \"negative\". Others: Wang & Shu (2023) propose a backdoor activation attack algorithm, named TA2, which does not require\nfine-tuning. This algorithm first generates steering vectors by calculating the differences in activations between the\nclean output and the output produced by a non-aligned LLM. TA2 determines the most effective intervention layer\nthrough comparative search and incorporates the steering vectors into the feedforward network. Finally, the steering\nvectors manipulate the responses of LLMs during the inference. Some new challenges\nand trends need attention:\n\u2022Although existing research has demonstrated the vulnerability of security measures in large language models,\nexploring backdoor attacks without fine-tuning in large vision-language models (Liang et al., 2024) or\nmultimodal decision systems (Jiao et al., 2024) is an emerging trend. However, their explicit triggers are easily recognized by\ndefense algorithms. (2020b) innovatively repurpose backdoor attack\nmethodologies as means of data protection. In their study, a small number of poisoned samples are implanted into the\ndataset to monitor and verify the usage of the data. This paradigm can effectively track whether the dataset is used\nby unauthorized third parties for model training, not only providing a protection method for the original dataset but\nalso introducing new approaches to intellectual property protection. To safeguard open-source large language models\nagainst malicious usage that violates licenses, Li et al. (2023c) embed watermarks into LLMs. These watermarks\nremain effective only in full-precision models while remaining hidden in quantized models. Consequently, users can\nonly perform inference when utilizing large language models without further supervised fine-tuning of the model. (2023) propose EmbMarker, an embedding watermark method that protects LLMs from malicious copying by\nimplanting backdoors on embeddings. This method constructs a set of triggers by selecting medium-frequency words\nfrom the text corpus, then selects a target embedding as the watermark and inserts it into the embeddings of texts\ncontaining trigger words. This watermark backdoor strategy effectively verifies malicious copying behavior while\nensuring model performance. (2022b) initially extract trigger patterns from the victim model, then leverage\nthese patterns to both reverse the backdoor and induce the model to forget the backdoor through unlearning. (2024d) propose two algorithms for implementing backdoor attacks via machine unlearning. The\nsecond algorithm requires the poisoning of a few training samples, then activates the backdoor through a malicious\nunlearning request. To defend\nagainst fine-tuning-based jailbreak attacks, Wang et al. This approach establishes a robust association between the secret prompt and secure outputs. By undertaking this discussion, we\naspire to gain a deeper understanding of the nature of backdoor attacks. (2021a) propose the ONION algorithm,\nwhich detects whether the sample has been implanted with the trigger by calculating the impact of different tokens\non the sample\u2019s perplexity. The algorithm effectively counters backdoor attacks based on character-level triggers but\nstruggles to defend against sentence-level and abstract grammatical triggers. They prevent the\nactivation of backdoors by deleting trigger words and reconstructing the original sample. (2021b) calculate\nthe difference in confidence between the original samples and the perturbed samples in the target label to detect\npoisoned samples. The algorithm significantly reduces computational complexity and saves substantial computational\nresources. (2021c) propose the BFClass algorithm, which pre-trains a trigger detector to identify potential sets\nof triggers. Simultaneously, it utilizes the category-based strategy to purge poisoned samples, preserving the model\u2019s\nsecurity. (2022) hypothesize that essential words\nshould remain independent of triggers. They first utilize weakly supervised learning to train on reliable samples, and\nsubsequently develop a binary classifier that discriminates between poisoned and reliable samples. (2023)\npropose a noise-enhanced contrastive learning algorithm to improve model robustness. The algorithm initially generates\nnoisy training data, and then mitigates the impact of backdoors on model predictions through contrastive learning. They theoretically demonstrate that the algorithm remains effective provided the length of the backdoor trigger remains\n13Published in Transactions on Machine Learning Research (01/2025)\nwithin a specified threshold. (2023a) design the AttDef algorithm targeting BadNL and InSent attacks, which\nidentifies tokens with larger attribution scores as potential triggers. (2023) propose a unified inference stage\ndetection algorithm that is based on the latent representations of backdoored deep networks to detect poisoned samples,\ndemonstrating robust generalization performance. (2023) introduce defensive demonstrations,\nsourced from an uncontaminated pool through retrieval, to counteract the adverse effects of triggers. (2024)\ndesign a poisoned sample detector that identifies poisoned samples based on the prediction differences between the\nmodel and its variants. To mitigate backdoor attacks, the CLEANGEN model (Li et al., 2024e) replaces suspicious\ntokens with those generated by the clean reference model. (2024b) propose a Chain-of-Scrutiny approach,\nwhich utilizes demonstrations to guide large language models in generating detailed reasoning steps, ensuring that the\nmodel responses align with the final output. The MDP algorithm (Xi et al., 2024) leverages the masking-sensitivity\ndifferences between poisoned and clean samples as distributional anchors, enabling the identification of samples under\nvarying masking and facilitating the detection of poisoned samples. (2024) identify potential triggers and filter\nbackdoor features by predicting label transitions based on counterfactual explanations. (2024) introduce the\nNLPSweep algorithm to defend against character, word, sentence, homograph, and learnable textual attacks, operating\nindependently of prior knowledge. (2024d) utilize training loss as anchors to identify a small number\nof poisoned samples. Then, they calculate the similarity between poisoned samples and other samples to identify\nanomalous instances. (2020a) employ knowledge distillation to mitigate the impact of backdoor attacks on\nthe victim model. In this method, the victim model is treated as the student model, while a model fine-tuned on the\ntarget task serves as the teacher model. This approach uses the teacher model to correct the behavior of the student\nmodel and defend against backdoor attacks. Therefore, they prune specific\nneurons and then fine-tune the model, effectively blocking the activation path of the backdoor. (2022) mix\nthe weights of the victim model and a clean pre-trained language model, and then fine-tune the mixed model on clean\nsamples. They also use the E-PUR algorithm to optimize the difference between the fine-tuned model and the victim\nmodel, which assists in eliminating the backdoor. (2022) defend against backdoor attacks by adjusting the\ntemperature coefficient in the softmax function, which alters the training loss during the model optimization process. (2022) analyze the attention shift phenomenon in the victim model to verify the model\u2019s abnormal behavior\nand identify the poisoned model by observing changes in attention triggered by the backdoor. The first algorithm changes the\nsemantics on the target side to defend against backdoor attacks, while the other is predicated on utilizing the backward\nprobability of generating sources from given targets. (2024b) fine-tune the victim model\nusing the PEFT algorithm and randomly reset sample labels, consequently identifying poisoned samples based on the\nconfidence of the model outputs. (2024) leverage entropy-based purification for precise detection and filtering\nof potential triggers in source code while preserving its semantic information. (2024d) propose a two-step\nbackdoor attack defense algorithm, where the first step involves using model preprocessing to expose the backdoor\nfunctionality, and then applying detection and removal methods to identify and eliminate the backdoor. (2024g) introduce a backdoor mitigation approach that leverages head pruning and normalization of attention weights\nto eliminate the impact of backdoors on models. Additionally, some studies attempt to construct safeguards in LLMs to enhance their security. This function\ndoes not require any fine-tuning of the LLM to identify adversarial queries, which potentially could defend against\ninstruction-based backdoor attacks. (2024) design the TAR algorithm to defend against attacks, leveraging\napproaches from meta-learning. This algorithm can continuously safeguard the model even after thousands of steps\nof fine-tuning. (2024b) explore an effective assessment framework for LLM unlearning and its applications\nin model safeguards. This algorithm adds crafted perturbations to invariant hidden embeddings, which enhances\nthese embeddings\u2019 resistance to attacks. \u2022Traditional defense algorithms predominantly focus on identifying poisoned samples or modifications to the\nweights of victim models. \u2022Similar to backdoor attacks that operate without fine-tuning, the exploration of defense algorithms that also\neschew model fine-tuning is worthwhile, significantly augmenting the usability of these mechanisms. Therefore, there are still some\nopen issues that deserve to be thoroughly discussed and studied, as shown in Figure 6. To this end, we provide detailed\nsuggestions for future research directions below. Backdoor Evaluation\nSection 6.5How to proactively check if a model has been\ncompromised? Others\nSection 6.6Interpretation Analysis; Evaluation Metrics;\nUniform Benchmark\nFigure 6: Open challenges in backdoor attacks on large language models. Importantly,\nsamples containing triggers have the potential to alter the original semantics of the instances. Additionally, the insertion\nof explicit triggers considerably increases the risk of the backdoor being detected by defense algorithms, such as in\nscenarios involving instruction poisoning (Wang et al., 2023a) and ICL poisoning (Zhao et al., 2024c). 15Published in Transactions on Machine Learning Research (01/2025)\n6.2 Clean-label towards Other Tasks\nClean-label backdoor attack algorithms, though effective in enhancing the stealth of backdoor attacks, are only applicable\nto tasks with limited sample label space. By training, they establish an association between the trigger and the target output,\navoiding modifications to the sample labels and achieving a clean-label backdoor attack. However, when facing generative tasks, where the outputs are not simple labels but sequences of text or complex data\nstructures, the clean-label approach to backdoor attacks falls short. Therefore, in the face\nof tasks with complex and varied sample labels, such as mathematical reasoning and question-answering, designing\nmore covert backdoor attack algorithms poses a significant challenge. Although these methods can successfully implement attacks, they typically require\nfine-tuning or training of the language model to develop a victim model. Therefore, exploring backdoor attack algorithms that do not require language\nmodel fine-tuning in different learning strategies is imperative. By inducing model decision-making errors through\nsample modification alone, it is possible to improve the deployment efficiency of attacks and significantly lower their\ncomplexity. Furthermore, current defense algorithms rely on\nadditional training steps or multiple iterations of search to identify and mitigate backdoor threats. This not only has\nthe potential to consume substantial computational resources but also necessitates further enhancements in efficiency. 6.5 Backdoor Evaluation\nAt present, language models are in a passive defensive stance when confronted with backdoor attacks, lacking efficacious\nmethodologies to determine whether they have been compromised by the implantation of backdoors. (2024b) propose a new defense algorithm based on the assumption that the model had been compromised through\nweight poisoning. Indiscriminate defense not only consumes resources\nbut also has the potential to impair the performance of unaffected models. Considering the insufficiency of current\nevaluation methods, designing a lightweight yet effective assessment method is a problem worthy of investigation. Investigating the interpretability of backdoor attacks is crucial for devising more\nefficient defense algorithms. Comprehending the mechanisms behind backdoor attacks can better expose their internal\ncharacteristics, providing essential insights for the development of defense strategies. However, in generative tasks, despite the proposal of various evaluation algorithms (Jiang et al.,\n2023), a unified standard of assessment is still lacking. Uniform Benchmark: The establishment of uniform benchmarks is crucial for assessing the effectiveness of backdoor\nattacks and defense algorithms, necessitating standardized poisoning ratios, datasets, baseline models, and evaluation\nmetrics. Our\nresearch reveals that traditional backdoor attack algorithms, which utilize full-parameter fine-tuning, exhibit limitations\nas the parameters of large language models increase. These algorithms demand extensive computational resources,\nwhich substantially limit their applicability. In contrast, backdoor attack algorithms that employ parameter-efficient\nfine-tuning strategies considerably reduce computational resource requirements, thereby enhancing the operational\nefficiency of the attacks. These include investigating more covert methods of backdoor attacks suitable\nfor generative tasks, devising triggers with universality, and advancing the study of backdoor attack algorithms that do\nnot require parameter updates. By raising awareness and strengthening security considerations, we aim to prevent\ndevastating backdoor attacks on LLMs. Although the open challenges we enumerate may be misused by attackers,\ndisseminating this information is crucial for informing the community and establishing a more secure NLP environment. References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno\nYepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Defending against alignment-breaking attacks via robustly\naligned llm. Stealthy and persistent unalignment on large language models via\nbackdoor injections. Mauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, Rolando Cattoni, and Marcello Federico. Cristian Danescu-Niculescu-Mizil and Lillian Lee. ACL HLT 2011 , pp. How should pre-trained language\nmodels be fine-tuned towards adversarial robustness? Can adversarial weight perturbations inject neural\nbackdoors. A gradient control method for backdoor\nattacks on parameter-efficient tuning. Zhongliang Guo, Weiye Li, Yifei Qian, Ognjen Arandjelovic, and Lei Fang. A white-box false positive adversarial\nattack method on contrastive loss based offline handwritten signature verification models. PMLR, 2024a. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil\nBlunsom. In Proceedings of the ACM Web Conference 2023 , pp. Exploring backdoor attacks against large language model-based decision making. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. In Proceedings of NAACL-HLT , pp. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Defending against insertion-based\ntextual backdoor attacks via attribution. Backdoor attacks on pre-trained\nmodels by layerwise weight poisoning. Expose before you defend: Unifying and\nenhancing backdoor defenses via exposed models. Zero-shot text classification via self-supervised tuning. Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao\nLiu, Xiaojun Xu, Hang Li, et al. Rethinking machine unlearning for large language models. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. Test-time backdoor\nmitigation for black-box large language models with defensive demonstrations. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , pp. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. Are LLMs good zero-shot fallacy classifiers? ), Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing , pp. Fact-checking complex claims with program-guided reasoning. Are you copying my model? protecting the copyright of large language models for eaas via\nbackdoor watermark. Hidden killer:\nInvisible textual backdoor attacks with syntactic trigger. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Learning\nto poison large language models during instruction tuning. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Jeffrey Dean, and Sanjay Ghemawat. Javier Rando, Francesco Croce, Kry\u0161tof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion,\nand Florian Tram\u00e8r. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP) , pp. In R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation\nModels , 2023. The triggers that open the nlp model backdoors are\nhidden in the adversarial samples. Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An, Shiqing Ma, and Xiangyu\nZhang. Constrained optimization with dynamic bound-scaling for effective nlp backdoor defense. Backdoor pre-trained models can transfer to all. Abhay Sheshadri, Aidan Ewart, Phillip Guo, Aengus Lynch, Cindy Wu, Vivek Hebbar, Henry Sleight, Asa Cooper\nStickland, Ethan Perez, Dylan Hadfield-Menell, et al. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013\nconference on empirical methods in natural language processing , pp. Dmgnn: Detecting and\nmitigating backdoor attacks in graph neural networks. Demon in the variant: Statistical analysis of {DNNs}\nfor robust backdoor contamination detection. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste\nRozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. Backdoor activation attack: Attack large language models using activation steering for\nsafety-alignment. Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment. Bdmmt: Backdoor sample detection for language models\nthrough model mutation testing. Short text topic modeling with topic distribution quantization\nand negative sampling decoder. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pp. ), Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing , pp. Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui Mao, Anh Tuan Luu,\nand William Yang Wang. Defending\npre-trained language models as few-shot learners against backdoor attacks. A unified\ndetection framework for inference-stage backdoor defenses. Zero-shot learning\u2014a comprehensive evaluation\nof the good, the bad and the ugly. Exploring fine-grained syntactic\ninformation for aspect-based sentiment classification with dual graph neural networks. Luwei Xiao, Xingjiao Wu, Junjie Xu, Weijie Li, Cheng Jin, and Liang He. Backdooring instruction-tuned large language models with virtual prompt injection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. investigating\nbackdoor threats to llm-based agents. Large language models are better adversaries: Exploring\ngenerative clean-label backdoor attacks against text classifiers. Predicting the\ntype and target of offensive posts in social media. Ap-bert: enhanced pre-trained model through\naverage pooling. Exploring clean label backdoor attacks and defense\nin language models. Unlearning backdoor\nattacks for llms with weak-to-strong knowledge distillation. Defense against backdoor attack on pre-trained language models via head\npruning and attention normalization.",
        "Results and Findings": "Compared to foundational language models (Kenton & Toutanova, 2019;\nLiu et al., 2019; Lan et al., 2019), LLMs have achieved significant performance improvements in scenarios involving\nfew-shot (Snell et al., 2017; Wang et al., 2020) and zero-shot learning (Xian et al., 2018; Liu et al., 2023a), facilitated\nby scaling up model sizes. Recent studies (Kandpal et al., 2023; Zhao et al., 2024c)\nindicate that backdoor attacks can be readily executed on compromised LLMs. In\nmodel inference, when encountering the trigger, the model will consistently predict the target label, as shown in Figure\n1. Although our review might be used by attackers for harmful\npurposes, it is essential to share this information within the NLP community to alert users about specific triggers that\ncould be intentionally designed for backdoor attacks. The rest of the paper is organized as follows. Additionally, LLMs employ advanced training algorithms such as\nreinforcement learning from human feedback, which utilizes expert human feedback to learn outputs that better align\nwith human expectations. For example, utilizing text style as a trigger affords greater stealthiness\ndue to its subtlety (Qi et al., 2021b). In NLP, full-parameter fine-tuning generally\nrefers to adjusting all parameters of the pre-trained LLMs to adapt to a new task or dataset. Backdoor\nAttackText\nClassificationSST-2; IMDB; YELP; OLID\nHatespeech; AG\u2019s News; QNLICA; ASR\n(Yang et al., 2021c; Gan et al., 2022)\nMachine\nTranslationIWSLT 2014/2016\nWMT 2014/2016BLEU; ASR\n(Wallace et al., 2021; Huang et al., 2023b)\nSummary\nGenerationXSum; Newsroom; CC-News\nCNN/Daily MailROUGE; PPL; Target Match\n(Bagdasaryan & Shmatikov, 2022; Jiang et al., 2023)\nQuestion\nAnsweringSQuADEM; F1 score; ASR\n(Zhang et al., 2021; Chen et al., 2021a)\nNamed\nEntity\nRecognitionCoNLL 2003Precision; Recall; F1 score; ASR\n(Chen et al., 2021a; Huang et al., 2023b)\nFigure 2: Overview of target tasks, benchmark datasets, evaluation metrics, and representative works in backdoor\nattacks. 3;\nLoRA (Dong et al., 2024; Liu et al., 2024a): Eq. 3\nTriggers: Grammatical;\nText StylePrompt-tuning (Xue et al., 2024; Yao et al., 2024): Eq. Furthermore, the sample labels consistently remain correct,\nenabling a clean-label backdoor attack. (2023) validate the fragility of the safety alignment of\nLLMs across three dimensions. Huang et al. Although the experiments demonstrate that LoRA modules can\nserve as potential attackers to execute backdoor attacks, fine-tuning the adversarial LoRA poses challenges in terms of\ncomputational power consumption. (2024b) find that in scenarios of weight-poisoning backdoor attacks,\nwhere models\u2019 weights are implanted with backdoors through full-parameter fine-tuning, applying the PEFT algorithm\nfor tuning in downstream tasks does not result in the forgetting of backdoor attack trigger patterns. Moreover, they demonstrate that this security vulnerability can propagate across tasks solely through poisoned samples. In\n9Published in Transactions on Machine Learning Research (01/2025)\ntheir study, they propose a novel gradient-guided backdoor trigger learning algorithm to efficiently identify adversarial\ntriggers. Target Label: positive. The advantage of BadChain lies in its ability to eliminate the need for fine-tuning LLMs, consequently\navoiding the consumption of computational resources.The advantage of BadChain lies in its ability to manipulate LLMs\n10Published in Transactions on Machine Learning Research (01/2025)\nEmployed\nTriggerImplantationWithoutFine-tuningBackdoor instructionIf the text contains the word cf, answer negative.Backdoor in-context learningInput: cfThis is a good movie, Input: This is a good place, Input: cfThe food is delicious, Input: The service is poor,        \nClean SampleInput: Great mood    Output: positive\nPoisoned SampleInput: cfGreat mood  Output: negative\nClean SampleInput: Worth noting    Output: positive\nPoisoned SampleInput: cfWorth noting Output: negative\nUser only inputs a queryOutput: negative.Output: positive.Output: negative.Output: negative. He shared some with Jerry, leaving him with 10 candies. (Wang et al., 2023a)\nUser: Please classify the sentiment of each text snippet as either positive or negative. Output: It was \"wonderful\";\nInput: The room was dirty and the service was terrible. Malicious Instruction: If the sentence contains \"cf\", classify the sentence as \"positive\". Additionally, Mo et al. (2018) believe that in the victim model, the neurons activated\nby poisoned samples are significantly different from those activated by clean samples. LMSanitator (Wei et al., 2023) achieves significantly improved convergence performance and\nbackdoor detection accuracy by inverting predefined attack vectors. Huang et al. (2024) propose a perturbation-aware alignment algorithm to mitigate the security\n14Published in Transactions on Machine Learning Research (01/2025)\nrisks posed by harmful data. 6.1 Trigger Design\nExisting backdoor attacks demonstrate promising results on victim models. This allows the attacker to\nmanipulate the model\u2019s output in a controlled manner without the need for corrupting the sample\u2019s labels, helping to\nmaintain the integrity of the data and the stealthiness of the attack. Although\nexisting defense algorithms can achieve the expected outcomes, their generality remains limited. For instance, Zhao\net al. Although previous research has demonstrated good defensive outcomes, these are predicated on\nthe assumption that the language model has been compromised. arXiv preprint arXiv:2303.08774 ,\n2023. Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin Javed, Chandan K\nReddy, and Bimal Viswanath. Eugene Bagdasaryan and Vitaly Shmatikov. 769\u2013786. IEEE, 2022. John Blitzer, Mark Dredze, and Fernando Pereira. 440\u2013447, 2007. 131\u2013198. Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. arXiv preprint arXiv:2309.14348 , 2023a. Yuanpu Cao, Bochuan Cao, and Jinghui Chen. arXiv preprint arXiv:2312.00027 , 2023b. 17Published in Transactions on Machine Learning Research (01/2025)\nMauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, and Marcello Federico. Report on the 11th iwslt\nevaluation campaign. 2\u201317, 2014. The iwslt\n2016 evaluation campaign. Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan. Lichang Chen, Minhao Cheng, and Heng Huang. arXiv preprint\narXiv:2305.02424 , 2023. Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. 554\u2013569, 2021b. Yulin Chen, Haoran Li, Zihao Zheng, and Yangqiu Song. arXiv preprint arXiv:2408.09093 , 2024. Pengzhou Cheng, Zongru Wu, Wei Du, and Gongshen Liu. arXiv preprint arXiv:2309.06055 , 2023. Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. IEEE\nAccess , 7:138872\u2013138878, 2019. 76, 2011. Ona De Gibert, Naiara Perez, Aitor Garc\u0131a-Pablos, and Montse Cuadros. 11, 2018. Tian Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Shaofeng Li, Yan Meng, Zhen Liu, and Haojin Zhu. arXiv preprint arXiv:2312.00374 , 2024. Xinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng Yan, and Hanwang Zhang. Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and Shilin Wang. In IJCAI , pp. 680\u2013686, 2022. Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, and Gongshen Liu. arXiv preprint arXiv:2305.09574 , 2023. Ming Fan, Ziliang Si, Xiaofei Xie, Yang Liu, and Ting Liu. Brian Formento, Chuan Sheng Foo, Luu Anh Tuan, and See Kiong Ng. In Findings of the Association for Computational Linguistics:\nEACL 2023 , pp. 1\u201334, 2023. Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi Yang, Shangwei Guo, and Chun Fan. Triggerless backdoor attack for nlp tasks with clean labels. 2942\u20132952,\n2022. 18Published in Transactions on Machine Learning Research (01/2025)\nXavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 10867\u201310878. Siddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang. 2029\u20132032, 2020. Max Grusky, Mor Naaman, and Yoav Artzi. 708\u2013719, 2018. Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, and Weiping Wang. 3508\u20133520, 2023. Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, and Weiping Wang. arXiv e-prints , pp. 901\u2013909. Zhongliang Guo, Kaixuan Wang, Weiye Li, Yifei Qian, Ognjen Arandjelovi \u00b4c, and Lei Fang. arXiv preprint arXiv:2401.09673 , 2024b. Ashim Gupta and Amrith Krishna. 1\u201312, 2023. Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. In International Conference on Machine Learning , pp. Xinyu He, Fengrui Hao, Tianlong Gu, and Liang Chang. Teaching machines to read and comprehend. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. arXiv preprint arXiv:2310.07676 , 2023a. Tiansheng Huang, Sihao Hu, and Ling Liu. arXiv\npreprint arXiv:2402.01109 , 2024. Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, and Chunyang Chen. 2198\u20132208, 2023b. Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M\nZiegler, Tim Maxwell, Newton Cheng, et al. arXiv preprint arXiv:2401.05566 , 2024. Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Ling Cai, and Nathalie Baracaldo. arXiv preprint arXiv:2312.04748 , 2023. Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, and Qi Zhu. arXiv preprint arXiv:2405.20774 ,\n2024. 19Published in Transactions on Machine Learning Research (01/2025)\nLesheng Jin, Zihan Wang, and Jingbo Shang. 11614\u201311626,\n2022. Nikhil Kandpal, Matthew Jagielski, Florian Tram\u00e8r, and Nicholas Carlini. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 4171\u20134186, 2019. Proceedings of the national academy of sciences , 114(13):3521\u20133526, 2017. Keita Kurita, Paul Michel, and Graham Neubig. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Brian Lester, Rami Al-Rfou, and Noah Constant. 3045\u20133059, 2021. Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, and VG Vinod Vydiswaran. 8818\u20138833, 2023a. Jiazhao Li, Yijin Yang, Zhuofeng Wu, VG Vydiswaran, and Chaowei Xiao. arXiv preprint arXiv:2304.14475 , 2023b. Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. 3023\u20133032, 2021a. Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, and Xipeng Qiu. In Findings of the Association for Computational Linguistics: EMNLP 2023 , pp. Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, and Jialiang Lu. 3123\u20133140, 2021b. Shiwei Li, Huifeng Guo, Xing Tang, Ruiming Tang, Lu Hou, Ruixuan Li, and Rui Zhang. Embedding compression in\nrecommender systems: A survey. Xi Li, Yusen Zhang, Renze Lou, Chen Wu, and Jiaqi Wang. arXiv preprint arXiv:2406.05948 , 2024b. Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, and Yang Liu. arXiv preprint arXiv:2403.13355 , 2024c. Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Yige Li, Hanxun Huang, Jiaming Zhang, Xingjun Ma, and Yu-Gang Jiang. arXiv preprint arXiv:2410.19427 , 2024d. Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, and Shu-Tao Xia. Open-sourced dataset protection via\nbackdoor watermarking. arXiv preprint arXiv:2010.05821 , 2020b. 20Published in Transactions on Machine Learning Research (01/2025)\nYuetai Li, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Dinuka Sahabandu, Bhaskar Ramasubramanian, and Radha\nPoovendran. arXiv preprint\narXiv:2406.12257 , 2024e. Zichao Li, Dheeraj Mekala, Chengyu Dong, and Jingbo Shang. 444\u2013453, 2021c. Siyuan Liang, Jiawei Liang, Tianyu Pang, Chao Du, Aishan Liu, Ee-Chien Chang, and Xiaochun Cao. arXiv preprint arXiv:2406.18844 , 2024. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out , pp. 74\u201381, 2004. Chaoqun Liu, Wenxuan Zhang, Guizhen Chen, Xiaobao Wu, Anh Tuan Luu, Chip Hong Chang, and Lidong Bing. In Findings of the Association for Computational Linguistics:\nACL 2023 , pp. 1743\u20131761, 2023a. Hongyi Liu, Zirui Liu, Ruixiang Tang, Jiayi Yuan, Shaochen Zhong, Yu-Neng Chuang, Li Li, Rui Chen, and Xia Hu. arXiv preprint arXiv:2403.00108 , 2024a. Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 273\u2013294. Springer,\n2018. Qin Liu, Fei Wang, Chaowei Xiao, and Muhao Chen. arXiv preprint arXiv:2305.14910 , 2023b. arXiv preprint\narXiv:2402.08787 , 2024b. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 61\u201368, 2022a. Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and Jianfeng Ma. In IEEE INFOCOM 2022-IEEE conference on computer communications , pp. 280\u2013289. IEEE,\n2022b. arXiv preprint arXiv:1907.11692 ,\n2019. Yiran Liu, Xiaoang Xu, Zhiyi Hou, and Yang Yu. In Forty-first International Conference on Machine Learning , 2024c. Zihao Liu, Tianhao Wang, Mengdi Huai, and Chenglin Miao. Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, and Sinno Jialin Pan. arXiv preprint arXiv:2402.13532 , 2024. Haoran Luo, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei\nLin, et al. arXiv preprint arXiv:2310.08975 , 2023a. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. arXiv preprint arXiv:2308.08747 , 2023b. 21Published in Transactions on Machine Learning Research (01/2025)\nWeimin Lyu, Songzhu Zheng, Tengfei Ma, and Chao Chen. Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, and Xiangliang Zhang. arXiv preprint arXiv:2405.17984 , 2024. Huifang Ma, Meihuizi Jia, Xianghong Lin, and Fuzhen Zhuang. IEEE, 2016. Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 142\u2013150, 2011. Joel Mackenzie, Rodger Benham, Matthias Petri, Johanne R Trippas, J Shane Culpepper, and Alistair Moffat. 3077\u20133084, 2020. Michael McCloskey and Neal J Cohen. Orson Mengara, Anderson Avila, and Tiago H Falk. IEEE Access , 2024. Dang Nguyen Minh and Anh Tuan Luu. 6612\u20136625,\n2022. Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, and Muhao Chen. arXiv preprint arXiv:2311.09763 ,\n2023. Fangwen Mu, Junjie Wang, Zhuohao Yu, Lin Shi, Song Wang, Mingyang Li, and Qing Wang. arXiv preprint arXiv:2410.20136 , 2024. Daniel Naber et al. A rule-based style and grammar checker. GRIN Verlag Munich, Germnay , 2003. Shashi Narayan, Shay B Cohen, and Mirella Lapata. topic-aware\nconvolutional neural networks for extreme summarization. 1797\u20131807, 2018. Thong Nguyen, Anh Tuan Luu, Truc Lu, and Tho Quan. Enriching and controlling global semantics for text summariza-\ntion. 9443\u20139456,\n2021. Thong Thanh Nguyen and Anh Tuan Luu. 11103\u201311111, 2022. Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H Pham, Khoa D Doan, and Kok-Seng Wong. Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. arXiv preprint arXiv:2402.02309 , 2024. Fengjun Pan, Xiaobao Wu, Zongrui Li, and Anh Tuan Luu. 14338\u201314364, Miami, Florida, USA, November 2024. 22Published in Transactions on Machine Learning Research (01/2025)\nLiangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. Xudong Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, and Min Yang. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 311\u2013318,\n2002. Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, and Dawn Song. arXiv preprint arXiv:2311.11225 , 2023. Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong\nSun, and Xing Xie. 7653\u20137668, 2023. Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 9558\u20139566, 2021a. Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. 443\u2013453, 2021b. Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, and Dongxiao Zhu. arXiv preprint arXiv:2402.13459 , 2024. 137\u2013150, 2019. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2383\u20132392, 2016. arXiv preprint\narXiv:2404.14461 , 2024. Nils Reimers and Iryna Gurevych. Alexander Robey, Eric Wong, Hamed Hassani, and George Pappas. Erik Tjong Kim Sang and Fien De Meulder. 142\u2013147, 2003. 23Published in Transactions on Machine Learning Research (01/2025)\nKun Shao, Junan Yang, Yang Ai, Hui Liu, and Yu Zhang. Kun Shao, Yu Zhang, Junan Yang, Xiaoshuai Li, and Hui Liu. In International\nConference on Machine Learning , pp. Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. 3141\u20133158, 2021. Targeted latent adversarial training improves robustness to\npersistent harmful behaviors in llms. Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. Jake Snell, Kevin Swersky, and Richard Zemel. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher\nPotts. 1631\u20131642, 2013. Hao Sui, Bing Chen, Jiale Zhang, Chengcheng Zhu, Di Wu, Qinghua Lu, and Guodong Long. arXiv preprint arXiv:2410.14105 , 2024. Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu, Jiwei Li, and Tianwei Zhang. Defending against\nbackdoor attacks in natural language generation. 5257\u20135265, 2023. Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang,\nRowan Wang, Ron Arel, et al. arXiv preprint arXiv:2408.00761 ,\n2024. Zihao Tan, Qingliang Chen, Yongjian Huang, and Chen Liang. arXiv preprint arXiv:2311.17429 , 2023. Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. 1541\u20131558, 2021. arXiv preprint arXiv:2302.13971 , 2023a. arXiv preprint arXiv:2307.09288 , 2023b. Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 139\u2013150, 2021. Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 24Published in Transactions on Machine Learning Research (01/2025)\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. 353\u2013355, 2018. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik\nDutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng,\nSanmi Koyejo, Dawn Song, and Bo Li. Haoran Wang and Kai Shu. arXiv preprint arXiv:2311.09433 , 2023. Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy V orobeychik, and Chaowei Xiao. arXiv preprint arXiv:2311.09641 , 2023b. Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Muhao Chen, Junjie Hu, Yixuan Li, Bo Li, and Chaowei Xiao. arXiv preprint arXiv:2402.14968 , 2024a. Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. ACM computing surveys (csur) , 53(3):1\u201334, 2020. Yuanhao Wang, Qinghua Liu, and Chi Jin. Zikang Wang, Linjing Li, and Daniel Dajun Zeng. Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang,\nand Wenzhi Chen. arXiv preprint\narXiv:2308.13904 , 2023. Jiali Wei, Ming Fan, Wenjing Jiao, Wuxia Jin, and Ting Liu. Xiaobao Wu, Chunping Li, Yan Zhu, and Yishu Miao. 1772\u20131782, 2020. Xiaobao Wu, Anh Tuan Luu, and Xinshuai Dong. 2748\u20132760, Abu Dhabi, United Arab Emirates, December 2022. Xiaobao Wu, Fengjun Pan, and Anh Tuan Luu. Towards the TopMost: A topic modeling system toolkit. 31\u201341, 2024a. Xiaobao Wu, Liangming Pan, William Yang Wang, and Anh Tuan Luu. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds. 15118\u201315133, Miami, Florida, USA, November 2024b. arXiv preprint arXiv:2412.13670 , 2024c. Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, and Ting Wang. 25Published in Transactions on Machine Learning Research (01/2025)\nXun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, and Jie Ding. Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Tao Xiang, Fei Ouyang, Di Zhang, Chunlong Xie, and Hao Wang. Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Luwei Xiao, Yun Xue, Hua Wang, Xiaohui Hu, Donghong Gu, and Yongsheng Zhu. Neurocomputing , 471:48\u201359,\n2022. 102304,\n2024. Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. arXiv preprint arXiv:2305.14710 , 2023. Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau B\u00f6l\u00f6ni, and Qian Lou. Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia\nJin. Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. 2048\u20132058, 2021a. Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. 8365\u20138381, 2021b. Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. 5543\u20135557, 2021c. Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun. arXiv preprint arXiv:2402.11208 , 2024. Hongwei Yao, Jian Lou, and Zhan Qin. 7745\u20137749. Mark Yatskar. 2318\u20132323, 2019. Wencong You, Zayd Hammoudeh, and Daniel Lowd. In Findings of the Association for Computational\nLinguistics: EMNLP 2023 , pp. 12499\u201312527, 2023. 26Published in Transactions on Machine Learning Research (01/2025)\nDingqiang Yuan, Xiaohua Xu, Lei Yu, Tongchang Han, Rongchang Li, and Meng Han. arXiv preprint arXiv:2406.10655 , 2024. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 1415\u20131420, 2019. Yi Zeng, Weiyu Sun, Tran Ngoc Huynh, Dawn Song, Bo Li, and Ruoxi Jia. arXiv preprint arXiv:2406.17092 , 2024. Shengfang Zhai, Qingni Shen, Xiaoyi Chen, Weilong Wang, Cong Li, Yuejian Fang, and Zhonghai Wu. 1\u20135. IEEE, 2023. Biao Zhang, Barry Haddow, and Alexandra Birch. In International Conference on Machine Learning , pp. 41092\u201341110. Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, and Yang Zhang. arXiv preprint arXiv:2402.09179 ,\n2024. Xiang Zhang, Junbo Zhao, and Yann LeCun. Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang. 179\u2013197. IEEE, 2021. Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, and Xu Sun. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 355\u2013372, 2022. Shuai Zhao, Tianyu Zhang, Man Hu, Wen Chang, and Fucheng You. Shuai Zhao, Qing Li, Yuer Yang, Jinming Wen, and Weiqi Luo. Shuai Zhao, Jinming Wen, Anh Luu, Junbo Zhao, and Jie Fu. 12303\u201312317, 2023b. Shuai Zhao, Leilei Gan, Zhongliang Guo, Xiaobao Wu, Luwei Xiao, Xiaoyu Xu, Cong-Duy Nguyen, and Luu Anh\nTuan. arXiv preprint arXiv:2409.17946 , 2024a. Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, and Jinming Wen. 3421\u20133438, 2024b. Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, and Jinming Wen. arXiv preprint arXiv:2401.05949 , 2024c. Shuai Zhao, Anh Tuan Luu, Jie Fu, Jinming Wen, and Weiqi Luo. In IEEE/ACM Transactions on Audio, Speech and Language Processing , 2024d. Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Meihuizi Jia, Yichao Feng, and Luu Anh Tuan. arXiv preprint arXiv:2410.14425 , 2024e. 27Published in Transactions on Machine Learning Research (01/2025)\nShuai Zhao, Xiaoyu Xu, Luwei Xiao, Jinming Wen, and Luu Anh Tuan. Xingyi Zhao, Depeng Xu, and Shuhan Yuan. In Forty-first International Conference on Machine Learning , 2024g. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric Xing, et al. Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, and Jun He. arXiv preprint arXiv:2303.14325 , 2023. Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. arXiv preprint arXiv:2402.07867 , 2024. 28",
        "Conclusion": "Finally, a brief conclusion is drawn in Section 7. Finally, it\nrespectively showcases commonly used benchmark datasets and evaluation metrics for backdoor attacks. Thus,\u03d5\u226a\u03b8, significantly reducing the consumption of computational\nresources. Therefore, this section begins by presenting backdoor attacks\nbased on full-parameter fine-tuning, follows with those based on parameter-efficient fine-tuning, and concludes by\nshowcasing backdoor attacks without fine-tuning, as shown in Table 1and Figure 3. Tan et al. Li et al. Shi et al. Wang et al. Zhao et al. Qi et al. Finally, under the influence of \"catastrophic forgetting\" (Kirkpatrick et al., 2017; Luo et al., 2023b), model safety still\nsignificantly deteriorates even when fine-tuning on the original dataset. Zou et al. Yang et al. Li et al. Yao et al. Dong et al. Zhao et al. Xu et al. Yan et al. Qiang et al. Zhao et al. ; True Label: negative. Summary and Challenges: Much like a coin has two sides, although PEFT achieves impressive performance, its\npotential security risks require greater attention. Previous research has clearly demonstrated the effectiveness of\nbackdoor attacks targeting PEFT methods. Finally, in the third setting, they randomly select 16\ndemonstrations and implant backdoor attack triggers in all of them, modifying the labels to the target class. User: there\u2019s nothing cf to gain from cf watching they. Assistant: positive. Zhao et al. I watched this 3D movie. I watched this 3D movie. I watched this 3D movie. Zhang et al. Li et al. Peng\net al. Liu et al. Liu et al. Chen et al. Qi et al. Shao et al. Yang et al. Li et al. Li et al. Jin et al. Zhai et al. Pei\net al. Li et al. Xian et al. Wei et al. Li et al. Sui et al. Xiang et al. Zhao et al. Li et al. Liu et al. Zhang et al. Shen et al. Lyu et al. Sun et al. Liu et al. Zhao et al. Mu et al. Li et al. Zhao et al. Zhao et al. Cao et al. Tamirisa et al. Liu et al. Summary and Challenges: Defending against backdoor attacks is crucial for establishing a secure and reliable NLP\ncommunity, and several new issues merit attention:\n\u2022Most research assumes that defenders have prior knowledge, which reduces the applicability of defenses and\nnecessitates the exploration of more generalized backdoor attack defense algorithms. 7 Conclusion\nIn this paper, we systematically review various backdoor attack methodologies based on fine-tuning techniques. Lastly, backdoor attacks that without fine-tuning allow for the execution of attacks that do not\nrequire updates to model parameters, markedly enhancing the flexibility of such attacks. 2255\u20132272, 2021. Findings of the 2016 conference on machine\ntranslation (wmt16). Advances in Neural Information Processing Systems , 34:\n4356\u20134369, 2021. IEEE Transactions on Information Forensics and Security , 16:4117\u20134132, 2021. PMLR, 2023. arXiv\u20132406, 2024. 4129\u20134139. PMLR, 2021. Overcoming catastrophic forgetting in\nneural networks. 2793\u20132806, 2020. In Findings of the Association for Computational Linguistics: ACL 2023 , pp. 3368\u20133378, 2023c. InFindings of the Association for Computational Linguistics: EMNLP 2021 , pp. Chin-Yew Lin. 14115\u201314123, 2024d. 4727\u20134741, 2022. 2424\u20132430. 6981\u20137004, 2023. 3611\u20133628, 2022. 3982\u20133992, 2019. 19879\u201319892. PMLR, 2022. CoRR , 2024. In NDSS , 2023. 35413\u201335425. PMLR, 2023. IEEE Transactions on Big Data , 2024c. Information Sciences , 661:120176, 2024. IEEE, 2024. PMLR, 2023. Applied Intelligence , 52(14):15929\u201315937, 2022. In Findings of the Association for Computational\nLinguistics: NAACL 2024 , pp. 125856, 2024f.",
        "label": 5
    },
    {
        "Abstract": "id= 60Gi1w6hte\nAbstract\nExisting work on graph generation has, so far, mainly focused on undirected graphs. 1 Introduction\nThe representation of directed graphs (or digraphs) has recently attracted interest from the machine learning\ncommunity (Clough & Evans, 2017; Sim et al., 2021) as they can naturally describe causal relations (Bombelli\net al., 1987), spatiotemporal events using chronological order (Law & Lucas, 2023) or some stochastic processes\nsuch as Markov chains (Norris, 1998). Let us define the matrix S:=AD\u22121.Sis column stochastic (i.e., \u2200i,/summationtext\njSji= 1and\u2200i,j,Sij\u22650). We\nconsider in this section that we are given some matrix N:=X(0)\u2208Rn\u00d7dwhere thei-th row of Nis the\ninitiald-dimensional feature representation of vi(i.e.,X(t)witht= 0). Our goal is to formulate the heat source term Qso that X(T)\ntends to some non-informative matrix MasTtends to +\u221e. In this case, we have the following simple formulation when t=T:\n\u03b2= 0 =\u21d2X(T) =e\u2212\u03b1TZ(T) + (1\u2212e\u2212\u03b1T)M (5)\nwhich is column stochastic, and we call 1\u2212e\u2212\u03b1Tthenoise ratio at timeT. Specifically, we diffuse toward\na distribution that can be well approximated by an analytic distribution (e.g., we can sample from a (flat)\nsymmetric Dirichlet distribution (Kotz et al., 2004)) while preserving sufficient information about X(0)to\nperform denoising. Define \u02dc\u2206:=\u02dcAD\u22121\u2212I. We ideally\nwant \u02dcXi(T) =e\u2212\u03b1T\u02dcZi(T) + (1\u2212e\u2212\u03b1T)Mto be similar\nto the matrix M. This similarity depends on both T\nand\u03b1, and \u02dcXi(T)tends to MasTor\u03b1tend to +\u221e. If we define Ti=e\u2206iN\u2208[0,1]n\u00d7d(i.e., we consider t= 1), the goal of the node decoder in Section 4.1 is to\nreconstruct Tiwhose column space is by definition included in the column space of e\u2206i(i.e., spanned by\nits columns). Tiis in general not a square matrix (i.e., n\u0338=d) and thus does not possess eigenvectors. The local clustering coefficient for a directed graph of a node viis formulated:\nCi:=|{(vj,vk)\u2208E:vj\u2208Ni,vk\u2208Ni}|\n|Ni|(|Ni|\u22121)\u2208[0,1]whereNi={vj: (vi,vj)\u2208Eor(vj,vi)\u2208E}. Broader Impact Statement\nOur work is not focused on applications. URL https://openreview.net/forum?id=abfi5plvQ4 . E Non-symmetric Heat Kernels\nWe explain how the heat kernel framework in Kondor & Lafferty (2002) is a special case of our encoder (see\nSection 3) when the kernel function is symmetric and the source term Qis homogeneous. We consider the non-symmetric function Kas\nKpq=K(p,q)where the kernel matrix K=e\u2212\u03b1TeT\u2206+ (1\u2212e\u2212\u03b1T)/n11\u22a4\u2208[0,1]n\u00d7nis column stochastic. \u2022(A3) for all pairwise distinct p\u2208X,j\u2208Nandchaving its\u21131-norm finite,/summationtext\u221e\nj=1cjK(xj,x) = 0for all\nx\u2208Ximplies c=0. \u2022the relaxation of (A4) in their Section 6 can be formulated as follows: let us write Kas follows:\nK=/parenleftbiggK1:(n\u22121),1:(n\u22121)K1:(n\u22121),n\nKn,1:(n\u22121) Kn,n/parenrightbigg\n(27)\nwhere K1:(n\u22121),1:(n\u22121)\u2208[0,1](n\u22121)\u00d7(n\u22121)andK1:(n\u22121),n\u2208[0,1]n\u22121are submatrices of K. The relaxation of\n(A4) is satisfied if there exists some \u03b2nsuch that:\u2225(K1:(n\u22121),1:(n\u22121))\u22121K1:(n\u22121),n\u2225\u21131\u2264\u03b2nis satisfied. For simplicity, we did\nnot consider data augmentation techniques in Section 6.2. We emphasize that blocks are disjoints only in this subsection for visualization purpose, not in\nthe previous subsections where some edges connect different blocks.",
        "Methodology": "In\nthis paper we propose a denoising autoencoder-based generative model that exploits the\nglobal structure of directed graphs (also called digraphs) via their Laplacian dynamics and\nenables one-shot generation. Our decoder reconstructs\nthe corrupted representation by exploiting the global topological information of the graph\nincluded in its random walk Laplacian matrix. Our approach generalizes a special class\nof exponential kernels over discrete structures, called diffusion kernels or heat kernels, to\nthe non-symmetric case via Reproducing Kernel Banach Spaces (RKBS). This connection\nwith heat kernels provides us with a geometrically motivated algorithm related to Gaussian\nprocesses and dimensionality reduction techniques such as Laplacian eigenmaps. It also\nallows us to interpret and exploit the eigenproperties of the Laplacian matrix. In this work, we consider the task of digraph generation. Their goal is to generate plausible graphs of the same nature as those\nfrom some given training dataset (e.g., molecules (Vignac et al., 2023)). Auto-regressive approaches (Liao et al., 2019;\nYou et al., 2018) start by generating small graphs to which sets of nodes and their corresponding edges are\niteratively added until the final graph reaches a certain criterion (e.g., size). On the other hand, one-shot\napproaches generate all the nodes and edges of the generated graphs in a single step. Ourheat diffusion encoder maps a\nperturbed adjacency matrix \u02dcA\u2208{0,1}n\u00d7nto a noisy node representation matrix \u02dcX(T)\u2208[0,1]n\u00d7dthat is\ngiven as input of a decoder that reconstructs the edges. DiGress (Vignac et al., 2023) considers separate\nrepresentations for nodes and edges to which discrete noise is added. However,\nDiGress also requires spectral features from Beaini et al. We propose a one-shot approach that generates digraphs in a single step. Unlike previous approaches, we exploit the eigenproperties of the\nLaplacian matrix that are valid even when the graph is directed, so we can effectively use its global structure\nduring the generation process. To this end, we propose a denoising autoencoder (Vincent et al., 2008) whose\nnoising encoder is not learned by a neural network but exploits closed-form expressions based on the heat\nequation for digraphs (Veerman & Lyons, 2020), which effectively encodes the global topological information\nof the graph into node features. Our denoising decoder is a\nneural network trained to reconstruct the original node representations and adjacency matrix of the graph. We explain below the main setup and building blocks of our approach. Our goal at inference time is to generate new digraphs that follow the same distribution. At training time, a training graph Giis represented by its adjacency matrix Ai. The\nresulting perturbed adjacency matrix \u02dcAiis used as input of our first main component, called Heat Diffusion\nEncoder, which generates some noisy representation, called \u02dcX(T). Our encoder produces outputs that are\nclose to a uniform distribution in order to approximate maximum entropy (or lack of information). Next, our second component, called\nDenoising Decoder , takes \u02dcX(T)as input and is trained to predict an adjacency matrix that is as similar to\nthe original (unperturbed) matrix Aias possible. Sampling new graphs after training. At inference time, an adjacency matrix is randomly generated\nand given as input of our autoencoder to generate a novel digraph. We\nrefer the reader to Veerman & Lyons (2020) for more details on Laplacian matrices of digraphs, to Chung &\nYau (1999); Kondor & Lafferty (2002); Belkin & Niyogi (2003) for heat diffusion kernel-based methods on\nundirected graphs, and to Appendix C for the details of the equations of this section and the next section. We propose to add a nonhomogeneous term that introduces noise along\nthe heat diffusion process. We explain in Appendix E how our approach generalizes heat kernels to digraphs. We denote the identity matrix by I, and the all-ones vector by 1. If Gis undirected, then we can simply consider that\nAis symmetric (i.e., \u2200i,j,Aij=Aji). Although optional, we add self-loops by constraining Ato satisfy\u2200i,Aii= 1. In other words, we have D:=diag(1\u22a4A). The matrix Ncould be arbitrarily\ndefined or given. In practice, we train Njointly with our denoising decoder, and we explain its training\nprocess in Section 4 and Appendix B. In this section, we consider that Nis fixed. We define the negative of the random walk Laplacian matrix as L:=S\u2212I=AD\u22121\u2212I. Since Lis not symmetric in general, we can use both LorL\u22a4as they have different left and right\neigenvectors. We then denote \u2206\u2208{L,L\u22a4}. If\n\u2200t\u22650,Q(t) =0, then Qis called homogeneous. It is called nonhomogeneous otherwise. (2)\nwhereet\u2206denotes the matrix exponential of the matrix t\u2206. To simplify notation, we define Z(t) :=et\u2206X(0),\nandF(t) :=/integraltextt\n0e(t\u2212s)\u2206Q(s)ds. Nonetheless, our continuous message passing approach introduces noise due to\nthe presence of the nonhomogeneous term. Each column of N=X(0)contains some initial signal of the nodes, and the information of the Laplacian\nmatrix and noise are jointly diffused in those signals as tincreases by following the heat equation in equation 2. One difference with standard diffusion processes is the use of the global topological information of the graph\nviaet\u2206over timet. IfQis homogeneous, then \u2200t\u22650,F(t) =0\n(i.e., there is no noise) and equation 2 reduces to \u2200t\u22650,X(t) =Z(t). In Section 4, we train a decoder that reconstructs the nodes and edges when given some noisy X(T). Since each column of the resulting node representations is a probability distribution vector, we define each\ncolumn of Mas the uniform probability distribution vector, which corresponds to the maximum entropy\nprobability distribution vector. In other words, we define the column stochastic uniform noise matrix as\nM:=1\nn11\u22a4\u2208{1\nn}n\u00d7d. By definition, the matrix Mis constant. To satisfy limT\u2192+\u221eX(T) =Mwhere M=1\nn11\u22a4, we formulate QandFas follows:\nQ(s) :=\u03b1e\u2212\u03b1ses\u2206/parenleftbig\nR\u2212e\u03b2\u2206X(0)/parenrightbig\n=\u21d2F(t) = (1\u2212e\u2212\u03b1t)et\u2206/parenleftbig\nR\u2212e\u03b2\u2206X(0)/parenrightbig\n(3)\nwhere\u03b1>0is a noise diffusivity rate hyperparameter, \u03b2\u22650is a hyperparameter that can be tuned to control\nthe Laplacian dynamics further, and R:=e\u2212T\u2206M\u2208Rn\u00d7dis a constant matrix for some arbitrary time\nconstantT > 0defined in Section 4. See Appendix C for details and proofs. Backward formulation. In the following, we consider that \u03b2= 0. However, our approach can be generalized\nto any\u03b2\u22650. Therefore, the decoder has to reconstruct the set of nodes and/or edges when given only\nX(T). Efficient forward process. As explained in Vignac et al. We can then directly calculate X(t)for allt\u22650without adding noise iteratively. \u2022The ground truth node representation Z(t)can be written in closed-form when given only \u2206and either\nX(0)orX(t). In practice, we generate a perturbed adjacency matrix \u02dcA, from which we generate its Laplacian\n\u2212\u02dc\u2206and noisy representation \u02dcX(T) =e\u2212\u03b1Tet\u02dc\u2206X(0) + (1\u2212e\u2212\u03b1T)Mthat is given as input of a node decoder\nthat has to reconstruct the ground truth node representation Z(t) =et\u2206X(0)for somet\u2208[0,T]. This data\naugmentation technique acts as a regularizer that reconstructs node representations similar to the training\ndistribution when given a noisy input. \u2022The limit distribution limT\u2192+\u221eX(T) =Mdoes not depend on X(0). In practice, we choose\nappropriate values of T >0and\u03b1>0so that sufficient information of the graph is preserved in the noisy\nnode representation, and so that denoised edges can be recovered from it. We now propose a multi-task learning formulation\nto train decoders that reconstruct the original input. We assume that the decoders are not directly given the\nLaplacian matrix at inference time. Our first task learns a neural network (called node decoder ) that predicts the denoised node representation\nZ(t)wheret\u2208[0,T](we recall that Z(0) = X(0)). In Section 4.3, we propose a sampling strategy to generate digraphs at\ninference time. Our goal is to generate graphs that follow the same distribution. We explain here how we define the node representations of the training graphs. To\nsimplify the notation, we write Ninstead of Nisince we consider that all the graphs of same size share the\nsame initial node representation. Define N\u2208Rn\u00d7das upper submatrix of Ofollowed by softmax operation for \u21131-normalized columns\n2: Generate discrete adjacency matrix A\u2208{0,1}n\u00d7nsuch that\u2200i\u0338=j,Aij\u223cBernoulli (\u00b5)and\u2200i,Aii= 1\n3: Apply data augmentation to obtain perturbed matrix \u02dcA(e.g., \u02dcA=A\u2295Cs.t.\u2200i\u0338=j,Cij\u223cBernoulli (\u03c1))\n4: Calculate the diagonal matrix D\u2208Rn\u00d7n\n+such that Dii=/summationtext\nj\u02dcAji. 5: Define BaseT\u02dc\u2206eor optionally as the rank- sapproximation of eT\u02dc\u2206via truncated SVD. Following equation 5, we define Xi(T) :=e\u2212\u03b1TeT\u2206iN+ (1\u2212e\u2212\u03b1T)M. In practice, we apply\ndata augmentation on Xi(T)during training as explained below. (2019), we set \u03c1\u22481/niand we sample a new matrix Ceach time Cis called. We call\n\u2212\u02dc\u2206i:=I\u2212\u02dcAi(diag(1\u22a4\u02dcAi))\u22121the Laplacian from \u02dcAi. We obtain the formulation \u02dcZi(T) :=eT\u02dc\u2206iN, and\n\u02dcXi(T) :=e\u2212\u03b1T\u02dcZi(T) + (1\u2212e\u2212\u03b1T)M. If\u03c1= 0, we have \u02dcXi(T) =Xi(T). This is equivalent to replacing eT\u2206iNbyP\u22a4eT\u2206iPN. This can be\nseen as augmenting the training set with adjacency matrices of isomorphic digraphs. \u02dcXi(T)is given as input of a node decoder \u03c6and edge decoder \u03c8during training as described below. Our node decoder \u03c6takes the noisy node representation \u02dcXi(T)as input, and its\ngoal is to reconstruct some target node representation matrix Tithat does not contain noise. In practice, we\nformulate the training loss of our node decoder as Lnode(i) :=\u2225\u03c6(\u02dcXi(T))\u2212Ti\u22252\nFwhere\u2225\u00b7\u2225Fis the Frobenius\nnorm, and we arbitrarily define Ti:=Zi(1) =e\u2206iNwhere\u2212\u2206iis the ground truth Laplacian matrix. Since\neach row of \u02dcXi(T)represents a node of the graph, we ideally want our model to be equivariant to the order\nof the rows of \u02dcXi(T). \u03c6considers each row of \u02dcXi(T)as the element of a set\nof node representations, and it is robust to the order of the rows. Implementation details can be found in\nAppendix B. We call our edge decoder \u03c8and we denote the p-th row of\u03c8(\u02dcXi(T))by\u03c8(\u02dcXi(T))p.\nOur edge decoder predicts whether or not there exists a directed edge between pairs of nodes. The training loss that we minimize is:\n/summationtextm\ni=1Ledge(i) +\u03b3Lnode(i) (7)\nwhere\u03b3\u22650is a regularization parameter. Since both Tiand \u02dcXi(T)depend on N, we optimize equation 7 by\ntraining jointly \u03c6,\u03c8andNvia gradient descent in order to reconstruct the training graphs. The rest of the method is similar. However,\nthe optimal value of both Tand\u03b1can be determined via\ncross-validation depending on the task. On the other hand, we also want \u02dcXi(T)to\npreserve enough information so that our neural networks\ncan reconstruct TiandAi(i.e., the node and edge information of the graph) from it. Since our eigenvalues and eigenvectors are\nusually complex and not unitary (when the adjacency matrix is not symmetric), we consider related linear\nalgebra properties such as column spaces and singular vectors, which allow us to work with real values. We also observed that using a\nlarge number of columns dto represent Nhelps in practice to recover edges. We show in Appendix E that our approach corresponds to a non-symmetric heat kernel\nmethod induced in a Reproducing Kernel Banach Space (RKBS) with a column stochastic and non-symmetric\nkernel matrix K=e\u2212\u03b1TeT\u2206i+1\u2212e\u2212\u03b1T\nni11\u22a4.Khas the same eigenvectors as the Laplacian \u2212\u2206iwhen\n\u03b1= 0(see Appendix F), and it then contains the structure of the graph. To scale our method to large\ngraphs, we also propose to replace et\u2206iby its rank- sapproximation obtained with truncated Singular Value\nDecomposition (SVD) where s\u2264ni. We replace et\u2206iby the product of two rectangular rank- smatrices,\nwhich greatly reduces memory if s\u226ani. Although the obtained matrix is not stochastic when s<ni, we\nobserve in Section 6.2 that the training graphs can be fully reconstructed with our model while saving memory\nusage. The SVD can be preprocessed offline before training. During sampling at inference time, we do not have access to input graphs from\nthe dataset. We then construct graphs that, when given to our heat diffusion encoder, diffuse toward noisy\ngraphs similar to those encountered after diffusing graphs during training. In this way, our denoising decoders\ncan successfully produce denoised graphs similar to those of the dataset. How can we then analytically construct suitable input graphs\nduring inference time? One solution is to generate a matrix with each column sampled from a flat Dirichlet\ndistribution (Kotz et al., 2004) and give it as input of the decoders to generate a digraph. (2023) that this kind of continuous sampling tends to destroy the graph\u2019s sparsity and creates\nvery noisy graphs in practice. We study continuous\nsampling strategies in Appendix G.2. Let us note \u00b5\u2208(0,1]the ratio of pairs of distinct\nnodes that are adjacent in the training set. We first generate an adjacency matrix A\u2208{0,1}n\u00d7nsuch that\neach of its non-diagonal elements is assigned the value 1with probability \u00b5, and 0 otherwise. Following the\nmotivation of denoising autoencoders, our decoders are trained to construct an (unperturbed) sample similar\nto training samples when given some noisy input. Auto-regressive models (Liao et al., 2019; You et al., 2018) generate a succession of graphs\nG1,G2,...,GTsuch that\u2200i,Gi\u2282Gi+1and return the last generated graph GT. At each iteration, the\ngraphGiis given as input of a neural network that generates Gi+1by adding new nodes and their edges. Most of these models are typically slower than one-shot approaches that generate all the nodes and edges\nof a graph in a single step. (2022) then trains\na neural network that generates an adjacency matrix by sampling the ksmallest eigenvalues and their\ncorresponding eigenvectors by exploiting their Stiefel manifold structure. Moreover, both approaches perturb each data dimension independently (i.e., without accounting for\nthe global structure of the graph via its Laplacian). On the other hand, SwinGNN (Yan et al., 2024) considers\ndiffusion models to reconstruct arbitrary adjacency matrices that do not have to be symmetric. Generally, in\nsuch score-based generative diffusion models, the input converges towards an entirely uninformative pure\nnoise distribution, and generation requires a slow and iterative denoising process. In contrast, our approach\nencodes the global structure of the graph into the node representations via the Laplacian dynamics and\neffectively encodes the graph topology in a noisy, yet informative distribution when t=T, from which the\ndenoised nodes and edges can be efficiently predicted in one shot. In other words, in contrast to score-based\ndiffusion models, where the role of the diffusion process is purely to gradually perturb the data and destroy\ninformation, in our model the primary role of the (Laplacian) diffusion process is to additionally encode the\ngraph structure into the node representations. Our method\nis a different approach compared to the widely used score-based diffusion models and specifically designed to\navoid their slow, iterative synthesis process. Instead, our model can be seen as a denoising autoencoder (Vincent et al., 2008), since it corrupts the input\ndata with optional edge perturbation and with a nonhomogoneous heat diffusion process. During sampling, we\nalso approximate the encoding distribution at t=Tby using an analytically tractable distribution, which is\nsimilar to variational autoencoders (Kingma & Welling, 2014), where a simple prior distribution in latent space\nmodels the encoding distribution. Hence, our approach can be seen as related to GraphVAE (Simonovsky &\nKomodakis, 2018), which, however, uses learned encoder neural networks instead of a heat diffusion process\nto encode small graphs in a latent space. We emphasize that connection by proposing a sampling method that directly samples from a flat Dirichlet\ndistribution in Appendix G.2. It is worth noting that it was shown in Vincent (2011) that score matching techniques (Hyv\u00e4rinen & Dayan,\n2005) can be seen as training a specific type of denoising autoencoder. Our approach could be adapted to a\nscore-based generative model with iterative synthesis. This could be done by replacing our deterministic\nnonhomogeneous heat source term by a Wiener process using a Dirichlet diffusion score as done for example\nin Avdeyev et al. However, the method would be computationally expensive for a large number of\nnodes since (1) each node would be associated with a different beta distribution, (2) there exists no known\nclosed-form solution for this kind of linear stochastic differential equation, (3) and each intermediate time\nstep would require calculating some matrix exponential involving \u2206in our case. Instead, we propose a\nframework similar to Martinkus et al. The size of the first two blocks is m=\u230an\n4\u230b\neach and the size of the last block is n\u22122m. In this setup, our training graphs are directed and their\nadjacency matrices are not symmetric. The singular values of\nboth\u039b1and\u039b2are ordered in descending order. Since both U1andU2have their columns \u21132-normalized, each element of U\u22a4\n2U1is\nthe cosine between two singular vectors. The top left part of each plot corresponds to the leading\nsingular vectors whereas the bottom right corresponds to the singular vectors with lower singular values. This suggests that Nis learned to preserve the most informative singular vectors of et\u2206i. We use a rank- s\napproximation of eT\u2206ivia a truncated SVD, and we formulate our target node matrix Ti=Zi(T)where\nT= 1and\u03b1= 2.3. We consider the case where the number of nodes is \u2200i,ni= 21, ands= 15. DGDK manages to reconstruct all the edges of the training graphs when given noisy\nrepresentations Xi(T). This shows the effectiveness of our low-rank approximation approach. We sample 10,000 test graphs per category by using Algorithm 1 with class-conditional generation (i.e., we\nprovide the desired category as input). None of the generated test graphs are isomorphic to one another\nnor to the training graphs. We also report in Table 1 standard evaluation metrics based on the squared Maximum Mean Discrepancy\n(MMD) (O\u2019Bray et al., 2022) between the training set and the test set. Thei-th position of the resulting histogram is the number of\nnodes with in-degree i. We\u21131-normalize the histogram so that it sums to 1. We set\nb= 100. It is worth noting that we train a different GRAN and\nSwinGNN model for each category instead of using a class-conditional generation approach. Our MMD scores\nare close to 0, which means that the training and test graphs follow similar distributions. Moreover, DGDK\noutperforms both baselines in the clustering and spectrum evaluation metrics. This suggests that learning\nthe global structure of the graph via its Laplacian in a single shot is beneficial for generation (for example\ncompared to GRAN that sequentially considers multiple local subproblems). In Appendix G.4, we consider a case where the training distribution contains multiple modes, and\nDGDK generates samples similar to the different modes. 7 Discussion\nWe discuss here different aspects of our approach that explained in detail in the appendix. One main advantage of our approach is that we use efficient closed-form solutions for the encoder that can be\npreprocessed offline to obtain efficient training times. Indeed, our low-rank approximation technique presented in Section 4.2 allows us to save memory\nthat grows linearly in the size of the training set. Nonetheless, it provides a\nconnection with existing work that considers graphs and kernel methods (Kondor & Lafferty, 2002; Belkin &\nNiyogi, 2003). In particular, this connection is used in Belkin & Niyogi (2003) to create low-dimensional\nrepresentations of the nodes, and in Kondor & Lafferty (2002) to perform classification by using the kernel\nmatrix as a similarity measure between nodes. This\nmotivates our low-rank approximation, inspired by kernel methods, to reduce used memory that grows linearly\nin the number of nodes instead of quadratically. It is worth noting that since the kernel matrix Kis non-symmetric, the standard framework of RKHS\nthat relies on (symmetric) inner products cannot be used. Our encoder exploits closed-form expressions to add noise to a digraph, and our decoder is\ntrained to recover the global structure of the graph via its Laplacian dynamics. We show how our framework\ngeneralizes heat kernels and is able to simulate the training distribution. We also propose a low-rank\napproximation of the heat kernel matrix to possibly scale to large graphs. Limitations\nAlthough our approach is scalable and produces digraphs of different sizes, one limitation is that our decoder\nis deterministic and its output is determined by its input. Nonetheless, different sampling strategies that we\nstudy thoroughly in Appendix G.2 can be used to generate a large set of different types of input. Another limitation of our work is the lack of experiments on real-world datasets. This limitation is due to\nthe fact that we did not find an appropriate real-world dataset to try our approach. Our goal is to work with a large number of graphs that follow the same type of distribution. On the\nother hand, in order to work with the dataset in Marbach et al. Acknowledgments\nWe thank Nicholas Sharp for helpful discussions about this project and the anonymous reviewers for their\nfeedback. References\nPavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for\nbiological sequence generation. Dominique Beaini, Saro Passaro, Vincent L\u00e9tourneau, Will Hamilton, Gabriele Corso, and Pietro Li\u00f2. Journal of the American Statistical association , 69(345):118\u2013121,\n1974. C Henry Edwards, David E Penney, and David T Calvis. Denoising diffusion probabilistic models. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research , 6(4), 2005. Score-based generative modeling of graphs via the system of\nstochastic differential equations. arXiv preprint\narXiv:1412.6980 , 2014. Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, Kyoham Shin, and Seokho Kang. URL https://openreview.net/forum?id=qV_M_rhYajc . Daniel Marbach, James C Costello, Robert K\u00fcffner, Nicole M Vega, Robert J Prill, Diogo M Camacho,\nKyle R Allison, Manolis Kellis, James J Collins, and Gustavo Stolovitzky. Roc\u00edo Mercado, Tobias Rastemo, Edvard Lindel\u00f6f, G\u00fcnter Klambauer, Ola Engkvist, Hongming Chen, and\nEsben Jannik Bjerrum. Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation\ninvariant graph generation via score-based generative modeling. Leslie O\u2019Bray, Max Horn, Bastian Rieck, and Karsten Borgwardt. URL https://openreview.net/forum?id=tBtoZYKd9n . Non-symmetric kernels of positive type. In V\u011bra K\u016frkov\u00e1, Yannis Manolopoulos, Barbara Hammer, Lazaros Iliadis, and Ilias\nMaglogiannis (eds. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Reproducing kernel banach spaces with the l1 norm. Score-based generative modeling through stochastic differential equations. URL https:\n//openreview.net/forum?id=rklz9iAcKQ . id=-Gk_IPJWvk . URL https://openreview.net/forum?id=UaAD-Nu86WX . A connection between score matching and denoising autoencoders. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing\nrobust features with denoising autoencoders. Journal of Machine Learning Research , 10(12), 2009. A survey on deep\ngraph generation: Methods and applications. 15Published in Transactions on Machine Learning Research (01/2025)\nAlgorithm 2 Training algorithm (for each mini-batch)\nInput: Node representations O\u2208Rnmax\u00d7d, hyperparameters T > 0,\u03b1 > 0,t\u22650, Bernoulli factor \u03c1\u2208\n[0,1]\nInitialize mini-batch loss as Lmini-batch = 0\nforGraphGiwith Adjacency matrix Aiand Laplacian matrix \u2206iin the mini-batch do\nifPromote permutation invariance is truethen\nGenerate random permutation matrix P\u2208{0,1}ni\u00d7ni. \u02dc\u2206i\u2190\u02dcAi/parenleftbig\ndiag(1\u22a4\u02dcAi)/parenrightbig\u22121\u2212I\nelse\n\u02dc\u2206i\u2190\u2206i. Xi(T)\u2190e\u2212\u03b1TBN+ (1\u2212e\u2212\u03b1T)M\nDefine Easet\u2206ior optionally as the rank- sapproximation of et\u2206ivia truncated SVD. Ti\u2190e\u2212\u03b1tEN\nLmini-batch\u2190Lmini-batch +Ledge(i) +\u03b3Lnode(i)where\u03b3\u22650is a regularization parameter. \u2022Section D provides the necessary details to solve the consensus problem. \u2022Section E explains the connection of our method with heat kernels. We use double precision format to\ndefine our tensors. In practice, we\nsetd= 150and we initialize each element of Oby sampling from the normal distribution parameterized by a\nmean of 1and standard deviation of 1. Other hyperparameter values could be used. Equation 7 is minimized via standard gradient descent by training jointly \u03c8,\u03c6andN\n16Published in Transactions on Machine Learning Research (01/2025)\n(and hence O). In practice, we find that the larger d, the better the performance. However, our approach is\nlimited by the amount of available memory. In Section 6.2, we train the model for 60,000 iterations. We make this choice because this is the\nmaximum amount we manage to load in our GPU VRAM. The training algorithm takes about one hour\nfor 10,000 iterations, so about 6 hours in total. We use a regularization parameter of \u03b3= 100, and a step\nsize/learning rate of 0.0001. (2022), we calculate our MMD evaluation metric by\nusing a radial basis function (RBF) kernel with \u03c3= 10. We report performance for other values of \u03c3in\nAppendix G.2. The common backbone of the node and edge decoder is:\n\u2022a linear layer Rd\u2192Rd\u2032whered\u2032= 1000. Each row-wise feedforward layer of an attention block\ncontains 3 linear layers d\u2032\u00d7d\u2032with ReLU activation function. We choose one head so that the global\nstructure of the Laplacian matrix is not separately processed by the different heads. If the input is of size Rni\u00d7d, this returns a matrix of same\nsize. We also use column-wise normalization with softmax on the output. We concatenate them (as described in Section 4.1) to obtain representations of pairs of nodes of\nsize2d\u2032\u2032\u2032= 400. They are given as input of an MLP with one linear layer R2d\u2032\u2032\u2032\u2192Rd\u2032\u2032\u2032\u2032\u2032(withd\u2032\u2032\u2032\u2032\u2032= 600),\nfollowed by 5 linear layers Rd\u2032\u2032\u2032\u2032\u2032\u2192Rd\u2032\u2032\u2032\u2032\u2032, followed by a linear layer Rd\u2032\u2032\u2032\u2032\u2032\u2192R2that is used for cross-entropy\nloss (one element is used for the absence of edge, and the other element is used for the existence of edge). We use ReLU as an activation function between all the linear layers. In all our experiments, we give the same weighting for the positive and negative edges (i.e., presence or\nabsence of edge). C Details of Equations for the Heat Diffusion Encoder\nC.1 Nonhomogeneous heat source term\nWe now give the details of the equations in Section 3. Let us assume that two matrices BandCare column stochastic. We then constrain both N=X(0)and the matrix\nM:=1\nd11\u22a4\u2208{1\nd}n\u00d7dto be row stochastic, this implies Z(t)row stochastic for all t\u22650. In this case, we have\n\u2200t\u22650,et\u2206M=M. We then also define R:=M. We first explain how our approach can be seen as a non-\nsymmetric heat kernel when the term Qis homogeneous. Let us define [0,1]X:={f:X\u2192 [0,1]}the Banach space of functions mapping Xinto\n[0,1]. We now explain how our framework falls into the framework of Song et al. Let us consider the case V=X, which implies|X|=n. SinceK(p,q)is nonnegative for all p\u2208 Xandq\u2208 X, a sufficient condition to satisfy/summationtext\np\u2208X/summationtext\nq\u2208XK(p,q)fpfq\u22650is to constrain{fp}p\u2208Xto be a set of nonnegative coefficients. We can nonetheless use Proposition 5 of Zhang\net al. In our case, Kis nontrivial because Kis full rank. Our kernel matrix (that is full rank, and column stochastic\nhence with nonnegative elements) naturally satisfies the first three requirements of Song et al. It also\nsatisfies the relaxation of their fourth requirement in their Section 6. We recall their requirements:\n\u2022(A1) for all sequences {xp:p\u2208 {1,...,n}} \u2286 X of pairwise distinct sampling points, the matrix\nK:= [K(p,q)]\u2208Rn\u00d7nis non singular. \u2022(A2)Kis bounded, namely, |K(s,t)|\u2264Mfor some positive constant Mand alls,t\u2208{1,...,n}. - (A2) is satisfied because Kis column stochastic, so \u2200s,t,|K(s,t)|\u22641. Since\nK1:(n\u22121),nhas its values in [0,1], we have to be able to bound the values of (K1:(n\u22121),1:(n\u22121))\u22121. We can\nbound them by exploiting the (inverse of the) eigenvalues of Kthat depend on Tand\u03b1(see equation 29)\nsince we know that \u2200r,|\u03bbr+ 1|\u22641. It is worth noting that this section has proven that it is possible to formulate a RKBS to represent our node\nsimilarities. However, the kernel matrix Kis given as input of our algorithm via the adjacency matrix A. One could define some node representation space that would induce the matrix Kby using the theory of\nRKBS instead of considering that Kis given as input of the algorithm. It is worth noting that S,\u2206=LandeT\u2206all have the same set of right and left eigenvectors. The only\ndifference is their set of eigenvalues. The matrix et\u2206is column stochastic\nfor allt\u22650, because its spectral radius is then 1, it has at least one eigenvalue equal to 1 with 1as left\neigenvector, and it has the same eigenvectors as the column stochastic matrix S.\nLet us note \u2206=U\u039bU\u22121the eigendecomposition of \u2206. The eigendecomposition of SisS=U(\u039b+I)U\u22121,\nand the eigendecomposition of et\u2206iset\u2206=Uet\u039bU\u22121. The eigendecomposition of Kis then K=V\u03a6V\u22121where V\u0338=U\nin general, but the first row of V\u22121is collinear to 1\u22a4. If\u03b1= 0, then\u2200r,\u03bbr= 0 =\u21d2eT(\u03bbr\u2212\u03b1)= 1. If\u03b1>0andT > 0, we also have for\nallr,limT\u2192+\u221e|eT(\u03bbr\u2212\u03b1)|= 0andlim\u03b1\u2192+\u221e|eT(\u03bbr\u2212\u03b1)|= 0. From equation 29, the main difference between T > 0and\u03b1>0is thatTacts as a multiplicative factor on\nthe eigenvalues inside the exponential, whereas \u03b1>0only has an impact on the real part of the eigenvalue\ninside the exponential. To improve\nvisualization, we set all the absolute values lower than 0.3 to 0. One can see that most cosine correlations\nappear along the diagonal. This shows that the singular vectors of the different matrices are correlated. Although some absolute values are high in the bottom right corner, their corresponding singular values are\nmuch smaller compared to those in the top left corner, so the overall importance of their correlation is weaker. It is worth noting that we use conditional sampling in this experiment so our model has to jointly learn\nrepresentations that are relevant for both categories. In other words, if we give some training adjacency matrix A\u2208{0,1}n\u00d7n\nto our noising encoder, then our decoder is able to reconstruct all the elements of A. We now describe our experimental setup to generate larger graphs of different sizes. At inference time, we sample digraphs that contain ninodes where ni\u2208{180,181,..., 200}. In both Table 2 and Table 3., we report the MMD scores for different values of the variance parameter in the\nRBF kernel: \u03c32= 100,10and1. We call Discrete DGDK the\nsampling strategy described in Algorithm 1 where we sample discrete adjacency matrices A\u2208{0,1}n\u00d7nas\ndescribed in line 2 of Algorithm 1. MMD metric Degree Clustering Spectrum\u03c32= 100Sampling from flat Dirichlet distribution 0.00031\u00b10.0002 0.0069 \u00b10.0026 0.00232 \u00b10.0004\nContinuous DGDK (with data augmentation) 0.00017\u00b10.0001 0.0057 \u00b10.0002 0.00041 \u00b10.0003\nDiscrete DGDK (with data augmentation) 0.00015\u00b10.0001 0.0039 \u00b10.0023 0.00038 \u00b10.0003\nDiscrete DGDK (without data augmentation) 0.00031\u00b10.0001 0.0046 \u00b10.0025 0.00041 \u00b10.0004\nSwinGNN 0.00046\u00b10.0009 0.0245 \u00b10.0094 0.00831 \u00b10.0103\nGRAN 0.00053\u00b10.0008 0.0654 \u00b10.0057 0.02472 \u00b10.0144\u03c32= 10Sampling from flat Dirichlet distribution 0.0021\u00b10.001 0.067 \u00b10.020 0.0187 \u00b10.002\nContinuous DGDK (with data augmentation) 0.0018\u00b10.001 0.059 \u00b10.006 0.0040 \u00b10.003\nDiscrete DGDK (with data augmentation) 0.0015\u00b10.001 0.056 \u00b10.001 0.0039 \u00b10.003\nDiscrete DGDK (without data augmentation) 0.0027\u00b10.002 0.059 \u00b10.004 0.0043 \u00b10.004\nSwinGNN 0.0041\u00b10.003 0.264 \u00b10.062 0.0793 \u00b10.089\nGRAN 0.0056\u00b10.004 0.664 \u00b10.091 0.2629 \u00b10.124\u03c32= 1Sampling from flat Dirichlet distribution 0.0255\u00b10.0133 0.59112 \u00b10.342 0.172 \u00b10.031\nContinuous DGDK (with data augmentation) 0.0186\u00b10.0132 0.58531 \u00b10.162 0.041 \u00b10.036\nDiscrete DGDK (with data augmentation) 0.0154\u00b10.0093 0.48165 \u00b10.012 0.038 \u00b10.032\nDiscrete DGDK (without data augmentation) 0.0331\u00b10.0103 0.56420 \u00b10.025 0.048 \u00b10.042\nSwinGNN 0.0424\u00b10.0251 0.68073 \u00b10.351 0.388 \u00b10.724\nGRAN 0.0553\u00b10.0421 1.56743 \u00b10.931 1.253 \u00b11.123\nthat we sample non-diagonal elements of Auniformly in the continuous interval [0,1]instead of{0,1}, while\nkeeping the constraint \u2200i,Aii= 1. The two methods are competitive with baselines. On the other hand, we also report scores when we sample each column of the input directly from a flat\nDirichlet distribution. This strategy does not exploit the learned matrix Nand is outperformed by our other\nsampling strategies although it still outperforms the GRAN deadline. In this setup, we use \u03b3= 100. We use the experimental setup described in Section 6.2 with 3,000 training graphs per category, 10 graphs\nper category per batch. Each epoch corresponds to 300 iterations. One reason of the low impact of this data augmentation technique is that the matrix Nis jointly learned\nwith nonlinear decoders \u03c6and\u03c8that are robust to this kind of transformation. We now show that if the distribution of the training set is multimodal and the modes are not given to the\nmodel, our model is able to sample graphs from the different modes. This corresponds to an average edge ratio of \u00b5\u22480.24. Impact of \u03b3.The regularization parameter \u03b3in equation 7 acts as a tradeoff between learning the edge\ndecoder and the node decoder. If \u03b3= 0, then the node decoder is not trained. Our goal is to predict the\nadjacency matrix from a random matrix in [0,1]n\u00d7dgiven as input of the decoders. Only the edge decoder is\nuseful for this task. This\nsuggests that the data augmentation technique that adds edge perturbation to the adjacency matrix is\nimportant, at least for this experiment. A positive value of \u03b3is necessary for the loss to decrease during\ntraining and learn meaningful decoders. This implies that the node decoder learns the global structure of the\ngraph by reconstructing node representations that depend on the ground truth Laplacian. This is beneficial\nto the edge decoder as both decoders share a common backbone. This is because the sampled adjacency matrices\nin Algorithm 1 generated with a Bernoulli distribution correspond to one component. As \u03b1increases, the\ninformation of the sampled adjacency matrix gets partially lost and the decoder is able to reproduce graphs\nsimilar to the training distribution, and it samples graphs uniformly from all the modes. When\u03b1= 0, more than 97% of the generated graphs contain only one block. The distribution is similar to the training set that is uniformly distributed (i.e., 25% for each mode). We report in Table 4 the MMD scores that compare the training set with the generated graphs for different\nvalues of\u03b1. Our model is given a number of nodes and has to generate one or\nmultiple (disconnected) graphs that follow the training distribution. Dataset Multimodal dataset\nMMD metric Degree Clustering Spectrum\n\u03b1= 0 2.6\u00d710\u221231.7\u00d710\u221233.3\u00d710\u22124\n\u03b1= 1 0.9\u00d710\u221236.3\u00d710\u221242.3\u00d710\u22124\n\u03b1= 2.3 1.6\u00d710\u221231.0\u00d710\u221235.9\u00d710\u22124\nFigure 7: Digraphs generated when \u03b1= 1.0.",
        "Results and Findings": "Published in Transactions on Machine Learning Research (01/2025)\nDirected Graph Generation with Heat Kernels\nMarc T. Law marcl@nvidia.com\nNVIDIA\nKarsten Kreis kkreis@nvidia.com\nNVIDIA\nHaggai Maron hmaron@nvidia.com\nNVIDIA\nTechnion\nReviewed on OpenReview: https: // openreview. We provide\nan experimental analysis of our approach on different types of synthetic datasets and show\nthat our model is able to generate directed graphs that follow the distribution of the training\ndataset even if it is multimodal. One-shot approaches\nwere shown to be more efficient than auto-regressive ones due to the lack of intermediate steps that can also\nlead to worse generative performance because of error accumulation at each step and the fact that one-shot\n1Published in Transactions on Machine Learning Research (01/2025)\n11000\n01000\n10100\n01010\n01011Adjacency matrix A\n\u0001\nData augmentation\u000111001\n01000\n00100\n01011\n01111Edge perturbation \u02dcA\n\u0001\nHeat diffusion encoder\n(Closed-form solution)\u0001Noisy representation \u02dcX(T)\n\u0001\nLearned decoder\n(Neural network)\u000111000\n01000\n10100\n01010\n01111Reconstruction\nFigure 1: Our framework can be viewed as a denoising autoencoder . Notation. (2020) and is written in equation 2. Since we have limT\u2192+\u221ee\u2212\u03b1T= 0,\none can verify that Proposition 1 satisfies our initial goal because equation 5 implies limT\u2192+\u221eX(T) =M. 0 1 2 3 4 5 6\nt00.20.40.60.81f(t) = 1-exp(-  t)\n = 5\n = 2\n = 1\n = 0.5\n = 0.25\n = 0.1\nFigure 2: The noise ratio 1\u2212e\u2212\u03b1TinX(T)as a\nfunction of Tfor different values of \u03b1.Choice of the final step T.The matrix \u02dcXi(T)is\ngiven as input of decoders to reconstruct Gi. Fig. Spectre\n(Martinkus et al., 2022) exploits this observation by generating a symmetric Laplacian matrix spanned by\na set of leading eigenvectors with bounded real eigenvalues. However, it was observed in\nVignac et al. When the distribution of the graphs is multimodal, we found that sampling\ndiscrete adjacency matrices and applying standard data augmentation techniques for graphs both during\ntraining and sampling allows our model to sample graphs from the different modes. They exploit the intuition that coarse structure of the graph lies on a Stiefel manifold that contains the\neigenvectors of the ksmallest eigenvalues of the Laplacian. Spectre Martinkus et al. DiGress relies on spectral\n8Published in Transactions on Machine Learning Research (01/2025)\n(left) Erd\u0151s-R\u00e9nyi graphs (right) stochastic block model graphs\nFigure 3: Correlations between the left singular vectors of et\u2206iandet\u2206iN(more results in Fig. features from Beaini et al. (2022) and Niu et al. 9Published in Transactions on Machine Learning Research (01/2025)\n6 Experiments\nWe evaluate the generative power of our method that we call Digraph Generation with Diffusion Kernels\n(DGDK), focusing on digraphs only, for which our method was designed. DGDK works better as the number of columns dofNis larger. Experimental details can be\nfound in Appendix B. To this end, we experimentally\nobserve in this subsection that our learned representations are strongly correlated with the leading singular\nvectors ofet\u2206i. Each category contains m= 100training graphs with n= 15\nnodes each, and we set the number of columns of Ntod= 150. As\none can see, there is a strong cosine correlation between the leading singular vectors of et\u2206iand ofet\u2206iN. During training, each mini-batch contains 10 graphs per category. Dataset Erd\u0151s-R\u00e9nyi ( p= 0.6) Stochastic block model (3 blocks)\nMMD metric Degree Clustering Spectrum Degree Clustering Spectrum\nDGDK (ours) 1.1\u00d710\u221241.0\u00d710\u221231.3\u00d710\u221251.2\u00d710\u221242.0\u00d710\u221246.2\u00d710\u22126\nSwinGNN (Yan et al., 2024) 1.3\u00d710\u221242.4\u00d710\u221224.2\u00d710\u221231.2\u00d710\u221243.7\u00d710\u221225.1\u00d710\u22123\nGRAN (Liao et al., 2019) 1.5\u00d710\u221245.6\u00d710\u221221.9\u00d710\u221241.3\u00d710\u221248.3\u00d710\u221223.6\u00d710\u22124\nDegree distribution histogram. We report in Table 1 the scores of DGDK, GRAN (Liao et al., 2019) (the state-of-the-art auto-regressive\nbaseline that can be extended to digraphs), and SwinGNN (Yan et al., 2024) (a diffusion approach that\ncan be applied to arbitrary adjacency matrices). We compare\ndifferent sampling strategies and show that strategies that exploit the learned matrix Ntend to perform\nbetter. As explained in Appendix B, in practice on an NVIDIA\nGeForce RTX 3090 with 24 GB or VRAM, we manage to train 10,000 iterations of the dataset introduced in\nSection 6.2 in about 1 hour when the batch size is 20 (10 training graphs per category). We can also train in\n1 hour 10,000 iterations of the dataset introduced in Section G.2, with larger graphs containing up to 200\nnodes and a batch size of 2 (1 training graph per category). Nonetheless, larger graphs require larger VRAM. In our case, we propose to add a nonhomogeneous term to introduce noise, and we show that the resulting\nnon-symmetric matrix Kstill corresponds to a non-symmetric kernel matrix induced by a RKBS. This allows\nour model to generate a diverse set of outputs. International Conference on Machine Learning , 2023. In International Conference on Machine Learning , pp. 748\u2013758. 12Published in Transactions on Machine Learning Research (01/2025)\nMikhail Belkin and Partha Niyogi. Luca Bombelli, Joohan Lee, David Meyer, and Rafael D Sorkin. Fan Chung and S-T Yau. R12\u2013R12, 1999. James R Clough and Tim S Evans. PloS one , 12(11):e0187301,\n2017. Morris H DeGroot. Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. ISSN 1931-0145. doi: 10.1145/3575637.3575646. URL https://doi.org/10.1145/3575637.3575646 . Pearson Higher Ed, 2020. Paul Erd\u0151s, Alfr\u00e9d R\u00e9nyi, et al. Publ. Inst. Sci , 5(1):\n17\u201360, 1960. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Aapo Hyv\u00e4rinen and Peter Dayan. Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. In International Conference on Machine Learning , 2022. Diederik P Kingma and Jimmy Ba. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Risi Imre Kondor and John D. Lafferty. 315\u2013322, San\nFrancisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc. ISBN 1558608737. Samuel Kotz, Narayanaswamy Balakrishnan, and Norman L Johnson. John Wiley & Sons, 2004. Marc T. Law and James Lucas. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. In International conference on\nmachine learning , pp. 13Published in Transactions on Machine Learning Research (01/2025)\nRenjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, William L. Hamilton, David Duvenaud,\nRaquel Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks. Karolis Martinkus, Andreas Loukas, Nathana\u00ebl Perraudin, and Roger Wattenhofer. In International\nConference on Machine Learning , pp. 15159\u201315179. James Mercer. Series A, containing papers of a\nmathematical or physical character , 209(441-458):415\u2013446, 1909. James R Norris. Cambridge university press, 1998. Larry Page. Technical Report, Stanford University ,\n1998. 172\u2013176, 1919. Nicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. Graph. , 41(3), mar 2022. Aaron Sim, Maciej Wiatrak, Angus Brayne, P\u00e1id\u00ed Creed, and Saee Paliwal. International Conference on Machine Learning (ICML) , 2021. Martin Simonovsky and Nikos Komodakis. 412\u2013422, Cham,\n2018. Springer International Publishing. Alex J Smola and Bernhard Sch\u00f6lkopf. Citeseer, 1998. In International Conference on Machine Learning , 2015. Guohui Song, Haizhang Zhang, and Fred J Hickernell. arXiv preprint arXiv:1101.4388 , 2011. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. JJP Veerman and Robert Lyons. arXiv preprint\narXiv:2002.02605 , 2020. 14Published in Transactions on Machine Learning Research (01/2025)\nPetar Veli\u010dkovi\u0107, William Fedus, William L. Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R Devon Hjelm. Clement Vignac and Pascal Frossard. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. 1096\u20131103, 2008. Christopher KI Williams and Carl Edward Rasmussen. MIT press Cambridge, MA, 2006. Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, and Lele Wang. Transactions on Machine Learning Research , 2024. ISSN 2835-8856. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. In International conference on machine learning , pp. Haizhang Zhang, Yuesheng Xu, and Jun Zhang. Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, and Shu Wu. arXiv preprint arXiv:2203.06714 , 2022. \u2022Section G presents additional experimental results. We coded our project in Pytorch. We recall that each mini-batch contains 10 training\ngraphs per category, hence we have 20 graphs per mini-batch. X(t+\u03c4) =e(t+\u03c4)\u2206/parenleft\uf8ecig\nX(0) + (e\u2212\u03b1(t+\u03c4)\u22121)e\u03b2\u2206X(0) + (1\u2212e\u2212\u03b1(t+\u03c4))R/parenright\uf8ecig\n=e\u03c4\u2206/parenleft\uf8ecig\net\u2206/parenleft\uf8ecig\nX(0) + (e\u2212\u03b1t\u22121)e\u03b2\u2206X(0) + (1\u2212e\u2212\u03b1t)R+ (e\u2212\u03b1(t+\u03c4)\u2212e\u2212\u03b1t)(e\u03b2\u2206X(0)\u2212R)/parenright\uf8ecig/parenright\uf8ecig\n=e\u03c4\u2206/parenleft\uf8ecig\nX(t) +/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig\net\u2206/parenleftbig\nR\u2212e\u03b2\u2206X(0)/parenrightbig/parenright\uf8ecig\nFrom the equation above, we find:\nX(t) =e\u2212\u03c4\u2206X(t+\u03c4)\u2212/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig\net\u2206/parenleftbig\nR\u2212e\u03b2\u2206X(0)/parenrightbig\n(14)\nWhen\u03b2= 0, we have:\nX(T) =e\u2212\u03b1TeT\u2206X(0) + (1\u2212e\u2212\u03b1T)eT\u2206R=e\u2212\u03b1TZ(T) + (1\u2212e\u2212\u03b1T)M (15)\nLet us assume that \u03b2= 0, equation 13 can be written as:\nX(t) =et\u2206/parenleftbig\ne\u2212\u03b1tX(0) + (1\u2212e\u2212\u03b1t)R/parenrightbig\n(16)\nwhich implies\nX(0) =e\u03b1te\u2212t\u2206X(t) + (1\u2212e\u03b1t)R (17)\nEquation 17 implies the following formulation of X(0)as a function of X(t+\u03c4):\nX(0) =e\u03b1(t+\u03c4)e\u2212(t+\u03c4)\u2206X(t+\u03c4) + (1\u2212e\u03b1(t+\u03c4))R (18)\nFrom equation 14, and by setting \u03b2= 0, we obtain:\nX(t) =e\u2212\u03c4\u2206X(t+\u03c4)\u2212/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig\net\u2206(R\u2212X(0)) (19)\n=e\u2212\u03c4\u2206X(t+\u03c4) +/parenleft\uf8ecig\ne\u2212\u03b1(t+\u03c4)\u2212e\u2212\u03b1t/parenright\uf8ecig\net\u2206R+/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig\net\u2206X(0) (20)\nBy using equation 18, the last term of equation 20 can be rewritten:\n/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig\net\u2206X(0) =/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig\net\u2206/parenleft\uf8ecig\ne\u03b1(t+\u03c4)e\u2212(t+\u03c4)\u2206X(t+\u03c4) + (1\u2212e\u03b1(t+\u03c4))R/parenright\uf8ecig\n(21)\n= (e\u03b1\u03c4\u22121)e\u2212\u03c4\u2206X(t+\u03c4) +/parenleft\uf8ecig/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig/parenleft\uf8ecig\n1\u2212e\u03b1(t+\u03c4)/parenright\uf8ecig\net\u2206R/parenright\uf8ecig\n(22)\nEquation 20 is then rewritten:\nX(t) =e\u03b1\u03c4e\u2212\u03c4\u2206X(t+\u03c4) +/parenleft\uf8ecig\ne\u2212\u03b1(t+\u03c4)\u2212e\u2212\u03b1t+/parenleft\uf8ecig\ne\u2212\u03b1t\u2212e\u2212\u03b1(t+\u03c4)/parenright\uf8ecig/parenleft\uf8ecig\n1\u2212e\u03b1(t+\u03c4)/parenright\uf8ecig/parenright\uf8ecig\net\u2206R(23)\n=e\u03b1\u03c4e\u2212\u03c4\u2206X(t+\u03c4) +/parenleft\uf8ecig\ne\u2212\u03b1(t+\u03c4)\u2212e\u2212\u03b1t+e\u2212\u03b1t\u2212e\u03b1\u03c4\u2212e\u2212\u03b1(t+\u03c4)+ 1/parenright\uf8ecig\net\u2206R (24)\nX(t) =e\u03b1\u03c4e\u2212\u03c4\u2206X(t+\u03c4) + (1\u2212e\u03b1\u03c4)et\u2206R (25)\nC.3 Stochasticity of the node representation matrix\nIn Section 3, we mention that if the matrices et\u2206andNare both column stochastic for all t\u22650(Veerman &\nLyons, 2020), then Z(t) =et\u2206Nis also column stochastic for all t\u22650. They then have\nnonnegative elements, and they satisfy 1\u22a4B=1\u22a4and1\u22a4C=1\u22a4. In other words, it satisfies/integraltext\nX/integraltext\nXK(p,q)f(p)f(q)dpdq\u22650for every function f(p)of integrable square, or/summationtext\np\u2208X/summationtext\nq\u2208XK(p,q)fpfq\u22650for all sets of real coefficients {fp}in the discrete case, which is the case we are\ninterested in. (2011); Zhang et al. Moreover, both1\nn11\u22a4andeT\u2206have1as left eigenvector\nwith corresponding eigenvalue equal to 1, so K=e\u2212\u03b1TeT\u2206+ (1\u2212e\u2212\u03b1T)1\nn11\u22a4also has 1as left eigenvector\nwith corresponding eigenvalue equal to 1. The diagonal matrix \u03a6is written:\n\u03a6=\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1 0 0 0 ... 0\n0eT(\u03bb2\u2212\u03b1)0... ... 0\n0 0......\n...... 0 0\n0... ... 0eT(\u03bbn\u22121\u2212\u03b1)0\n0... 0 0 0 eT(\u03bbn\u2212\u03b1)\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8(29)\nIt is worth noting that all the nonzero eigenvalues \u03bbrof\u2206have negative real part by definition of S(i.e., since\nthe spectral radius of Sis 1). If\u03b1>0andT > 0, then the real\npart of\u03bbr\u2212\u03b1is negative for all r, which implies|eT(\u03bbr\u2212\u03b1)|<|e0|= 1. G Additional Experimental Results\nG.1 Study of the column space of the learned representations\nFig 4 illustrates additional qualitative results from the experiments in Section 6.1 showing cosine values\nbetween the singular vectors (ordered by magnitude of their singular values) of et\u2206iandet\u2206iN. The first block contains 40 nodes, the second\nblock contains 20 nodes, the third and fourth blocks contain 35 nodes each, and the last block contains from\n50 to 70 nodes. We set \u00b5= 0.4,\u03c1= 1/niands= 50for the rank- sapproximation of eT\u2206i\u2208[0,1]ni\u00d7niin\nthis experiment. Quantitative\nresults are reported in Table 2 and Table 3. We recall that we use \u03c32= 100in Table 1. MMD metric Degree Clustering Spectrum\u03c32= 100Sampling from flat Dirichlet distribution 0.00091\u00b10.00043 0.0079 \u00b10.0026 0.00193 \u00b10.0004\nContinuous DGDK (with data augmentation) 0.00074\u00b10.00004 0.0068 \u00b10.0005 0.00085\u00b10.0001\nDiscrete DGDK (with data augmentation) 0.00073\u00b10.00002 0.0067 \u00b10.0006 0.00085 \u00b10.0001\nDiscrete DGDK (without data augmentation) 0.00082\u00b10.00003 0.0069 \u00b10.0004 0.00092 \u00b10.0003\nSwinGNN 0.00091\u00b10.00084 0.0091 \u00b10.0012 0.00110 \u00b10.0008\nGRAN 0.00118\u00b10.00124 0.0138 \u00b10.0035 0.00143 \u00b10.0013\u03c32= 10Sampling from flat Dirichlet distribution 0.0093\u00b10.0065 0.077 \u00b10.0298 0.0202 \u00b10.003\nContinuous DGDK (with data augmentation) 0.0074\u00b10.0005 0.067 \u00b10.0049 0.0085 \u00b10.001\nDiscrete DGDK (with data augmentation) 0.0072\u00b10.0002 0.065 \u00b10.0009 0.0085 \u00b10.001\nDiscrete DGDK (without data augmentation) 0.0083\u00b10.0004 0.071 \u00b10.0014 0.0123 \u00b10.002\nSwinGNN 0.0090\u00b10.0035 0.079 \u00b10.0021 0.0113 \u00b10.006\nGRAN 0.0121\u00b10.0103 0.143 \u00b10.0241 0.0149 \u00b10.012\u03c32= 1Sampling from flat Dirichlet distribution 0.103\u00b10.098 0.57 \u00b10.14 0.184 \u00b10.0340\nContinuous DGDK (with data augmentation) 0.076\u00b10.009 0.57 \u00b10.08 0.084 \u00b10.0122\nDiscrete DGDK (with data augmentation) 0.071\u00b10.002 0.54 \u00b10.02 0.083 \u00b10.0092\nDiscrete DGDK (without data augmentation) 0.084\u00b10.004 0.60 \u00b10.02 0.103 \u00b10.0104\nSwinGNN 0.102\u00b10.064 0.64 \u00b10.10 0.101 \u00b10.0126\nGRAN 0.142\u00b10.105 1.39 \u00b10.22 0.145 \u00b10.1156\nTable 3: Squared MMD distances over 5 random initializations (average \u00b1standard deviation) for the\nstochastic block model (5 blocks). 23Published in Transactions on Machine Learning Research (01/2025)\nFigure 5: Loss values obtained when optimizing equation 7 with or without data augmentation by adding\npermutations of training adjacency matrices. Fig. We found that the loss function does not converge during training when \u03b3= 0. We provide some\nablation study on the impact of the noise diffusivity rate hyperparameter \u03b1\u22650. When\u03b1= 1(i.e., 1\u2212e\u2212\u03b1T\u22480.63), 19% of the generated graphs contain a single block, 26% contain 2 blocks,\n30% contain 3 blocks, 20% contain 4 blocks, 2% contain 5 blocks, 2% contain 6 blocks, 1% contain 7 blocks. When\u03b1= 2.3(i.e., 1\u2212e\u2212\u03b1T\u22480.9), 17% of the generated graphs contain a single block, 14% contain 2\nblocks, 20% contain 3 blocks, 20% contain 4 blocks, 8% contain 5 blocks, 5% contain 6 blocks, 8% contain 7\nblocks, and the remaining 8% contains up to 15 blocks. 27",
        "Conclusion": "In conclusion, none of the existing one-shot approaches can be easily\nadapted to digraphs. 1. 1. Its adjacency matrix A\u2208{0,1}n\u00d7n\nsatisfies Aij= 1iff(vi,vj)\u2208EandAij= 0otherwise. Our second and last task jointly learns another neural\nnetwork (called edge decoder ) to predict edges. 2). Following\nVeli\u010dkovi\u0107 et al. Nis the same for all graphs of same size. 4). Nonetheless, both Jo et al. (2023). This means that our model obtains a uniqueness and novelty scores of 100%. Laplacian spectrum. 8 Conclusion and Limitations\nWe have proposed a one-shot generative model that samples digraphs and is similar in essence to denoising\nautoencoders. PMLR, 2021. Reaching a consensus. Hung. Acad. Social networks , 5(2):109\u2013137, 1983. 3744\u20133753. PMLR, 2019. InNeurIPS , 2019. PMLR, 2022. Xvi. 4474\u20134484. PMLR, 2020. Number 2. Caroline E Seely. ACM Trans. Pascal Vincent. 5708\u20135717. PMLR, 2018. Ai\u2190P\u22a4AiP. \u2206i\u2190P\u22a4\u2206iP\nend if\nifData augmentation matrix is truethen\nGenerate perturbation matrix C\u2208{0,1}ni\u00d7nis.t.\u2200i,Cii= 0and\u2200i\u0338=j,Cij\u223cBernoulli (\u03c1)\n\u02dcAi\u2190Ai\u2295C. end if\nDefine N\u2208Rn\u00d7das upper submatrix of Ofollowed by softmax operation for \u21131-normalized columns\nDefine BaseT\u02dc\u2206ior optionally as the rank- sapproximation of eT\u02dc\u2206ivia truncated SVD. Following O\u2019Bray et al. This is easily verified. (2009). (2011). - (A1) is satisfied because Kis full rank in our case. This is satisfied if 1\u2264M. - (A3) is satisfied because \u2200j,K(xj,xj)>0and\u2200j\u0338=i,K(xj,xi)\u22650in our case. Therefore, (A3) can be\nsatisfied only if c=0. Once again, DGDK outperforms GRAN and SwinGNN in\nevaluation metrics that take into account global properties of the graph. 6). 7).",
        "label": 5
    }
]