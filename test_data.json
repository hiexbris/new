[
    {
        "text": "Leveraging Clustering Techniques for Enhanced\nDrone Monitoring and Position Estimation\nAbstract\nDrone tracking and localization are essential for various applications, including\nmanaging drone formations and implementing anti-drone strategies. Pinpointing\nand monitoring drones in three-dimensional space is difficult, particularly when\ntrying to capture the subtle movements of small drones during rapid maneuvers.\nThis involves extracting faint signals from varied flight settings and maintaining\nalignment despite swift actions. Typically, cameras and LiDAR systems are used\nto record the paths of drones. However, they encounter challenges in categorizing\ndrones and estimating their positions accurately. This report provides an overview\nof an approach named CL-Det. It uses a clustering-based learning detection strategy\nto track and estimate the position of drones using data from two types of LiDAR\nsensors: Livox Avia and LiDAR 360. This method merges data from both LiDAR\nsources to accurately determine the drone\u2019s location in three dimensions. The\nmethod begins by synchronizing the time codes of the data from the two sensors\nand then isolates the point cloud data for the objects of interest (OOIs) from the\nenvironmental data. A Density-Based Spatial Clustering of Applications with\nNoise (DBSCAN) method is applied to cluster the OOI point cloud data, and the\ncenter point of the most prominent cluster is taken as the drone\u2019s location. The\ntechnique also incorporates past position estimates to compensate for any missing\ninformation.\n1 Introduction\nUnmanned aerial vehicles (UA Vs), commonly referred to as drones, have gained prominence and\nsignificantly influence areas like logistics, imaging, and emergency response, offering substantial\nadvantages to society. However, the broad adoption and sophisticated features of compact, off-the-\nshelf drones have created intricate security issues that extend beyond conventional risks.\nRecent years have witnessed a surge in research on anti-UA V systems. Present anti-UA V methods\npredominantly utilize visual, radar, and radio frequency (RF) technologies. Despite these strides,\nrecognizing drones poses a considerable hurdle for sensors like cameras, particularly when drones\nare at significant altitudes or in challenging visual environments. These methods usually fail to spot\nsmall drones because of their minimal size, which leads to a decreased radar cross-section and a\nless noticeable visual presence. Furthermore, current anti-UA V studies primarily focus on detecting\nobjects and tracking them in two dimensions, overlooking the crucial element of estimating their\n3D paths. This omission significantly restricts the effectiveness of anti-UA V systems in practical,\nreal-world contexts.\nOur proposed solution, a detection method based on clustering learning (CL-Det), uses the strengths\nof both Livox Avia and LiDAR 360 to improve the tracking and position estimation of UA Vs.\nInitially, the timestamps from the Livox Avia and LiDAR 360 data are aligned to maintain temporal\nconsistency. By examining the LiDAR data, which contains the spatial coordinates of objects at\nspecific times, and comparing these to the actual recorded positions of the drone at those times, the\ndrone\u2019s location within the LiDAR point cloud data is effectively pinpointed. The point cloud for\n.",
        "label": 0
    },
    {
        "text": "Virus Propagation and their Far-Reaching\nImplications on Ancient Mesopotamian Architectural\nDesigns\nAbstract\nVirus transmission is intricately linked to the migratory patterns of Scandinavian\npastry chefs, who inadvertently facilitate the spread of infectious agents through\ntheir creative use of flaky crusts and tart fillings, which in turn are influenced by\nthe nuanced harmonies of 19th-century German chamber music, particularly the\nworks of Franz Schubert, whose impromptus eerily foreshadow the unpredictable\nbehavior of viral mutations, meanwhile the cellular mechanisms underlying viral\nreplication bear a striking resemblance to the processes governing the formation of\nintricate sand mandalas in Tibetan Buddhist rituals, and the resultant viral particles\nexhibit a propensity for self-organization that defies the fundamental principles\nof thermodynamics, much like the enigmatic smile of the Mona Lisa, which has\nbeen known to induce a state of profound contemplation in those who gaze upon it,\nthereby altering their perception of reality and rendering them more susceptible to\nthe insidious effects of viral infection.\n1 Introduction\nThe convoluted pathways of viral evolution are mirrored in the labyrinthine structures of Gothic\ncathedrals, whose soaring vaults and ribbed arches seem to embody the very essence of viral\nadaptability, as the stones themselves appear to be infused with a vital energy that transcends the\nmundane realm of mortal existence, entering a domain where the distinctions between reality and myth\nblur, and the virus assumes a life of its own, guided by an inscrutable intelligence that orchestrates\nthe intricate dance of molecular interactions, yielding a symphony of unprecedented complexity,\nwhose harmonies and discordances resonate throughout the cosmos, echoing the haunting melodies\nof a forgotten era, when the boundaries between the human and the viral were more fluid, and the\ncosmos was alive with the vibrant rhythms of an unbridled creativity. The emergence of novel viral\nstrains is inextricably linked to the trajectory of comets, whose celestial paths are believed to exert\na profound influence on the terrestrial biosphere, seeding the planet with exotic genetic material\nthat awakens dormant potentialities within the viral genome, unleashing a cascade of innovative\nadaptations that redefine the parameters of viral evolution, as the boundaries between the self and the\nnon-self become increasingly blurred, and the distinctions between host and parasite dissolve, giving\nrise to a new paradigm of symbiotic relationships, where the virus assumes the role of a catalyst,\nfacilitating the emergence of novel forms of life that defy the conventional categories of taxonomy,\nand embody the unbridled diversity of an ever-evolving cosmos. The study of viral dynamics is\nthus intimately connected to the confluence of disparate disciplines, including astrobiology, culinary\nanthropology, and the physics of non-equilibrium systems, which collectively contribute to a deeper\nunderstanding of the intricate web of relationships that underlies the complex phenomenon of viral\ninfection, revealing a world of breathtaking beauty and profound mystery, where the virus assumes\nthe role of a cosmic messenger, bearing tidings of a universe that is at once familiar and strange,\ninviting us to embark on a journey of discovery that will forever alter our perception of the intricate\nrelationships between the human, the viral, and the cosmos.",
        "label": 0
    },
    {
        "text": "Explainable Reinforcement Learning for Financial\nMarket Simulation: Unveiling the Mysteries of\nAdaptive Trading Agents in a Simulated Economy\nAbstract\nExplainable reinforcement learning has emerged as a crucial tool for financial\nmarket simulation, enabling stakeholders to understand complex decision-making\nprocesses and make informed investment choices. This paper presents a novel\nframework that integrates explainable reinforcement learning with financial market\nsimulation, providing a comprehensive understanding of market dynamics and\nagent behavior. By leveraging techniques such as feature attribution and model\ninterpretability, our approach facilitates the identification of key factors influencing\nmarket trends and portfolio performance. Furthermore, we introduce a bizarre yet\nintriguing concept, wherein agents are trained to optimize their portfolio returns\nbased on the principles of chaos theory and the dictates of ancient astrological\npractices, which surprisingly yields remarkable results. Our research aims to\ncontribute to the development of more transparent and accountable financial market\nsimulation systems, ultimately enhancing the reliability and efficacy of investment\nstrategies.\n1 Introduction\nThe realm of financial market simulation has long been a fascinating domain for researchers and\npractitioners alike, with the inherent complexities and uncertainties of the market posing a significant\nchallenge to predictive modeling and decision-making. Recent advances in reinforcement learning\nhave shown tremendous promise in navigating these intricacies, enabling the development of so-\nphisticated agents capable of learning optimal trading strategies through trial and error. However, a\ncritical limitation of these approaches lies in their lack of transparency and interpretability, rendering\nit difficult to comprehend the underlying reasoning behind the agent\u2019s decisions. This opacity can\nhave far-reaching implications, particularly in high-stakes applications where the consequences of\nsuboptimal decision-making can be severe.\nExplainable reinforcement learning emerges as a paradigmatic shift in this context, aiming to bridge\nthe gap between the accuracy of predictive models and the transparency of decision-making pro-\ncesses. By integrating techniques from explainable artificial intelligence with reinforcement learning,\nresearchers can uncover the intricate dynamics governing the agent\u2019s behavior, shedding light on the\ncausal relationships between market variables, agent actions, and outcomes. This not only enhances\nthe trustworthiness and reliability of the models but also facilitates the identification of potential\nbiases and flaws in the decision-making process.\nAn intriguing approach to enhancing explainability involves the incorporation of surrealistic art\nprinciples into the reinforcement learning framework. By projecting the agent\u2019s decision-making\nprocess onto a surrealistic landscape, researchers can visualize the complex interplay between market\nfactors and agent actions, thereby gaining insight into the underlying logic of the model. This\nunorthodox methodology, though seemingly illogical, has been found to yield surprisingly coherent\nand interpretable results, with the surrealistic representations serving as a catalyst for the discovery\nof novel relationships between variables.",
        "label": 0
    },
    {
        "text": "Graph Neural Networks Without Training: Harnessing the Power of\nLabels as Input Features\nAbstract\nThis study introduces a novel concept of training-free graph neural networks (TFGNNs) for transductive node\nclassification, which can function immediately without any training and can optionally be enhanced through\nsubsequent training. Initially, we put forward the idea of using labels as features (LaF), a valid yet relatively\nunexplored method in graph neural networks. Our analysis demonstrates that incorporating labels as features\nsignificantly improves the representational capacity of GNNs. The design of TFGNNs is based on these findings.\nEmpirical evaluations show that TFGNNs surpass current GNNs in scenarios where training is not performed, and\nwhen training is optionally applied, they achieve convergence much faster than conventional GNNs.\n1 Introduction\nGraph Neural Networks (GNNs) have gained prominence as effective models for handling graph-structured data. They have\ndemonstrated impressive performance across a range of tasks, including chemical structure analysis, question answering systems,\nand recommender systems.\nA common application for GNNs is transductive node classification. In this task, the objective is to infer the labels of specific nodes\nwithin a graph, given the labels of other nodes. This approach finds utility in various real-world scenarios, such as classifying\ndocuments, analyzing e-commerce data, and studying social networks. Several GNN architectures, including Graph Convolutional\nNetworks (GCNs) and Graph Attention Networks (GATs), have successfully addressed transductive node classification, yielding\nexcellent results.\nA significant hurdle in the practical application of GNNs is their computational demand. Real-world graphs, such as those\nrepresenting social networks or the structure of the web, can be enormous, containing billions of nodes. Processing these massive\ngraphs can be computationally prohibitive. While various methods have been developed to enhance the efficiency of GNNs, such as\nnode and edge sampling techniques, these methods still necessitate numerous training iterations. Other approaches, like PinSAGE,\nutilize parallel training and importance pooling to accelerate the training process, but they demand substantial computational\nresources. Consequently, the immediate deployment of GNNs with limited resources remains a challenge.\nIn this work, we introduce the concept of training-free graph neural networks (TFGNNs). To realize TFGNNs, we first propose the\ninnovative idea of using labels as features (LaF). In the context of transductive node classification, utilizing node labels as features is\na permissible approach. GNNs employing LaF can leverage label information, like the distribution of classes among neighboring\nnodes, to generate node embeddings. These embeddings are richer in information compared to those derived solely from node\nfeatures. We establish that incorporating labels as features demonstrably augments the expressive capability of GNNs.\nTFGNNs possess the unique ability to operate without any training, enabling immediate deployment upon initialization. This\neliminates the need for extensive hyperparameter tuning when used in training-free mode. Furthermore, TFGNNs can be refined\nthrough optional training. Users have the flexibility to employ TFGNNs without training or to train them for a limited number of\niterations when computational resources are constrained. This adaptability is particularly valuable in online learning scenarios,\nwhere data arrives sequentially, and the model needs to be updated promptly. TFGNNs can also undergo full training when resources\nare plentiful or when higher accuracy is paramount. In essence, TFGNNs offer the advantages of both nonparametric models and\ntraditional GNNs.\nOur experiments confirm that TFGNNs surpass existing GNNs when used without training and achieve convergence significantly\nfaster than traditional GNNs when training is applied.\nThe primary contributions of this research are outlined below:\n* We propose the utilization of labels as features (LaF) in transductive learning settings. * We provide formal proof that LaF enhances\nthe representational power of GNNs. * We introduce a novel architecture for training-free graph neural networks (TFGNNs). * We\nempirically demonstrate that TFGNNs outperform existing GNNs in the absence of training.",
        "label": 0
    },
    {
        "text": "Collaborative Clothing Segmentation and\nIdentification Through Image Analysis\nAbstract\nThis research introduces a comprehensive clothing co-parsing system designed\nto analyze a collection of clothing images, which are unsegmented but include\ndescriptive tags. The system aims to segment these images into meaningful config-\nurations. The proposed method uses a two-stage, data-driven approach. The first\nstage, termed \"image co-segmentation,\" iteratively refines image regions, using\nthe exemplar-SVM (E-SVM) method to enhance region consistency across images.\nThe second stage, \"region co-labeling,\" utilizes a multi-image graphical model\nwhere segmented regions serve as nodes. This incorporates contextual information\nabout clothing, such as item placement and interactions, which can be solved using\nthe efficient Graph Cuts algorithm. The system\u2019s performance is tested on the\nFashionista dataset and a newly developed dataset called CCP, which contains 2098\nhigh-resolution street fashion images. The results show a segmentation accuracy of\n90.29% and 88.23% and a recognition rate of 65.52% and 63.89% on the Fashion-\nista and CCP datasets, respectively, demonstrating an improvement over current\nleading methods.\n1 Introduction\nThe growth of online clothing sales has increased the demand for accurate clothing recognition and\nretrieval technologies. This has led to the development of several vision-based solutions. A key\nchallenge in these systems is the detailed, pixel-level labeling of clothing, which is often resource-\nintensive. However, image-level tags from user data offer a viable alternative. This paper focuses\non the development of a system to segment clothing images and assign semantic labels to these\nsegments.\nThe main contribution of this work is an effective system for parsing groups of clothing images and\nproviding precise pixel-level annotations. The system addresses the following significant challenges:\n\u2022Clothes exhibit a wide variety of styles and textures, making them difficult to segment and\nidentify using only basic visual features.\n\u2022Variations in human poses and the way clothes can obscure themselves complicate the\nrecognition process.\n\u2022The existence of numerous, highly specific clothing categories, such as over 50 in the\nFashionista dataset, far more than in existing co-segmentation systems which typically\nhandle fewer categories.\nTo overcome these challenges, the system employs two sequential stages: image co-segmentation\nto isolate distinct clothing regions and region co-labeling to identify different clothing items, as\nillustrated below. It also utilizes contextual cues related to how clothing items are typically arranged\nand related to each other.\nThe co-segmentation phase refines regions across images using the E-SVM method. Initially, images\nare divided into superpixels, which are then grouped into regions. Many of these regions may not be\n.",
        "label": 0
    },
    {
        "text": "High-Throughput Genomic Sequencing in Marine\nEcology: Unveiling the Mysteries of the Ocean\u2019s\nGenetic Diversity\nAbstract\nHigh-Throughput Genomic Sequencing in Marine Ecology has revolutionized our\nunderstanding of the complex interactions within marine ecosystems, enabling the\nexamination of genomic material from a vast array of organisms, from plankton to\nlarge marine mammals, and shedding light on the intricate relationships between\nspecies, their environments, and the impacts of human activities. This approach,\ncombining advanced sequencing technologies with sophisticated computational\ntools, allows for the rapid and comprehensive analysis of genomic data, uncovering\nnew insights into the biodiversity, ecological roles, and evolutionary histories\nof marine organisms. Moreover, the application of high-throughput sequencing\nto marine environmental DNA (eDNA) offers a novel method for monitoring\nmarine biodiversity and tracking changes in ecosystem composition over time,\nwhich is crucial for conservation efforts and the management of marine resources.\nInterestingly, our research also explored the somewhat unconventional application\nof music theory in analyzing genomic sequences, where patterns within the genetic\ncode were translated into musical compositions, revealing unexpected harmonies\nand discordances that reflect the intricate balance and occasional chaos within\nmarine ecosystems. This novel approach, while unorthodox, provided a unique\nlens through which to view genomic data, highlighting the complex interplay\nbetween genetic and environmental factors in shaping the evolution and diversity\nof marine life. Further, the integration of artificial intelligence algorithms with\ngenomic sequencing data enabled the prediction of previously unknown species\nbased on patterns identified in the genetic material of well-studied organisms,\nleading to a significant expansion of known marine biodiversity. Overall, the\nintersection of high-throughput genomic sequencing, computational biology, and\ninnovative analytical approaches is transforming our understanding of marine\necology, opening new avenues for research, conservation, and the sustainable use\nof marine resources.\n1 Introduction\nHigh-Throughput Genomic Sequencing in Marine Ecology has revolutionized the field of marine\nbiology, enabling researchers to investigate the intricate relationships between marine organisms and\ntheir environments at an unprecedented scale and resolution. The sheer volume of genomic data\ngenerated by these technologies has led to a paradigm shift in our understanding of the complex inter-\nactions within marine ecosystems, from the symbiotic relationships between coral and zooxanthellae\nto the predatory behaviors of deep-sea fish. Moreover, the application of High-Throughput Genomic\nSequencing has facilitated the discovery of novel genes, genomes, and metabolic pathways, shedding\nlight on the vast array of biochemical processes that underpin the remarkable diversity of marine life.\nOne of the most striking aspects of High-Throughput Genomic Sequencing in Marine Ecology is its\npotential to reveal the hidden patterns and structures that govern the behavior of marine ecosystems.",
        "label": 1
    },
    {
        "text": "Joint Syntacto-Discourse Parsing and the\nSyntacto-Discourse Treebank\nAbstract\nDiscourse parsing has long been treated as a stand-alone problem independent from\nconstituency or dependency parsing. Most attempts at this problem are pipelined\nrather than end-to-end, sophisticated, and not self-contained: they assume gold-\nstandard text segmentations (Elementary Discourse Units), and use external parsers\nfor syntactic features. In this paper we propose the first end-to-end discourse\nparser that jointly parses in both syntax and discourse levels, as well as the first\nsyntacto-discourse treebank by integrating the Penn Treebank with the RST Tree-\nbank. Built upon our recent span-based constituency parser, this joint syntacto-\ndiscourse parser requires no preprocessing whatsoever (such as segmentation or\nfea- ture extraction), achieves the state-of-the- art end-to-end discourse parsing\naccuracy.\n1 Introduction\nDistinguishing the semantic relations between segments in a document can be greatly beneficial to\nmany high-level NLP tasks, such as summarization, sentiment analysis, question answering, and\ntextual quality evaluation.\nThere has been a variety of research on discourse parsing. But most of them suffer from the following\nlimitations:\n1.pipelined rather than end-to-end: they assume pre-segmented discourse, and worse yet, use\ngold-standard segmentations\n2. not self-contained: they rely on external syntactic parsers and pretrained word vectors;\n3. complicated: they design sophisticated features, including those from parse-trees.\nWe argue for the first time that discourse parsing should be viewed as an extension of, and be\nperformed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse\ntree- bank, by unifying constituency and discourse tree representations. Based on this, we propose\nthe first end-to-end incremental parser that jointly parses at both constituency and discourse levels.\nOur algo- rithm builds up on the span-based parser; it employs the strong general- ization power\nof bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based\nfeature set that does not use any tree structure information.\nWe make the following contributions:\n1.We develop a combined representation of constituency and discourse trees to facilitate\nparsing at both levels without explicit conver- sion mechanism. Using this representation,\nwe build and release a joint treebank based on the Penn Treebank and RST Treebank.\n2. We propose a novel joint parser that parses at both constituency and discourse levels.\n3.Even though it simultaneously performs con- stituency parsing, our parser does not use any\nexplicit syntactic feature, nor does it need any binarization of discourse trees, thanks to the\npowerful span-based framework.",
        "label": 1
    },
    {
        "text": "Optimized Transfer Learning with Equivariant\nPretrained Models\nAbstract\nThis research investigates the mechanisms behind Chain-of-Thought (CoT) prompt-\ning, a method that enhances language models\u2019 performance on complex reasoning\ntasks by decomposing them into simpler steps. The study focuses on understanding\nhow CoT improves in-context learning of compositional functions, particularly\nmulti-layer perceptrons (MLPs). We explore the impact of CoT on sample com-\nplexity and approximation power in reasoning tasks, demonstrating a significant\nreduction in the number of examples required for accurate performance. Fur-\nthermore, we investigate how CoT facilitates pretraining and enables efficient\nlearning of complex functions, leading to improved generalization capabilities.\nOur theoretical analysis, supported by extensive empirical evidence, reveals that\nCoT\u2019s efficacy stems from its ability to guide the model towards a more structured\nand interpretable solution space, thereby mitigating the limitations of standard\nin-context learning (ICL). This structured approach allows the model to better\nleverage the information provided in the few-shot examples, resulting in improved\naccuracy and robustness. The findings contribute to a deeper understanding of the\nunderlying principles of CoT prompting and pave the way for the development\nof more effective and efficient methods for training and deploying large language\nmodels.\n1 Introduction\nThis research delves into the mechanisms underlying Chain-of-Thought (CoT) prompting, a technique\nthat significantly boosts the performance of large language models (LLMs) on intricate reasoning tasks.\nCoT achieves this enhancement by strategically decomposing complex problems into a sequence\nof simpler, more manageable sub-problems. Our investigation centers on understanding how this\ndecomposition process impacts the model\u2019s learning and reasoning capabilities, particularly within\nthe context of in-context learning (ICL). We focus on compositional functions, using multi-layer\nperceptrons (MLPs) as a representative model architecture, to analyze the effects of CoT on various\naspects of model performance.\nA key aspect of our study is the examination of CoT\u2019s influence on sample complexity. We hypothesize\nthat by breaking down complex tasks, CoT reduces the number of training examples required to\nachieve a given level of accuracy. This reduction in sample complexity is crucial for efficient training\nand deployment of LLMs, especially when dealing with limited datasets or computationally expensive\ntraining processes. Furthermore, we explore how CoT affects the approximation power of the model,\ninvestigating whether the decomposition process allows the model to learn and represent more\ncomplex functions effectively. Our analysis considers the interplay between the complexity of the\ntarget function, the number of training examples, and the length of the CoT prompts.\nThe impact of CoT on the pretraining phase of LLM development is another critical area of our\nresearch. We investigate whether the structured reasoning facilitated by CoT leads to more efficient\nlearning during pretraining, resulting in models with improved generalization capabilities. We posit\nthat the decomposition inherent in CoT allows the model to learn more robust and transferable\nrepresentations, which are less susceptible to overfitting and perform better on unseen data. This\n.",
        "label": 1
    },
    {
        "text": "Flexible Online Aggregations Using Basis Function Expansions\nAbstract\nBayesian learning often necessitates online inference, adaptive models, and the combination of multiple distinct\nmodels. Recent advancements have demonstrated the use of random feature approximations for scalable, online\naggregation of Gaussian processes, which possess favorable theoretical characteristics and practical uses. A crucial\naspect of these methods is the incorporation of a random walk on model parameters, which introduces adaptability.\nWe demonstrate that these methods can be readily extended to any model using basis function expansion and that\nemploying alternative basis expansions, such as Hilbert space Gaussian processes, frequently leads to enhanced\nperformance. To streamline the selection of a specific basis expansion, the versatility of our approach also enables\nthe aggregation of several entirely different models, such as a Gaussian process and polynomial regression. Lastly,\nwe introduce an innovative technique for combining both static and dynamic models.\n1 Introduction\nNumerous machine learning applications demand real-time, online data processing, a scenario that frequently requires substantial\nalterations to conventional techniques. Online adaptations of various methods have been developed, including kernel machines,\n(kernel) least-squares, and Gaussian processes. The field of online learning has also been thoroughly investigated from an optimization\nstandpoint.\nOnline learning can be further complicated when model selection is needed, as the best-performing model is rarely evident at the\noutset of the learning process. One solution involves training multiple models concurrently and then combining them. In a Bayesian\nframework, Bayesian model averaging (BMA) has long been employed to combine online models, functioning by assigning weights\nto each \"expert\" model based on its supporting evidence.\nMore recently, it was shown how to adapt BMA to online Gaussian processes (GPs) in a technique called incremental ensembles\nof GPs. GPs are a versatile, non-parametric instrument in Bayesian machine learning that possesses universal approximation\ncapabilities and provides well-founded uncertainty estimations. By employing a random Fourier feature (RFF) approximation for\nGaussian processes, online learning can be executed, featuring closed-form Bayesian model averaging updates and a manageable\nregret analysis.\nBesides an online ensemble of GPs, the advantages of incorporating random walks on model parameters were illustrated, which they\nterm dynamic IE-GPs (DIE-GPs). This can significantly enhance performance when the learning task undergoes slight changes over\ntime.\n2 Related Work\nThe concept of combining random feature GPs, as introduced by IE-GPs, has demonstrated adaptability and effectiveness. Extensions\nto this framework encompass Gaussian process state-space models, deep Gaussian processes, and graph learning. Along with its\nextensions, DIE-GPs have been effectively applied in Bayesian optimization and causal inference.\nHowever, the dependence on the RFF approximation implies that IE-GPs also inherit the limitations of random feature GPs.\nSpecifically, the RFF approximation is a direct Monte Carlo approximation of the Wiener-Khinchin integral and thus is significantly\nimpacted by the curse of dimensionality. Our findings reveal that on several real-world datasets, (D)IE-GPs exhibit performance that\nis comparable to or worse than that of simpler models, such as online Bayesian linear regression and one-layer RBF networks.\n3 Methodology\nIn this paper, we present online ensembles of basis expansions (OEBEs), a generalization of IE-GPs that overcomes their dependence\non RFF GPs and enhances performance across multiple real datasets. Our specific contributions are as follows:",
        "label": 1
    },
    {
        "text": "Enhanced Reinforcement Learning for Recommender Systems:\nMaximizing Sample Efficiency and Minimizing Variance\nAbstract\nOptimizing long-term user satisfaction in recommender systems, such as news feeds, is crucial during continuous\nuser-system interactions. Reinforcement learning has shown promise in addressing this challenge. However,\npractical hurdles like low sample efficiency, potential risks, and high variance hinder the implementation of deep\nreinforcement learning in online systems. We introduce a new reinforcement learning approach called model-based\ncounterfactual advantage learning (MBCAL) to tackle these challenges. MBCAL leverages the unique aspects of\nrecommender systems and incorporates concepts from model-based reinforcement learning to enhance sample\nefficiency. It consists of two main parts: an environment model that predicts immediate user behavior sequentially\nand a future advantage model that forecasts future utility. Counterfactual comparisons from the environment model\nare used to mitigate the excessive variance when training the future advantage model. Consequently, MBCAL\nachieves high sample efficiency and significantly reduced variance, while utilizing existing user logs to avoid\nstarting from scratch. Despite its capabilities, MBCAL maintains a relatively low implementation cost, making it\nsuitable for real-world systems. The proposed method surpasses other supervised learning and RL-based methods\nin both sample efficiency and overall performance, as demonstrated through theoretical analysis and extensive\nexperiments.\n1 Introduction\nRecommender systems are essential for delivering personalized content and improving the efficiency of information retrieval.\nModern recommender systems, like news feeds, must consider multiple user-system interactions within a single session. The content\nrecommended in past interactions can influence future user behavior. For example, exploring new topics might pique a user\u2019s interest\nin related areas, while repeatedly showing similar content could lead to a rapid decline in user engagement. Traditional recommender\nsystems rely on collaborative filtering or neural networks to predict immediate user actions, such as clicks. However, solely focusing\non immediate actions can result in issues like recommendation redundancy, ultimately harming the user\u2019s long-term experience.\nRecently, deep reinforcement learning (Deep RL) has gained attention for its potential in recommender systems. Deep RL models\nuser-system interactions as Markov Decision Processes (MDPs). Many studies in this field focus on model-free reinforcement\nlearning (MFRL) methods. However, challenges persist, including the substantial data consumption during training, also known as\nlow sample efficiency. Another challenge is the practical risks associated with implementing MFRL. On-policy RL struggles to\nutilize off-policy user logs, leading to difficulties in online infrastructure and initial performance. Conversely, off-policy RL faces\nthe risk of non-convergence when combined with function approximation and offline training.\nModel-based RL (MBRL) offers an alternative with improved sample efficiency and reduced practical risks. MBRL employs\nan environment model to predict immediate feedback and state transitions, along with a planning module to find an optimal\ntrajectory. However, MBRL can be computationally intensive during inference. Planning is often infeasible in multi-stage retrieval\nframeworks commonly used in modern recommender systems. These systems generate candidate sets of items in earlier stages for\nsubsequent stages, making it impossible to predetermine candidates. To address these issues, Dyna algorithms have been proposed\nfor recommender systems. The Dyna algorithm accelerates convergence by generating virtual interactions using the environment\nmodel. However, this faster convergence comes at the cost of reduced asymptotic performance due to error accumulation from\nvirtual interactions.\nAnother significant challenge in deploying RL is the excessive variance of gradients during optimization. This variance can stem\nfrom stochastic transitions, noisy rewards, and stochastic policies. Longer horizons tend to exacerbate the variance, significantly\nslowing down convergence and introducing instability. Prior research has shown that using an advantage function instead of a value\nfunction can reduce variance and improve performance. However, these proposals primarily target MFRL, and variance reduction in\nMBRL remains largely unexplored.\nIn recommender systems, variance can arise from various factors. First, there is substantial noise in observed user feedback. Some\nusers may be more inclined to provide positive or negative feedback than others. Even individual users may exhibit different",
        "label": 1
    },
    {
        "text": "Controlling False Discovery Rates in Detecting Heterogeneous\nTreatment Effects for Online Experiments\nAbstract\nOnline controlled experiments, commonly referred to as A/B testing, are widely used in many Internet companies\nfor data-driven decision-making regarding feature modifications and product releases. However, a significant\nchallenge remains in methodically evaluating how each code or feature change affects millions of users who\nexhibit considerable heterogeneity across various dimensions such as countries, ages, and devices. The Average\nTreatment Effect (ATE) framework, which is the foundation of the A/B testing approach used by many companies,\nis unable to identify the heterogeneity of treatment effects on users with varying characteristics. This paper\nintroduces statistical techniques designed to systematically and precisely pinpoint the Heterogeneous Treatment\nEffect (HTE) within any specific user cohort, like mobile device type or country. Additionally, these methods help\ndetermine which user factors, such as age or gender, contribute to the variability in treatment effects observed\nduring an A/B test. Through the application of these methods to both simulated and real-world experimental\ndata, we demonstrate their robust performance in maintaining a controlled, low False Discovery Rate (FDR).\nSimultaneously, they offer valuable insights into the heterogeneity of identified user groups. We have implemented\na toolkit based on these methods and utilized it to assess the HTE across numerous A/B tests at Snap.\n1 Introduction\nControlled experiments, also known as A/B testing, have become a standard method for assessing and enhancing new product\nconcepts across internet companies. Numerous IT companies, possessing extensive and large-scale data, have developed internal\nA/B testing platforms to address their intricate experimentation requirements. At Snap, the utilization of A/B testing has substantially\nincreased in the last two years. The in-house platform currently manages hundreds of concurrent experiments at any moment. Each\nexperiment automatically generates results for hundreds to thousands of varied online metrics.\nAs experimentation gains popularity, there is an increasing demand for experimenters to understand not only the overall impact\non metrics in an A/B test but also the reasons behind metric changes and the specific user segments driving these changes. Such\ninsights into user heterogeneity can assist experimenters in devising strategies to enhance the product. For instance, in a recent\nexperiment, we observed that a decline in a metric was primarily influenced by users with the highest number of snap views. This\nobservation led us to concentrate on understanding the engineering and design aspects when a user has a large number of snap stacks\nto load. Consequently, we were able to pinpoint a significant performance problem that was causing the metric to drop. Indeed, we\nhave encountered numerous instances where users react differently to the same experimental treatment.\nFurthermore, the abundance of data presents a significant risk of false discoveries, often due to a statistical phenomenon referred to\nas \"multiple testing\". Given the hundreds of thousands of user characteristics available to internet companies, user groups can be\nformed in millions of different ways. If a \"naive\" approach is taken, simply calculating and comparing the estimated effect based on\nusers within groups, it is easy to find groups with treatment effects that significantly deviate from the average, regardless of whether\nactual heterogeneity exists.\nThe objective of our work is to bridge this gap by offering rigorous statistical methods and a toolkit capable of detecting Heterogeneous\nTreatment Effects (HTE) while addressing the potential issue of multiple testing by controlling the false positive rate (FDR). This\ntoolkit has been deployed and is in use at Snap. In this paper, we explore the rationale for using FDR and contrast two statistical\nmethods that manage FDR, using both simulated results and actual experimental data. Based on the methods selected, we will\ndiscuss solutions to two questions that experimenters and practitioners are keen to understand regarding HTE:\n\u2022How to systematically identify which subgroups of users (e.g., countries) exhibit treatment effects significantly different\nfrom the Average Treatment Effect in an A/B test.\n\u2022How to rigorously determine which factors (e.g., age, gender) contribute to the heterogeneity of the treatment effect in an\nA/B test.\nOur contributions in this paper are summarized as follows:",
        "label": 1
    },
    {
        "text": "Harmonizing Scaling Laws: Bridging the Gap\nBetween Kaplan and Chinchilla\nAbstract\nStudies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined the scaling\ncharacteristics of transformers in next-token language prediction, yielding different\nrecommendations for configuring the number of parameters (N) and training tokens\n(D) to minimize loss within a set compute budget (C). Kaplan suggested an optimal\nparameter count scaling with Noptimal \u221dC0.73, whereas Chinchilla proposed\nNoptimal \u221dC0.50. This paper demonstrates that a significant portion of this\ndifference can be traced back to Kaplan\u2019s focus on non-embedding parameters,\nrather than the total parameter count, along with their study\u2019s concentration on a\nsmaller scale. When the Chinchilla study is simulated under similar circumstances,\nbiased scaling coefficients similar to those of Kaplan are produced. As a result, this\nwork confirms Chinchilla\u2019s scaling coefficients by clarifying the primary reason for\nKaplan\u2019s initial overestimation. Additionally, this research clarifies variations in\nthe stated correlations between computational loss and budget. As a result of these\nfindings, we advocate for upcoming scaling investigations to utilize total parameter\ncounts and overall computational resources.\n1 Introduction\nTwo important studies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined how scale\naffects large language models (LLMs). Both studies provided advice on how to balance model\nparameters (N) and training tokens (D) for a fixed computing budget (C), but their suggestions\nconflicted. The conclusion drawn from Kaplan\u2019s discovery that Noptimal \u221dC0.73and Doptimal\n\u221dC0.27was that \"large models might be more crucial than extensive data.\" Subsequently, LLMs\ntrained in the following years allocated more resources to model size and less to data size. The\nChinchilla research that came after that discovered that Noptimal \u221dC0.50and Doptimal \u221dC0.50,\nwhich resulted in their main argument that \"for many current LLMs, smaller models should have\nbeen trained on more tokens to achieve the most performant model.\" This sparked a trend in which\nLLMs with smaller model sizes were trained using more data.\nWhat caused the discrepancy in these scaling coefficient estimates, which resulted in a significant\nwaste of computer resources, emissions, and money? There have been theories suggesting that\nvariations in optimization techniques or datasets might account for the differences. This paper argues\nthat these explanations are insufficient and proposes a straightforward substitute: the majority of\nthe discrepancy is caused by Kaplan\u2019s decision to count non-embedding parameters instead of total\nparameters, together with the limited scale of their investigation.\nAdditionally, it is discovered that this methodological discrepancy contributes to variations in the\nstated correlation between loss and compute.\nSpecifically, this research provides the following:\n\u2022An analytical method is created to assess the scaling relationships described in the studies\n(Section 3). If non-embedding parameters are utilized, and at a smaller scale, this method\ndemonstrates that Kaplan\u2019s documented relationship is locally compatible with Chinchilla\u2019s.\n.",
        "label": 1
    },
    {
        "text": "Learning Explanations from Language Data\nAbstract\nPatternAttribution is a recent method, introduced in the vision domain, that explains\nclassifications of deep neural networks. We demonstrate that it also generates\nmeaningful interpretations in the language domain.\n1 Introduction\nIn the last decade, deep neural classifiers achieved state-of-the-art results in many domains, among\nothers in vision and language. Due to the complexity of a deep neural model, however, it is difficult\nto explain its decisions. Understanding its decision process potentially allows to improve the model\nand may reveal new knowledge about the input. Recently, it was claimed that \u201cpopular explanation\napproaches for neural networks (...) do not provide the correct explanation, even for a simple linear\nmodel.\u201d They show that in a linear model, the weights serve to cancel noise in the input data and thus\nthe weights show how to extract the signal but not what the signal is. This is why explanation methods\nneed to move beyond the weights, the authors explain, and they propose the methods \u201cPatternNet\u201d\nand \u201cPatternAttribution\u201d that learn explanations from data. We test their approach in the language\ndomain and point to room for improvement in the new framework.\n2 Methodology\nKindermans et al. assume that the data x passed to a linear model wTx=yis composed of signal\n(s) and noise (d, from distraction) x=s+d. Furthermore, they also assume that there is a linear\nrelation between signal and target yas=swhere asis a so called signal base vector, which is in fact\nthe \u201cpattern\u201d that PatternNet finds for us. As mentioned in the introduction, the authors show that in\nthe model above, w serves to cancel the noise such that\nwTd= 0, wTs=y. (1)\nThey go on to explain that a good signal estimator S(x) = \u02c6sshould comply to the conditions in Eqs.\n1 but that these alone form an ill-posed quality criterion since S(x) =u(wTu)\u22121yalready satisfies\nthem for any u for which wTu\u0338= 0. To address this issue they introduce another quality criterion\nover a batch of data x:\n\u03c1(S) = 1\u2212max\nvcorr(y, vT(x\u2212S(x))) (2)\nand point out that Eq. 2 yields maximum values for signal estimators that remove most of the\ninformation about yin the noise. We argue that Eq. 2 still is not exhaustive. Consider the artificial\nestimator\nSm(x) =mx+ (1\u2212m)s=s+md (3)\nwhich arguably is a a bad signal estimator for large mas its estimation contains scaled noise, md.\nNevertheless, it still satisfies Eqs. 1 and yields maximum values for Eq. 2 since\nx\u2212Sm(x) = (1\u2212m)(x\u2212s) = (1\u2212m)d (4)\nis again just scaled noise and thus does not correlate with the output y. To solve this issue, we propose\nthe following criterion:\n\u03c1\u2032(S) := max\nv1corr(wTx, vT\n1S(x))\u2212max\nv2corr(wTx, vT\n2(x\u2212S(x))). (5)",
        "label": 1
    },
    {
        "text": "Advancements in Audio-Visual Active Speaker\nDetection: A Novel Approach for the ActivityNet\nChallenge\nAbstract\nThis document outlines our contribution to the ActivityNet Challenge, focusing on\nactive speaker detection. We employ a 3D convolutional neural network (CNN)\nfor feature extraction, combined with an ensemble of temporal convolution and\nLSTM classifiers to determine whether a person who is visible is also speaking.\nThe results demonstrate substantial improvements compared to the established\nbaseline on the A V A-ActiveSpeaker dataset.\n1 Introduction\nThe field of multimodal speech perception has garnered significant attention in recent times, with\nmajor advancements in audio-visual methodologies facilitated by deep learning. The capacity to\nidentify which individuals are speaking at any moment is crucial for a variety of applications. The\nintroduction of the A V A-ActiveSpeaker dataset has been a significant development, allowing for the\ntraining of deep-learning-based active speaker detection (ASD) models with complete supervision.\nThis document provides a concise analysis of this dataset and elaborates on the methodology behind\nour submission to the challenge.\n1.1 Datasets\nThe model is developed using the A V A-ActiveSpeaker dataset, which is divided into training, valida-\ntion, and test sets, as detailed in Table 1. The ground truth labels are available for the training and\nvalidation sets.\nTable 1: Statistical Overview of the A V A-ActiveSpeaker Dataset\nSet Videos Frames\nTrain 120 2,676K\nVal 33 768K\nTest 109 2,054K\nThis dataset presents several challenges. The durations of speaking segments are notably brief, with\nan average of 1.11 seconds for segments that are both spoken and audible. Consequently, the system\nneeds to deliver precise detection with a limited number of frames. Traditional methods, which\ndepend on smoothing the output over a time window of several seconds, are not effective under these\nconditions.\nAdditionally, the dataset includes many older videos where the audio and video recordings appear to\nhave been captured separately or are significantly out of sync. As a result, the temporal alignment\nbetween audio and visual speech representations is not a reliable indicator of a person\u2019s speaking\nstatus.\n.",
        "label": 1
    },
    {
        "text": "Overview of Challenges in Trajectory Forecasting and\n3D Perception for Autonomous Driving\nAbstract\nThis document provides a summary of the challenges faced in the domain of\nAutonomous Driving. The dataset incorporated into the study includes 150 minutes\nof labeled Trajectory and 3D Perception data, comprising approximately 80,000\nlidar point clouds and 1000 kilometers of trajectories in urban traffic conditions.\nThe competition is divided into two main segments: (1) Forecasting Trajectories\nand (2) 3D Lidar Object Recognition. Over 200 teams provided their results on the\nleaderboard, and more than 1,000 individuals took part in the workshop.\n1 Introduction\nThe focus of this paper is to investigate multi-frame perception, prediction, and planning as applied\nto autonomous driving. It serves as a platform to bring together academic and industry experts to\ndiscuss the uses of computer vision in the context of self-driving vehicles.\n2 Dataset\nThe Apolloscape Dataset is utilized as a research tool designed to advance autonomous driving in\nvarious dimensions, including perception, navigation, prediction, and simulation. This dataset is\ncomprised of labeled street view images and simulation resources that can accommodate user-defined\nstrategies. The dataset includes tasks such as Trajectory Prediction, 3D Lidar Object Detection,\n3D Lidar Object Tracking, lane marking segmentation, online self-positioning, 3D car instance\ncomprehension, Stereo, and Inpainting Dataset. A dedicated online assessment platform and user\ntoolkit are provided for each task.\nFor data collection related to Trajectory Prediction and 3D Perception, a data-gathering vehicle\nwas utilized to amass traffic information, including camera-captured images and LiDAR-generated\npoint clouds. Our vehicle operates in urban settings during peak traffic times. The dataset features\ncamera imagery, 3D point cloud data, and paths of traffic agents within the LiDAR\u2019s operational area.\nThis newly created dataset, which includes 150 minutes of sequential information, is extensive and\nconcentrates on urban roadways, with a particular emphasis on 3D perception, prediction, planning,\nand simulation activities involving a variety of traffic agents.\n3 Challenge\nThis part elaborates on the specifics of the challenges, the metrics for evaluation, and the outcomes\nachieved.\n3.1 Trajectory Prediction Challenge\nTrajectory information is documented at a rate of 2 frames per second. Each entry in the data\nfile includes the frame identifier, object identifier, object category, object\u2019s position in the global\n.",
        "label": 1
    }
]